---
title: Setting up a Python environment for AWS S3 bucket interaction
tech: AWS
tag: logo-aws
date: 11 March 2023
---

A step-by-step guide on how to set up a Python with Jupyter Notebook environment (in Windows) to be able to connect with your Amazon Web Services (AWS) S3 bucket. 

This was a prerequisite step for one of my Python projects that uses the YouTube API to download videos to a local folder (see blog post for that set up **[here](https://datamesse.github.io/#/post/1663509600)**), then upload them to an S3 bucket.


## Step 1. Install Python

Download and install the latest version of Python.

* **[https://www.python.org/downloads/](https://www.python.org/downloads/)**

During the installation process, ensure you tick the _"Add Python to environment variables"_ option.

![Python installation](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2023-03-11--01.png?raw=true)


## Step 2. Install classic Jupyter Notebook

For Windows machines, open Command Prompt and run the following code:

```
pip install notebook
```

* **[https://jupyter.org/install](https://jupyter.org/install)**

![Classic Jupyter Notebook installation](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2023-03-11--02.png?raw=true)


## Step 3. Create a project directory and a new notebook

Create a local folder where your project's code will be kept.

![Project folder](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2023-03-11--03.png?raw=true)

From Windows File Explorer, navigate to that folder and replace the address bar's contents with cmd, then press Enter.

![Run cmd from project folder](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2023-03-11--04.png?raw=true)

This will open the Command Prompt to that location.

Type jupyter notebook, then press Enter.

![Run jupyter notebook from cmd](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2023-03-11--05.png?raw=true)

This will open an instance of Jupyter Notebook, where you can create a new Notebook from.

![Jupyter Notebook](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2023-03-11--06.png?raw=true)


## Step 4. Create an AWS S3 Bucket

Log into AWS's S3 console and create a new S3 bucket, ensuring you enable the _"Block all public access"_ setting for it.

* **[https://s3.console.aws.amazon.com/s3/](https://s3.console.aws.amazon.com/s3/)**

Note your name for the bucket. In this example, I named mine "youtube-video-analysis".

![Create S3 Bucket](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2023-03-11--07.png?raw=true)

For more information on S3 buckets, visit:

* **[https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html)**


## Step 5. Create an IAM policy and apply it to an AWS account

Navigate to AWS's IAM console (Identity and Access Management), then from the left menu, click on the Access Management section's Policies link. Once in, click _"Create policy"_.

![Create S3 Bucket](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2023-03-11--08.png?raw=true)

Manually define your policy via the JSON tab as per the code below:

In your case, substitute _"youtube-video-analysis"_ for your buckets name.

**Note**: The second _"arn:aws:s3:::youtube-video-analysis/*"_ entry allows access to all subfolders and objects for the bucket. Both the bucket-level and sub-bucket-level entries for Resource are needed.

```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "ConsoleAccess",
            "Effect": "Allow",
            "Action": [
                "s3:*"
            ],
            "Resource": [
                "arn:aws:s3:::youtube-video-analysis",
                "arn:aws:s3:::youtube-video-analysis/*"
            ]
        }
    ]
}
```

This will update the Visual editor with the equivalent access levels.

In this example, I have named the policy "Custom_IAM_Policy_For_S3_Bucket".

![Create IAM Policy](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2023-03-11--09.png?raw=true)

Navigate to the Access Management section in the left menu, and click the "Users" link, then open the AWS account you intend to use for your development work. Ideally, this should **not** be your root account, for security reasons.

Then edit the account to manually apply the custom policy you created. This will allow the user to bypass the block all public access restriction placed on the bucket.

In my example, I have a user named "Python" that I added the policy to.

![Apply IAM Policy to a User](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2023-03-11--10.png?raw=true)

Ensure you have both the Access Key ID and Secret Access Key for that User account, as we'll need it in the next section.

This can be found under the User's account, and by navigating to the _"Security credentials"_ tab, under the _"Access Keys"_ section. The Access Key ID will be visible there, and you can generate the Secret Access Key from it.

![AWS Access Keys](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2023-03-11--11.png?raw=true)


## Step 6. Install the AWS CLI (Command Line Interface)

Install the AWS' command line interface, as this is where we will save our credentials such that we do not need to inline embed them in our project files.

* **[https://docs.aws.amazon.com/cli/v1/userguide/install-windows.html#msi-on-windows](https://docs.aws.amazon.com/cli/v1/userguide/install-windows.html#msi-on-windows)**

![Install AWS CLI](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2023-03-11--12.png?raw=true)

Open a new Command Prompt, then type:

```
aws configure
```

Then set your AWS Access Key ID and AWS Secret Access Key. You can leave the default region name and output format as blank by Entering through.

![Configure AWS CLI](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2023-03-11--13.png?raw=true)

From that, AWS CLI will save a config and credentials file that stores your keys locally.

![Configure AWS CLI](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2023-03-11--14.png?raw=true)


## Step 7. Test 1: Python (via Jupyter) upload files to AWS S3 Bucket

Open a new Command Prompt window, then install the following dependencies:

```
pip install pytube
pip install boto3
```

![pip installs](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2023-03-11--15.png?raw=true)


Create a folder in the directory where your (perhaps still Untitled) Jupyter Notebook file is, then add files you want to upload into S3 into that new folder.

In this example, I created a folder named "filestoupload" and added animated gifs of my project previews into it.

![Files for testing S3 upload](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2023-03-11--16.png?raw=true)

Now let's create code to upload all the files in that subfolder into a subfolder in the S3 bucket.

```
from pytube import YouTube
import boto3
import os

local_subfolder = os.getcwd().replace('\\','/')+'/filestoupload'
aws_s3_bucket_name = "youtube-video-analysis"
aws_s3_bucket_subfolder = 'animated_gifs'

files_to_upload = os.listdir(local_subfolder)

def UploadToS3Bucket(file_to_load):
    full_file_path = local_subfolder + '/' + file_to_load
    s3 = boto3.client('s3')
    s3.upload_file(
        Filename=full_file_path,
        Bucket=aws_s3_bucket_name,
        Key=aws_s3_bucket_subfolder + '/' + file_to_load,
    )
    os.remove(full_file_path)

for i in files_to_upload:
    UploadToS3Bucket(i)
```

![Python code to upload to S3](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2023-03-11--17.png?raw=true)

We can see the files we had locally are now gone...

![Files removed from local folder](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2023-03-11--18.png?raw=true)


...and our S3 bucket now has the files in the new subfolder we defined.

![Files uploaded to S3 bucket](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2023-03-11--19.png?raw=true)


## Step 8. Test 2: Python (via Jupyter) download files from AWS S3 Bucket

I haven't been able to get the download from S3 to to a local subfolder, so the files just download to the same folder as the Python code below.

```
def ListS3BucketSubfolderContents():
    contents = []
    client = boto3.client('s3')

    response = client.list_objects_v2(
        Bucket=aws_s3_bucket_name,
        Prefix=aws_s3_bucket_subfolder + '/')

    for content in response.get('Contents', []):
        contents.append(content['Key'])
    
    return contents

aws_s3_bucket_subfolder_list = ListS3BucketSubfolderContents()

print(aws_s3_bucket_subfolder_list)

s3 = boto3.resource('s3')
bucket = s3.Bucket(aws_s3_bucket_name)

for i in aws_s3_bucket_subfolder_list:
    filename = i.split(f'{aws_s3_bucket_subfolder}/',1)[1]
    bucket.download_file(i,filename)
```

![Python code to download from S3](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2023-03-11--20.png?raw=true)

Files successfully downloaded from S3 bucket to locally.

![Files downloaded to script folder from S3](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2023-03-11--21.png?raw=true)



### References

* YouTube video **[Upload and Download files from AWS S3 Bucket using python 2022 by Code Bear](https://www.youtube.com/watch?v=K68AtRsNEyo)**

* Website **[How to Upload And Download Files From AWS S3 Using Python (2022) by Bex Tuychiev](https://towardsdatascience.com/how-to-upload-and-download-files-from-aws-s3-using-python-2022-4c9b787b15f2)**



Click **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2023-03-11.md)** for this post's markdown file in GitHub.