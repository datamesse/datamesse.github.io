---
title: International Marketplace dataset created with Integration Services
tag: logo-sqlserver
date: 12 January 2022
---

SQL Server Integration Services (SSIS) was used to create this fictional dataset, by merging Microsoft's *Wide World Importers* database and *Contoso* data warehouse, with Tableau's *Sample - APAC Superstore* dataset, with some data alterations.

![International Marketplace SSIS package](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--01.png?raw=true)

![International Marketplace in Power BI sales by city](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--00.png?raw=true)

**Final output**

You can download Excel copies of the completed dataset:

* Star schema (for Power BI data visualisation) **[download here](https://github.com/datamesse/data-visualisation-datasets/raw/main/International%20Marketplace%20sales/International%20Marketplace%20Normalised%20for%20Power%20BI.xlsx)**.

![International Marketplace: Normalised star schema](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--02.png?raw=true)

* Denormalised (for Tableau data visualisation) **[download here](https://github.com/datamesse/data-visualisation-datasets/raw/main/International%20Marketplace%20sales/International%20Marketplace%20Denormalised%20for%20Tableau.xlsx)**.

![International Marketplace: Denormalised](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--03.png?raw=true)

This blog post outlines the planning and challenges in making the dataset, which was my first attempt at creating an SSIS package from scratch. For information on the data flows and SQL scripts used in the package, visit the **[GitHub repository](https://github.com/datamesse/data-visualisation-datasets/tree/main/International%20Marketplace%20sales)**.

**REVIEWING THE ORIGINAL DATA SOURCES**

**APAC Superstore**

Tableau's APAC Superstore dataset can be found and extracted from Tableau Desktop's *Saved Data Sources*.

 - Customers: 795 (by name), 5,220 (by name and city)
 - Products: 1,913
 - Countries: 23
 - Cities: 537
 - Sales records: 10,925
 - Sales years: 2018 to 2021

This is a succinct and denormalised dataset, where customers have no personal locations, as the same names are replicated across many countries. The names are also not very ethnically diverse, given the name of the dataset.

![APAC Superstore order map](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--04.png?raw=true)

![APAC Superstore data source](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--05.png?raw=true)

At the time of writing this blog post, the order dates covered are from 2018 to 2021. I'm highlighting that because I suspect that unlike the other datasets, Tableau's dataset has its dates automatically changed over time, and the SSIS package we create needs to accommodate for that.

The structure of the dataset merge will be based on APAC Superstore, since it has the least dimensions and facts compared to the Microsoft ones, which are proper databases.


**Wide World Importers**

Microsoft's Wide World Importers database backup (.bak) file can be downloaded from here:
**[https://github.com/Microsoft/sql-server-samples/releases/tag/wide-world-importers-v1.0](https://github.com/Microsoft/sql-server-samples/releases/tag/wide-world-importers-v1.0)**

 - Customers: 663
 - Products: 227
 - Countries: 1
 - Cities: 655
 - Sales records: 228,265
 - Sales years: 2013 to 2016

This is an extensive normalised dataset with customers based only in the United States. Each customer only exists in one city, but there are customer names for the same corporation in different cities e.g.
 - Tailspin Toys (Arietta, NY)
 - Tailspin Toys (Trentwood, WA)
These make up 60.6% of the customer records (402 out of 663).

![Wide World Importers customer map](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--06.png?raw=true)

Whilst 263 customer records have them based in a specific city, most have invoices for multiple other cities, presumably reflecting B2B sales.

![Wide World Importers tables](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--07.png?raw=true)


**Contoso BI Demo Dataset for Retail Industry**

Microsoft's Contoso BI Demo Dataset for Retail Industry data warehouse .bak file can be downloaded from here:
**[https://www.microsoft.com/en-us/download/details.aspx?id=18279](https://www.microsoft.com/en-us/download/details.aspx?id=18279)**

 - Customers: 18,785 (by name) 18,868 (by name and city)
 - Products: 2,516
 - Countries: 29
 - Cities: 476
 - Sales records: 3,324,410 (online sales only)
 - Sales years: 2007 to 2009

![Contoso customer map](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--08.png?raw=true)

This is the largest of the 3 datasets in terms of the number of sales records. Each sales row represents a single unit for a product, meaning aggregations need to be calculated where the same product is sold multiple times under the same order number.

So a lot of record removal is needed to bring down the dataset to a more manageable size, only using the latest order number for each customer (retaining multiple order line rows), which will leave a usable 19 thousand records.

 - Customers: 18,674 (by name) 18,753 (by name and city)
 - Products: 463
 - Countries: 29
 - Cities: 476
 - Sales records: 19,419
 - Sales years: 2007 to 2009

Contoso also has a lot of trailing spaces in values, which need to be trimmed as part of the ETL's data cleaning. Geographic corrections also need to be made, for example, the city Perth is incorrectly listed under South Australia.

![Contoso tables](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--09.png?raw=true)

The table below shows a summary of how the dataset columns align as-is, and the T-SQL needed to populate respective columns with existing data.

![Comparing columns across the 3 datasets](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--10.png?raw=true)

**CHALLENGES**

**Region inconsistency**

Wide World Importers has a comprehensive Country table for Region and Subregion. We will remap the countries from APAC Superstore and Contoso to those regions for consistency.

**Support for special characters**

A large number of products use commas, and many names, states, cities and the like, use special characters, which I found to store accurately in Unicode tab-delimited text files, as opposed to some other file format types which cannot render the characters correctly. So both the APAC Superstore, and the remapping files use this file format.

**Customer names are incoherent and do not reflect country-of-origin**

The APAC Superstore and Contoso datasets have customers outside the United States, but their names do not reflect those locations. Using my past experience with country-based name randomisation (plugging my previous blog post **[here](https://datamesse.github.io/#/post/1635598800)**), I created customer name re-mapping files **[here](https://github.com/datamesse/data-visualisation-datasets/tree/main/International%20Marketplace%20sales/text%20files%20for%20value%20remapping%20in%20SSIS)**.

Contoso also has company names with a missing space before the "Company" suffix. As these are just the city with "Company" suffixed, they are remapped with a person's name instead.

**Mostly United States sales**

Wide World Importers is a United States sales dataset, so the U.S. sales data from Contoso will be remapped to other countries, to give the final dataset more global reach. The geographic remapping file for this is **[here](https://github.com/datamesse/data-visualisation-datasets/tree/main/International%20Marketplace%20sales/text%20files%20for%20value%20remapping%20in%20SSIS)**.

**Product, Category/Subcategory, and Supplier remapping**

There are shared _Supplier_ names across Contoso and Wide World Importers, but the spelling needs to be amended so they align. World Wide Importers uses _Stock Groups_, where each product item can belong to multiple groups (1:M), whereas APAC Superstore and Contoso have 1:1 Category and Sub-Category for their products. So all of these have been remapped and re-classified as a whole. Remapping files are **[here](https://github.com/datamesse/data-visualisation-datasets/tree/main/International%20Marketplace%20sales/text%20files%20for%20value%20remapping%20in%20SSIS)**.

**Customer segmentation**

APAC Superstore defines its segmentation by Consumer, Home Office, and Corporate. International Marketplace merges Home Office with Consumer, as Contoso uses Person and Company for its segmentations. So these are easily remapped:

```
case
   when c.CustomerType = 'Person' then N'Consumer'
   when c.CustomerType = 'Company' then N'Corporate'
   else null end as "Segment"
```

World Wide Importers made the remapping simple as company names are suffixed with the office's city in parenthesis:

```
case
   when c.CustomerName like '%(%' then N'Corporate'
   else N'Consumer' end as "Segment"
```

**Ship Date missing**

Only APAC Superstore has this column, with no decent equivalents in Contoso and World Wide Importers. So we used T-SQL randomisation to populate this column.

As Contoso is a regional dataset, the _Ship Date_ is set to randomise up to 17 days after the _Order Date_.

```
convert(varchar, (dateadd(day,(abs(checksum(newid())) % 18),s.Datekey)),103) "Ship Date"
```

To give some variation toward shorter ship times, World Wide Importers randomises up to 7 days after the _Order Date_.

```
convert(varchar,dateadd(day,(abs(checksum(newid())) % 8),o.OrderDate),103) "Ship Date",
```

**Ship Mode**

Like _Ship Date_, only APAC Superstore has this column. This value is randomised in different phases for Contoso and Wide World Importers.

The first pass involves checking where the _Order Date_ and _Ship Date_ are the same value, and approximately 66.6% of them are randomised to be treated as "Same Day" shipment.

```
update zs2
   set zs2.FromShipMode = 'Same Day'
   from z_sales zs2
   inner join
      (select cast((ABS(CHECKSUM(NewId()))%3) as bigint) as "random", zs1.SalesStagingID
       from  z_sales zs1
       where zs1.FromOrderDate = zs1.FromShipDate and zs1.FromShipMode is null) as rzs
      on zs2.SalesStagingID = rzs.SalesStagingID
   where rzs.random < 2;
```

As randomisation in SQL Server seems to result in near-equal value distribution, all remaining sales are allocated _Ship Mode_ based on a 0 to 9 randomisation with categories disproportionately assigned to integer ranges. 

```
update zs2
   set zs2.FromShipMode =
      case
         when rzs.random between 0 and 5 then N'Standard Class'           
         when rzs.random between 6 and 8 then N'Second Class'
         else N'First Class' end
   from z_sales zs2
   inner join
      (select cast((ABS(CHECKSUM(NewId()))%10) as bigint) as "random", zs1.SalesStagingID
       from  z_sales zs1
       where zs1.FromShipMode is null) as rzs
      on zs2.SalesStagingID = rzs.SalesStagingID;
```

**Different date ranges**

Aligning the order dates across the datasets to cover the same years is done with an initial randomised approximately 50:50 split of the sales data to be moved to 2022.

```
update zs2
   set
      zs2.toshipdate = dateadd(year,(2022-cast(convert(varchar(4),zs2.fromshipdate,112) as int)),zs2.fromshipdate),
      zs2.toorderdate = dateadd(year,(2022-cast(convert(varchar(4),zs2.fromorderdate,112) as int)),zs2.fromorderdate)
   from z_sales zs2
   inner join
      (select cast((ABS(CHECKSUM(NewId()))%2) as bigint) as "random", zs1.SalesStagingID
       from  z_sales zs1 where zs1.toorderdate is null and zs1.toshipdate is null) as rzs
       on zs2.SalesStagingID = rzs.SalesStagingID
   where rzs.random < 1;
```

With the remainder set to the following year. In hindsight, it would have been better to implement this as a package-scoped variable to allow the user to place any year they wanted.

```
update zs
   set
      zs.toshipdate = dateadd(year,(2022-cast(convert(varchar(4),zs.fromshipdate,112) as int)+1),zs.fromshipdate),
      zs.toorderdate = dateadd(year,(2022-cast(convert(varchar(4),zs.fromorderdate,112) as int)+1),zs.fromorderdate)
   from z_sales zs
   where zs.toorderdate is null and zs.toshipdate is null;
```

**OTHER LESSONS LEARNED**

**Creating bins for histograms**

APAC Superstore uses a Profit (bin) field to round down profits to nearest $200, including going into negatives. To apply that to the invoice line profit field in Wide World Importers, the basic select for this would be:

```
floor( il.LineProfit / 200 ) * 200
```

**For Flat File connections, Visual Studio may retain cached file loads**

Simply running the Play button won't reflect changes to the file, but I found closing and reopening Visual Studio before hitting play to work in some cases.

**Unicode and non-Unicode conversion**

For this error message that appears when using OLE DB Source: _"Column cannot convert between unicode and non-unicode string data types."_
Right-click the OLE DB Source element > Show Advanced Editor > navigate to "Input and Output Properties" tab > expand "OLE DB Source Output" > expand "Output Columns" > check the DataType field for each column reported, and adjust where needed.

**Flat file data sources, with double quotations as string delimiter**

To get rid of them, the following can be used in Derived Column transformations **[https://stackoverflow.com/questions/65176461/ssis-flat-file-source-get-rid-of-some-embedded-unwanted-double-quotes](https://stackoverflow.com/questions/65176461/ssis-flat-file-source-get-rid-of-some-embedded-unwanted-double-quotes)**.

```
REPLACE([4-2-27  Data Conversion 3].Product,"\"","")
```
**Note:** There's a backspace before the second double quotation mark, in order to escape the double quotation. Which is ironic since markdown files do the same thing any you may not see it in the code above.


**SSIS Derived Columns can truncate flat file columns**

This can be resolved by right-clicking the Derived Column element > Show Advanced Editor, see above Unicode error for same steps, but this time involves changing the length for Derived Column Output **[https://nishantrana.me/2019/05/08/error-the-derived-column-failed-because-truncation-occurred-and-the-truncation-row-disposition-on-derived-column-outputsderived-column-output-columnsfilepath-specifies-failure-on-truncat/](https://nishantrana.me/2019/05/08/error-the-derived-column-failed-because-truncation-occurred-and-the-truncation-row-disposition-on-derived-column-outputsderived-column-output-columnsfilepath-specifies-failure-on-truncat/)**.

**Creating text file-based remapping files may not work if NULLs are involved**

Remapping files for Wide World Importers had to be separated from the other 2 data sources because it alone had null on categories. Otherwise produced whitespace that could not be trimmed because it used an ASCII 194 160 combination **[https://stackoverflow.com/questions/42424555/trim-whitespace-ascii-character-194-from-string](https://stackoverflow.com/questions/42424555/trim-whitespace-ascii-character-194-from-string)**.

**Data Flow encapsulation**

I noticed a strange behaviour where putting the flows to populate both the main State and City tables inside a single dataflow populates just State, but not the City table, but if the flows were separated into their own data flows, then they each populate. Currently suspecting it may be because they share the same OLE DB Source (though different SQL was used).


Click **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2022-01-12.md)** for this post's markdown file in GitHub.