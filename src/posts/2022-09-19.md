---
title: How to webscrape a YouTube channel's video titles using Python and Visual Studio Code
tech: logo-python
tags: ["Python"]
date: 19 September 2022
---

This post shows how to webscrape all the video titles from a YouTube channel, despite the 50 result per request limit of the YouTube API version 3 (as at time of writing), using Visual Studio Code, and a mix of Python libraries.


## Step 1. Log into console.cloud.google.com and create a new project.

In the example below, I created a new project named *"Le Wagon YouTube Channel"*, but the name you use may not particularly matter, as the purpose of creating the project is just to get an API key for web scraping.

As at the time of writing, you can get a Twitter Developer account off the back of an existing Twitter account or create a new one, but you will need to provide your mobile phone number for verification.

* **[https://console.cloud.google.com/](https://console.cloud.google.com/)**

![Google Cloud Console: new project](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--01.png?raw=true)


## Step 2. Enable the YouTube Data API v3 and create an API key

From within the project, go to the APIs and services section > Library > search for YouTube Data API v3 > Enable. 

![Google Cloud Console: enable YouTube API](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--02.png?raw=true)

Once enabled, go to the APIs and services section > Credentials > create API Key

![Google Cloud Console: generate API key](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--03.png?raw=true)


## Step 3. Install Python, create a virtual environment, and install libraries

Install Python
* **[https://www.python.org/downloads/windows/](https://www.python.org/downloads/windows/)**

Create a folder for your Python project and open it in Visual Studio Code.
In this example, the folder name is *Project* and is on the Desktop.

Create your virtual environment as a subfolder of your project.

```
python -m venv C:\Users\Administrator\Desktop\Project\venv
```

Install the Python Extension for Visual Studio Code.

![Visual Studio Code: create and activate virtual environment](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--04.png?raw=true)

You can find more detailed screenshot steps of the above from a previous blog post:
* **[Add forecasts from Python using Visual Studio Code to Power BI](https://datamesse.github.io/#/post/1650117600)**


From within the Python virtual environment, install the required dependencies for the script.

```
pip install google-api-python-client
pip install pandas
pip install numpy
```

![Visual Studio Code: install libraries](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--05.png?raw=true)


## Step 4. Find the YouTube channel's ID via browser source code

Navigate to the main page for the YouTube channel you want to scrape, then right-click > View page source.

![Browser: YouTube channel](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--06.png?raw=true)

Press Ctrl + F to find the *externalId* attribute, as its value is the channel ID.

![Browser: YouTube channel ID](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--07.png?raw=true)

In this example, Le Wagon's channel ID is **'UChCDYcBCrb8tuPAO6e0P-Hw'**

Before creating the python file to add and run the script, I've outlined how it works below.

If you do not want an overview how the code works and just want to implement the code, then skip the *Code explanation* to Step 5, otherwise you can continue below:


## Code explanation

The first half of the code is mostly based on this YouTube tutorial by techTFQ:

* YouTube video **["Python Project to Scrape YouTube using YouTube Data API | Analyze and Visualize YouTube data" by Thoufiq Mohammed](https://www.youtube.com/watch?v=SwSbnmqk3zY)**

However, even though I copied the code from the tutorial, I found that I was unable to pull more than 50 records into a single file, despite the video showing it was capable of doing that.

I was unsure if this was due to issues with the code demonstrated, or perhaps changes to YouTube's API, such as regarding immediate subsequent calls as part of the 50 result limit, that made the code no longer capable of pulling more than 50 videos worth of data.

Rather than spend time trying to figure that out, I instead took the approach of running a loop that would send separate API requests and store each result into individual temporary files, then use Python to append all the results into a single file.

### Use the channel ID to retrieve the uploads ID

This is the first part of the code, where you would need to substitute the ?????????? with your API key and the channel ID retrieved in earlier steps. It uses that information to pull the uploads ID for the channel, which lists all the video IDs.

```
from googleapiclient.discovery import build
import pandas as pd
import numpy as np
import csv
import glob
import re
import json

api_key = '??????????'
service = build('youtube', 'v3', developerKey=api_key)
channel_id = '??????????'



# EXTRACT YOUTUBE CHANNEL'S UPLOADS ID FOR FULL PLAYLIST

def get_uploads_id(service, channel_id):

    request = service.channels().list(
        part='contentDetails',
        id=channel_id
    )
    response = request.execute()

    for channel in response['items']:
        channelstats = channel['contentDetails']['relatedPlaylists']['uploads']

    return channelstats

uploadsID = (get_uploads_id(service, channel_id))
```

![Visual Studio Code: Python part 1](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--08.png?raw=true)


### Store all the video ids from the channel

This second part of the code is pretty much straight from Thoufiq Mohammed's YouTube tutorial, and it retrieves all of the video IDs for the selected channel, then stores them into an array. In order to account for the 50 result per request limit, Thoufiq's code uses page tokens and a for loop to go through each "page" of 50 video IDs.

* YouTube video **["Python Project to Scrape YouTube using YouTube Data API | Analyze and Visualize YouTube data" by Thoufiq Mohammed](https://www.youtube.com/watch?v=SwSbnmqk3zY)**

```
# EXTRACT YOUTUBE VIDEO IDS TO SEPARATE CSV FILES

def get_video_ids(service, uploadsID):

    request = service.playlistItems().list(
        part='contentDetails',
        playlistId = uploadsID,
        maxResults = 50)
    response = request.execute()

    video_ids = []

    for i in range(len(response['items'])):
        video_ids.append(response['items'][i]['contentDetails']['videoId'])

    next_page_token = response.get('nextPageToken')
    more_pages = True

    while more_pages:
        if next_page_token is None:
            more_pages = False
        else:
            request = service.playlistItems().list(
                        part='contentDetails',
                        playlistId = uploadsID,
                        maxResults = 50,
                        pageToken = next_page_token)
            response = request.execute()   

            for i in range(len(response['items'])):
                video_ids.append(response['items'][i]['contentDetails']['videoId'])

            next_page_token = response.get('nextPageToken')
 
    return (video_ids)

video_ids = get_video_ids(service, uploadsID)
```

![Visual Studio Code: Python part 2](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--09.png?raw=true)

### Extract all the video ids into separate csv files

Thoufiq's code stores data for the API requests with variable arrays, as you would typically expect. But as it was not working for me, rather than figure out what other variation of similar in-memory based API requests would work, I took a primitive approach of separating the array into csv files with 50 video IDs each.

Whether it's because of the different approach to pulling the data for the API requests, or its implied slower performance for subsequent API requests, I placed my chances on this allowing me to get what I needed.

This third part of the code is simple:

```
chunk_size = 50

video_id_chunks = [video_ids[i:i + chunk_size] for i in range(0, len(video_ids), chunk_size)]

for i in range(len(video_id_chunks)):

    filename = 'video_ids_' + str(i) + '.csv'
    file = open(filename, 'w', newline='', encoding='utf-8')
    writer = csv.writer(file)
    writer.writerow(video_id_chunks[i])
    file.close()
```

![Visual Studio Code: Python part 3a](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--10.png?raw=true)

Here you can see an example of what the comma separated files look like:

![Visual Studio Code: Python part 3b](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--11.png?raw=true)

### Loop each csv for YouTube API requests then store results into separate json files

This fourth part of the code creates a glob variable to list all the temp files containing the video IDs, then loops it for each file, which in turn each file has all its videos looped through to retrieve the video details via the API such as title and publish date, and exports each result set to separate temporary json files.

There is one issue here that I have not yet solved. Note that the dictionary keys for count of views, likes, and comments are commented out.

It is possible to uncomment these and retrieve results. However, in my testing I found that some videos returned a KeyError that stopped the whole process. Typically the way to deal with this is to create a try excepts, but as I am still a Python novice, could not figure out how to design it for a dict(), and didn't really need these fields, I didn't spend time to try to solve that yet.

```
# EXTRACT YOUTUBE VIDEO DETAILS TO SEPARATE JSON FILES

all_files_ids = glob.glob("video_ids_*.csv")

for i in range(len(all_files_ids)):

    with open(all_files_ids[i]) as csvfile:
        video_ids_for_details = np.loadtxt(csvfile, delimiter=',', dtype='str')

    def get_video_details(service, video_ids_for_details):

        all_video_stats = []

        for i in range(0, len(video_ids_for_details), 50):
            request = service.videos().list(
                part='snippet,statistics',
                id=','.join(video_ids_for_details[i:i+50]))
            response = request.execute()

            for video in response['items']:
                video_stats = dict(Title = video['snippet']['title'],
                                   Video_id = video['id'],
                                   # Views = video['statistics']['viewCount'],
                                   # Likes = video['statistics']['likeCount'],
                                   # Comments = video['statistics']['commentCount'],
                                   Published_date = video['snippet']['publishedAt'])
                all_video_stats.append(video_stats)

            return all_video_stats

    csv_video_list = pd.DataFrame(get_video_details(service, video_ids_for_details))
    csv_video_list.to_json(re.sub('_ids_','_details_', re.sub('.csv', '', all_files_ids[i])) + '.json', orient='records')
```

![Visual Studio Code: Python part 4a](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--12.png?raw=true)

Here you can see an example of what the json files look like (though honestly I applied the pretty-print function from Notepad++ to clean it up a little):

![Visual Studio Code: Python part 4b](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--13.png?raw=true)

### Combine the temporary json files into a single json file

The fifth and final step of the code is to combine all the data from each of the temporary json files into a single json file output. Looking back on this, I should have included a clean up script component to delete the temporary csv and json files.

```
# COMBINE YOUTUBE VIDEO DETAILS INTO A SINGLE JSON FILE

all_files_details = glob.glob("video_details_*.json")

def merge_JsonFiles(filename):
    result = list()
    for f1 in filename:
        with open(f1, 'r') as infile:
            result.extend(json.load(infile))

    with open('compileddata.json', 'w') as output_file:
        json.dump(result, output_file)

merge_JsonFiles(all_files_details)
```

![Visual Studio Code: Python part 5](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--14.png?raw=true)


## Step 5. Install Python, create a virtual environment, and install libraries

Create a new Python file in your project directory, in my case it is named *extract-script.py*.

Then copy-and-paste the completed code below (substituting your API key and channel ID), then click Run Python File.

![Visual Studio Code: Run Python File](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--15.png?raw=true)

The completed json output is the file named *compileddata.json*.

![Visual Studio Code: Output json file](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--16.png?raw=true)

**Note:** As-is, this code only extracts the YouTube video IDs, titles, and publish date. You can try to uncomment the view, like, and comment count, but there is a chance the script may fail due to KeyErrors.
That caveat aside, you now have all the video titles for a YouTube channel, and can explore more fields to pull via the YouTube Data API v3 official site.

```
from googleapiclient.discovery import build
import pandas as pd
import numpy as np
import csv
import glob
import re
import json

api_key = '??????????'
service = build('youtube', 'v3', developerKey=api_key)
channel_id = 'UChCDYcBCrb8tuPAO6e0P-Hw'



# EXTRACT YOUTUBE CHANNEL'S UPLOADS ID FOR FULL PLAYLIST

def get_uploads_id(service, channel_id):

    request = service.channels().list(
        part='contentDetails',
        id=channel_id
    )
    response = request.execute()

    for channel in response['items']:
        channelstats = channel['contentDetails']['relatedPlaylists']['uploads']

    return channelstats

uploadsID = (get_uploads_id(service, channel_id))



# EXTRACT YOUTUBE VIDEO IDS TO SEPARATE CSV FILES

def get_video_ids(service, uploadsID):

    request = service.playlistItems().list(
        part='contentDetails',
        playlistId = uploadsID,
        maxResults = 50)
    response = request.execute()

    video_ids = []

    for i in range(len(response['items'])):
        video_ids.append(response['items'][i]['contentDetails']['videoId'])

    next_page_token = response.get('nextPageToken')
    more_pages = True

    while more_pages:
        if next_page_token is None:
            more_pages = False
        else:
            request = service.playlistItems().list(
                        part='contentDetails',
                        playlistId = uploadsID,
                        maxResults = 50,
                        pageToken = next_page_token)
            response = request.execute()   

            for i in range(len(response['items'])):
                video_ids.append(response['items'][i]['contentDetails']['videoId'])

            next_page_token = response.get('nextPageToken')
 
    return (video_ids)

video_ids = get_video_ids(service, uploadsID)

chunk_size = 50

video_id_chunks = [video_ids[i:i + chunk_size] for i in range(0, len(video_ids), chunk_size)]

for i in range(len(video_id_chunks)):

    filename = 'video_ids_' + str(i) + '.csv'
    file = open(filename, 'w', newline='', encoding='utf-8')
    writer = csv.writer(file)
    writer.writerow(video_id_chunks[i])
    file.close()



# EXTRACT YOUTUBE VIDEO DETAILS TO SEPARATE JSON FILES

all_files_ids = glob.glob("video_ids_*.csv")

for i in range(len(all_files_ids)):

    with open(all_files_ids[i]) as csvfile:
        video_ids_for_details = np.loadtxt(csvfile, delimiter=',', dtype='str')

    def get_video_details(service, video_ids_for_details):

        all_video_stats = []

        for i in range(0, len(video_ids_for_details), 50):
            request = service.videos().list(
                part='snippet,statistics',
                id=','.join(video_ids_for_details[i:i+50]))
            response = request.execute()

            for video in response['items']:
                video_stats = dict(Title = video['snippet']['title'],
                                   Video_id = video['id'],
                                   # Views = video['statistics']['viewCount'],
                                   # Likes = video['statistics']['likeCount'],
                                   # Comments = video['statistics']['commentCount'],
                                   Published_date = video['snippet']['publishedAt'])
                all_video_stats.append(video_stats)

            return all_video_stats

    csv_video_list = pd.DataFrame(get_video_details(service, video_ids_for_details))
    csv_video_list.to_json(re.sub('_ids_','_details_', re.sub('.csv', '', all_files_ids[i])) + '.json', orient='records')



# COMBINE YOUTUBE VIDEO DETAILS INTO A SINGLE JSON FILE

all_files_details = glob.glob("video_details_*.json")

def merge_JsonFiles(filename):
    result = list()
    for f1 in filename:
        with open(f1, 'r') as infile:
            result.extend(json.load(infile))

    with open('compileddata.json', 'w') as output_file:
        json.dump(result, output_file)

merge_JsonFiles(all_files_details)
```



### References

* YouTube video **["Python Project to Scrape YouTube using YouTube Data API" | Analyze and Visualize YouTube data by Thoufiq Mohammed](https://www.youtube.com/watch?v=SwSbnmqk3zY)**

* YouTube video **["How to split a list into evenly sized chunks in Python | Python Tutorial" by Jie Jenn](https://www.youtube.com/watch?v=SuEk_TBkReQ)**

* YouTube video **["How to Read Multiple CSV Files in Python | For-Loop + 2 More" by Business Science](https://www.youtube.com/watch?v=TN_Cvyq_rxE)**

* Soft Hints blog post **[How to Merge Multiple JSON Files Into Pandas DataFrame blog post by John D K](https://softhints.com/merge-multiple-json-files-pandas-dataframe/)**

* Stack Overflow Question **[KeyError: 'commentCount' using Youtube API in Python](https://stackoverflow.com/questions/70103870/keyerror-commentcount-using-youtube-api-in-python)**


Click **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2022-09-19.md)** for this post's markdown file in GitHub.