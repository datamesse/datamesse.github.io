[{"id":1650117600,"title":"Add forecasts from Python using Visual Studio Code to Power BI","tag":"logo-python","date":"April 17, 2022","content":"\r\nThis is a record of my first attempt at utilising Python for predictive analytics and embedding it into Power BI, using the International Marketplace dataset created using SQL Server Integration Services from my [previous blog post](https://datamesse.github.io/#/post/1641906000).\r\n\r\nThe normalised version of the output is used, which you can download from here:\r\n\r\n**[https://github.com/datamesse/data-visualisation-datasets/blob/main/International%20Marketplace%20sales/International%20Marketplace%20Normalised%20for%20Power%20BI.xlsx?raw=true](https://github.com/datamesse/data-visualisation-datasets/blob/main/International%20Marketplace%20sales/International%20Marketplace%20Normalised%20for%20Power%20BI.xlsx?raw=true)**\r\n\r\n\r\n\r\n## Step 1. Setup Visual Studio Code to run Python\r\n\r\n### Install the Python extension for Visual Studio Code\r\n\r\nInstall the Microsoft verified Python extension.\r\n\r\n![Visual Studio Code Python extension](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--01.png?raw=true)\r\n\r\n\r\n\r\n### Create a Virtual Environment to allow import of Python modules\r\n\r\nThe set up of a virtual environment is so that you can install as many libraries for data analysis and visualisation using Python (i.e. model training and testing), which may be more than the actual libraries needed for the code you ultimately embed in your Power BI report.\r\n\r\nIf you want to link your project to GitHub, follow the standard setup steps, i.e.:\r\n* Create a public repository in GitHub.com\r\n* Create a local repository for the project (Code > Open with GitHub Desktop)\r\n\r\nFrom Visual Studio Code, open a cmd terminal\r\n\r\n![Open a command terminal, part 1](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--02.png?raw=true)\r\n\r\nVisual Studio opens a PowerShell terminal by default, so switch it to Command Prompt.\r\n\r\n![Open a command terminal, part 2](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--03.png?raw=true)\r\n\r\nThe reason for this is PowerShell will by default block running potentially harmful scripts.\r\nSo you should be using cmd terminal from within Visual Studio Code, rather than the PowerShell terminal.\r\n\r\n![Open a command terminal, part 3](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--04.png?raw=true)\r\n\r\nFrom the Visual Studio Code cmd terminal, use the following command structure:\r\n\r\n```\r\npython -m venv yourpythonprojectfolder\\yourvenvname\r\n```\r\n\r\nFor example:\r\n```\r\npython -m venv C:\\Project\\python-for-power-bi\\venv\r\n```\r\n\r\n![Create virtual environment, part 1](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--05.png?raw=true)\r\n\r\nThe venv virtual environment will appear as a folder under VS Code's Explorer pane.\r\n\r\n![Create virtual environment, part 2](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--06.png?raw=true)\r\n\r\n\r\n\r\n### Activating the Virtual Environment\r\n\r\nCommand structure:\r\n```\r\nyourvenvname\\Scripts\\activate\r\n```\r\n\r\nFor example:\r\n```\r\nvenv\\Scripts\\activate\r\n```\r\n\r\n![Run virtual environment](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--07.png?raw=true)\r\n\r\n**Note:** When you close Visual Studio Code, the environment will be stopped. To activate it again, use the above command from a cmd terminal in Visual Studio Code.\r\n\r\n\r\n\r\n### Add the Virtual Environment to .gitignore\r\n\r\nLooking at the previous screenshot, you can see the Source Control icon has over 900 items pending change due to the venv environment. The Python modules that will be later installed for model training and testing, will also be added to that folder.\r\n\r\nSo to avoid pushing all those unnecesary files to GitHub (which _will_ stall it), create a new .gitignore file in your folder's directory and add venv (or whatever your virtual environment’s folder name is), inside that .gitignore file.\r\n\r\nYou will notice the number of pending changes are cut down, and your virtual environment's folder should be greyed out.\r\n\r\n![Add virtual environment to .gitignore](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--08.png?raw=true)\r\n\r\n\r\n\r\n### Deactivate and delete Virtual Environment\r\n\r\nTo deactivate the environment (can be re-activated again later), use the deactivate command.\r\n\r\n```\r\ndeactivate\r\n```\r\n\r\nIf you need to delete the environment, use the following code via a PowerShell terminal if its folder was named venv, otherwise replace venv with the folder name you used.\r\n\r\n```\r\nrm -r venv\r\n```\r\n\r\n\r\n\r\n### Import Python libraries\r\n\r\nAfter performing the steps above (minus deactivating then deleting the environment), you can now install Python libraries a.k.a. modules. Be sure the (venv) prefix is displayed before running, so you know it installs to the virtual environment.\r\n\r\nCommand structure:\r\n```\r\npip install pythonmodulename\r\n```\r\n\r\nFor example:\r\n```\r\npip install pandas\r\npip install numpy\r\npip install scikit-learn\r\npip install matplotlib\r\npip install seaborn\r\n```\r\n\r\n![Install Python libraries](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--09.png?raw=true)\r\n\r\nYou may encounter a pip message preventing installation of subsequent libraries due to version incompatibility. In my case, the message asked to run the following, which you may need to do for your own folder.\r\n\r\n```\r\nc:\\project\\python-for-power-bi\\venv\\scripts\\python.exe -m pip install --upgrade pip\r\n```\r\n\r\nYou can test the environment and Python libraries, by creating a Python script with .py file format extension in your main folder with the following code.\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\njusticeLeague = np.array([\r\n   [1, 'Superman', 1938],\r\n   [2, 'Batman', 1939],\r\n   [3, 'Wonderwoman', 1941],\r\n   [4, 'Flash', 1940,]\r\n])\r\n\r\ncolumnNames = [\r\n   'League ID',\r\n   'Codename',\r\n   'First Appearance'\r\n]\r\n\r\ndf = pd.DataFrame(data = justiceLeague, columns = columnNames)\r\nprint(df)\r\n```\r\n\r\nThen click Run to render the script. If the data displays, then Python is working from the virtual environment.\r\n\r\n![Test Python script displays](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--10.png?raw=true)\r\n\r\n\r\n\r\n### Clearing terminal\r\n\r\nVisual Studio Code has removed the hotkey shortcut for clearing console, which is going to be regularly used for trial-and-error coding and displaying results.\r\n\r\nTo create a shortcut for it, from Visual Studio Code navigate to File > Preferences > Keyboard Shortcuts, then search for the command *workbench.action.terminal.clear*, and apply your keybinding, for example Ctrl + K.\r\n\r\n![Set up shortcut to clear terminal](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--11.png?raw=true)\r\n\r\n\r\n\r\n### Allow plotting to display within Visual Studio Code\r\n\r\nTo allow Visual Studio Code to display plots i.e. visualisations, place this tag at the top of your Python script: #%%\r\n\r\nIt will automatically add a paired tag to the end of the script.\r\n\r\nFor example, replace your test Python file's contents with this:\r\n\r\n```\r\n#%%\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nmonthlySales = np.array([\r\n   [1, 21],\r\n   [2, 17],\r\n   [3, 25],\r\n   [4, 29],\r\n   [5, 15],\r\n   [6, 11],\r\n   [7, 18],\r\n   [8, 13],\r\n   [9, 7],\r\n   [10, 3],\r\n   [11, 14],\r\n   [12, 32]\r\n])\r\n\r\ncolumnNames = [\r\n   'Month',\r\n   'Sales'\r\n]\r\n\r\ndf = pd.DataFrame(data = monthlySales, columns = columnNames)\r\nmonth = df.loc[:, 'Month'].values\r\nsales = df.loc[:, 'Sales'].values\r\nplt.plot(month, sales)\r\n```\r\n\r\nThen click the \"Run Cell\" hyperlink at the top of the script. If it isn't already installed, ipykernel package will prompt to be installed in your virtual environment, which you will need.\r\n\r\n![Install ipykernel](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--12.png?raw=true)\r\n\r\nOnce complete, the plot should display.\r\n\r\n![Data visual in Visual Studio Code](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--13.png?raw=true)\r\n\r\n\r\n\r\n## Step 2. Import data and do preliminary visualisation\r\n\r\nFor the moment, I am choosing to just look at the OrderDate and Profit columns i.e. ignoring other independent variables.\r\nAlthough the following can easily be done in an Excel or Power BI linechart, using Python I want to see the monthly profits.\r\n\r\nFirst by listing out the results:\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndata = pd.read_excel (r'https://github.com/datamesse/data-visualisation-datasets/blob/main/International%20Marketplace%20sales/International%20Marketplace%20Normalised%20for%20Power%20BI.xlsx', sheet_name='FactSales', parse_dates = True)\r\ndf = pd.DataFrame(data, columns= ['OrderDate','Profit'])\r\ndf.set_index('OrderDate')\r\n\r\n# create YearMonth column, and total column by YearMonth\r\ndf['YearMonth'] = df['OrderDate'].dt.to_period(\"M\")\r\ngroupby = df.groupby(['YearMonth'])\r\ndf['MonthlyProfit'] = groupby['Profit'].transform(np.sum)\r\n# remove OrderDate and Profit column so duplicate rows (of YearMonth and MonthlyProfit) becomes evident\r\n# .duplicated() checks if entire row's values match a previous row, and returns True if the case except for the first occurrence\r\ndf = df.drop(columns=['OrderDate','Profit'])\r\ndf.drop_duplicates(keep='first', inplace = True)\r\ndf.set_index('YearMonth')\r\ndf.index.freq = 'MS'\r\n\r\nprint(df)\r\n```\r\n\r\n![List of monthly profit](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--14.png?raw=true)\r\n\r\nThen adding the following code after that to plot/visualise it:\r\n\r\n```\r\nimport matplotlib.pyplot as plt\r\n\r\ndf['YearMonth'] = pd.to_datetime(df['YearMonth'].astype(str) + '-01')\r\nyearMonth = df.loc[:, 'YearMonth'].values\r\nmonthlyProfit = df.loc[:, 'MonthlyProfit'].values\r\nplt.plot(yearMonth, monthlyProfit, c = 'b', marker = '.', markersize = 10)\r\nplt.xticks(fontsize = 8)\r\nplt.yticks(fontsize = 8)\r\nplt.xlabel('Year-Month', fontsize = 10)\r\nplt.ylabel('Millions $', fontsize = 10)\r\n```\r\n\r\n![Plot of monthly profit](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--15.png?raw=true)\r\n\r\n\r\n\r\n## Step 3. Training and testing prediction models\r\n\r\nThis is the point where you would train and test models, with the ultimate objective of generating a list of forecasted figures to load into Power BI as a separate data file. The reason for this is it's better than embeddding the Python model inside the Power BI report directly, especially as complexity and result sets increase.\r\n\r\nAs at the time of writing, I am still aways off from being any kind of statistician, so I'll defer to using the exponential smoothing method Absent Data took in his blog post:\r\n\r\n**[https://www.absentdata.com/power-bi/forecasting-with-python-and-power-bi/](https://www.absentdata.com/power-bi/forecasting-with-python-and-power-bi/)**\r\n\r\nLike Absent Data, I've started off with a daily dataset that I want aggregated to monthly values (as the Power BI report I'll be designing won't be allowing drill down past monthly level for the forecast).\r\n\r\nFrom Step 2, you can see that I've needed to do some tinkering to get the monthly totals so that I could visualise them. That code will need to be incorporated into what we add into Power BI later, as it'll be a needed prep work step, otherwise I end up with forecasted values for Day 1 for each month, rather than the sum of all days for each month.\r\n\r\nFrom Visual Studio Code, the results look like this when listed:\r\n\r\n```\r\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\r\nmodel = ExponentialSmoothing(df['MonthlyProfit'],trend='mul',seasonal='mul',seasonal_periods=12).fit()\r\nrange = pd.date_range('01-01-2024', periods=12, freq='MS')\r\npredictions = model.forecast(12)\r\npredictions_range = pd.DataFrame({'MonthlyProfit':predictions,'YearMonth':range})\r\n\r\nprint(predictions_range)\r\n```\r\n\r\n![List of forecasted monthly profit](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--16.png?raw=true)\r\n\r\nThen when visualised, show a similar pattern to the original dataset:\r\n\r\n```\r\nimport matplotlib.pyplot as plt\r\n\r\nyearMonthForecast = predictions_range.loc[:, 'YearMonth'].values\r\nmonthlyProfitForecast = predictions_range.loc[:, 'MonthlyProfit'].values\r\ndf['YearMonth'] = pd.to_datetime(df['YearMonth'].astype(str) + '-01')\r\nyearMonth = df.loc[:, 'YearMonth'].values\r\nmonthlyProfit = df.loc[:, 'MonthlyProfit'].values\r\nplt.plot(yearMonth, monthlyProfit, c = 'b', marker = '.', markersize = 10)\r\nplt.plot(yearMonthForecast, monthlyProfitForecast, c = 'orange', marker = '.', markersize = 10)\r\nplt.xticks(fontsize = 8)\r\nplt.yticks(fontsize = 8)\r\nplt.xlabel('Year-Month', fontsize = 10)\r\nplt.ylabel('Millions $', fontsize = 10)\r\n```\r\n\r\n![Plot of forecasted monthly profit](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--17.png?raw=true)\r\n\r\n\r\n\r\n## Step 4. Integrate Python model into Power BI\r\n\r\nIn this scenario, only 12 values are produced, so there is likely minimal impact from incorporating it into Power BI.\r\n\r\n**Note:** However, if your scenario involves a larger output, it may be better to export the data from Visual Studio Code to a flat file using either of these commands, then loading the file to Power BI:\r\n\r\n```\r\ndf.to_csv(path_or_buf='export/filename.csv', index=False)\r\ndf.to_excel(excel_writer='export/filename.xlsx', index=False)\r\n```\r\n\r\nContinuing with our example, from Power Query in Power BI, I reference the fact table and perform a Group By to sum the daily profits of the original dataset.\r\n\r\n![Power Query Group By](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--18.png?raw=true)\r\n\r\nThen convert the OrderDate to data type text, and split column on the forwardslash / delimiter at every occurrence.\r\n\r\n![Power Query split date column](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--19.png?raw=true)\r\n\r\nThe reason for this is to create a new _text datatype_ column using dash delimiter, to align with Python's date formatting.\r\n\r\n```\r\n[Year] & \"-\"  & [Month] & \"-\" & \r\n(if Text.Length([Day]) = 1\r\nthen \"0\"\r\nelse \"\") & [Day]\r\n```\r\n\r\n![Power Query add custom date column](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--20.png?raw=true)\r\n\r\nThen remove the unnecessary columns, so only the essential ones are included for the dataset that Python ingests.\r\n\r\n![Power Query remove unneeded columns](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--21.png?raw=true)\r\n\r\nFrom Power Query's Transform tab, click \"Run Python Script\".\r\n\r\n![Power Query Run Python Script](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--22.png?raw=true)\r\n\r\nWhilst most of the code from your Python script can be copy-and-pasted, note that the data source is the step in Query Settings prior to your Python script, and is named \"dataset\", so you need to adjust your script to accommodate for that.\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\ndf = pd.DataFrame(dataset, columns = ['OrderDate','Profit'])\r\n\r\ndf['OrderDate'] = pd.to_datetime(df['OrderDate'], errors = 'ignore')\r\n\r\ndf.set_index('OrderDate')\r\ndf['YearMonth'] = df['OrderDate'].dt.to_period(\"M\")\r\ngroupby = df.groupby(['YearMonth'])\r\ndf['MonthlyProfit'] = groupby['Profit'].transform(np.sum)\r\ndf = df.drop(columns=['OrderDate','Profit'])\r\ndf.drop_duplicates(keep='first', inplace = True)\r\ndf.set_index('YearMonth')\r\ndf.index.freq = 'MS'\r\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\r\nmodel = ExponentialSmoothing(df['MonthlyProfit'],trend='mul',seasonal='mul',seasonal_periods=12).fit()\r\nrange = pd.date_range('01-01-2024', periods=12, freq='MS')\r\npredictions = model.forecast(12)\r\npredictions_range = pd.DataFrame({'MonthlyProfit':predictions,'YearMonth':range})\r\n```\r\n\r\n![Power Query add Python script](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--23.png?raw=true)\r\n\r\n**Important!**: Upon clicking OK, you may encounter error messages preventing the script from running, asking for you to install Python libraries in order to proceed. The reason for this is that the libraries you installed in Visual Studio Code were for your virtual environment. However, Power BI does not access that virtual environment, and instead is accessing your native Python installation, which you may not have installed the libraries to.\r\n\r\nThen click to expand Table for the predictions_range variable.\r\n\r\n![Power Query expand Table](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--24.png?raw=true)\r\n\r\n![Power Query expanded Table with monthly forecast values](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--25.png?raw=true)\r\n\r\nConnect the output to your date or calendar table to help ensure the values plot in your Power BI time series visuals. **Note:** I am aware that best practice modelling typically advises to avoid 2 way relationships as much as possible, so that's something that needds to be looked into amending the data model here later on.\r\n\r\n![Power BI Model view connect to Calendar table](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--26.png?raw=true)\r\n\r\nHere are the two line charts of actuals and forecasted figures next to each other:\r\n\r\n![Power BI line chart comparing actuals and forecast](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--27.png?raw=true)\r\n\r\nThen following one of many tutorials you can find on YouTube to merge actuals and forecasted figures, you can create a single linechart like below:\r\n\r\n![Power BI line chart comparing actuals and forecast](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--28.png?raw=true)\r\n\r\n\r\nMy recommendations are:\r\n\r\n* **[Showing actuals and forecasts in the same chart with Power BI by SQLBI](https://www.youtube.com/watch?v=DKgF-5QHY68)**\r\n\r\n* **[Combining ACTUALS and FORECAST as one LINE in Power BI by How to Power BI](https://www.youtube.com/watch?v=_TAGpAJ9rTQ)**\r\n\r\nThe second takes the first video one step futher by showing the calculation used to sum the actuals and forecasts into a single measure.\r\n\r\n\r\n\r\n### References\r\n\r\n* Blog post **[Forecasting with Python and Power BI by Absent Data](https://www.absentdata.com/power-bi/forecasting-with-python-and-power-bi/)**\r\n\r\n* YouTube video **[How do I find and remove duplicate rows in pandas?](https://www.youtube.com/watch?v=ht5buXUMqkQ)**\r\n\r\n* YouTube video **[How to install Python Libraries in Visual Studio Code by Aditya Thakur](https://www.youtube.com/watch?v=ThU13tikHQw)**\r\n\r\n* Stack Overflow question **[How to show Python plots in Visual Studio Code](https://stackoverflow.com/questions/49992300/)**\r\n\r\n* Stack Overflow question **[How to clear Visual Studio Code terminal](https://stackoverflow.com/questions/49992300/)**\r\n\r\n* Stack Overflow question **[Python script in Power BI returns date as Microsoft.OleDb.Date](https://stackoverflow.com/questions/51929420/python-script-in-power-bi-returns-date-as-microsoft-oledb-date)**\r\n\r\n* Stack Overflow question **[Setting the x axis as time (years, months) for a set of values](https://stackoverflow.com/questions/28948898/setting-the-x-axis-as-time-years-months-for-a-set-of-values)**\r\n\r\n* LinkedIn Learning course **[Python: Working with Predictive Analytics by Isil Berkun](https://www.linkedin.com/learning/python-working-with-predictive-analytics/)**\r\n\r\n* LinkedIn Learning course **[Python for Data Visualization by Michael Galarnyk and Madecraft](https://www.linkedin.com/learning/python-for-data-visualization)**\r\n\r\n\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2022-04-17.md)** for this post's markdown file in GitHub."},{"id":1645880400,"title":"Find grouped records with a date that matches another date column using DAX","tag":"logo-powerbi","date":"February 27, 2022","content":"\r\nHow to use DAX to group records by an identity column, and see if the most recent row has a date value that matches a date value in another column.\r\n\r\nIn previous posts I've tried to find __[window aggregate values using DAX](https://datamesse.github.io/#/post/1634994000)__, and as many Power BI developers highlight, there are many ways to get the same outcome. However the difference between that previous post and this one is that the prior one applied a value to a specific row of the group, whereas this one applies the value to all rows of each grouping, which will be needed for other calculated columns.\r\n\r\nWhilst working on a small data integrity report at work, I came across this method which is a slightly modified script from a Power BI Communities post, intended to pull the desired value per grouping and apply it to all rows of each group.\r\n\r\n**DAX structure**\r\n```\r\nCustom column 1 =\r\nVAR identifiercolumn = 'Table'[ID column]\r\nVAR findlatestrecord = CALCULATE( MAX( 'Table'[Date A] ), ALLEXCEPT( 'Table', 'Table'[ID column] ) )\r\nRETURN\r\n   CALCULATE ( \r\n              MAX( 'Table'[Date A] ),\r\n              FILTER( 'Table', 'Table'[Date A] = findlatestrecord ),\r\n                      'Table'[ID column] = identifiercolumn               \r\n   )\r\n```\r\nWhat should be noted is the aggregation under RETURN doesn't matter in this case i.e. it could be MIN or MAX, because the second declared variable targets the specific row to get its value from, rather than actually aggregating all values for the group.\r\n\r\nThe scenario I applied this to involves a series of flat files produced on separate days to represent historic and future orders. Each order is scheduled to be delivered on a specific date at a certain cost. The report is intended to highlight problematic data, such as Order IDs that were scheduled for a specific date in one file, but do not appear in the other files generated on that same date.\r\n\r\nIn this context, we want to know the most recent *Planned Delivery Date* applied to an *Order ID* based on the latest *File's date* that the *Order ID* appears in, then apply that value to all the rows for that *Order ID*\r\n\r\nIn the example below, there are 2 Order IDs, 8 and 9. *Order ID* 8 has its *Planned Delivery Date* moved earlier, and *Order ID* 9 has its own moved later, in subsequent files. The ones highlighted orange are the ones we want to display.\r\n\r\n![Sample dataset 1](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-02-27--01.png?raw=true)\r\n\r\nApplying the above DAX structure, we get the following:\r\n\r\n**DAX implementation**\r\n```\r\nMost recent Planned Delivery Date = \r\nVAR orderid = 'Data integrity audit'[Order ID]\r\nVAR highestfiledatebyorderid =\r\n    CALCULATE(\r\n        MAX( 'Data integrity audit'[File's date] ), ALLEXCEPT( 'Data integrity audit','Data integrity audit'[Order ID] )\r\n    )\r\nRETURN\r\n    CALCULATE(\r\n        MAX ( 'Data integrity audit'[Planned Delivery Date] ),\r\n        FILTER( 'Data integrity audit', 'Data integrity audit'[File's date] = highestfiledatebyorderid ),\r\n                'Data integrity audit'[Order ID] = orderid\r\n    )\r\n```\r\n\r\n![Applied DAX structure to find recent date value by Order ID](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-02-27--02.png?raw=true)\r\n\r\n![Successfully applied DAX column added to table 1](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-02-27--03.png?raw=true)\r\n\r\nNow that we have applied the most recent *Planned Delivery Date* for every row based on *Order ID* as an anchor, we just need to check to if the *Order ID* has a row reflecting that it appears in a file whose generation date is the same as the most recent *Planned Delivery Date*. In this context, it serves as affirmation that it was delivered on that day.\r\n\r\n**DAX structure**\r\n```\r\nCustom column 2 =\r\nIF (\r\n    ISBLANK(\r\n            CALCULATE(\r\n               FIRSTNONBLANK( 'Table'[ID column], 1 ),\r\n               FILTER( ALLEXCEPT( 'Table', 'Table'[ID column] ),\r\n                                  'Table', 'Table'[Custom column 1] = 'Table'[Date B]               \r\n               )\r\n            )\r\n    )\r\n)\r\n```\r\n\r\nLooking at *Order ID* examples 8 and 16, the former has a matchng record and the latter does not.\r\n\r\n![Sample dataset 2](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-02-27--04.png?raw=true)\r\n\r\n**DAX implementation**\r\n```\r\nDoes Order ID appear in File with date matching Delivery Date? = \r\nIF(\r\n    ISBLANK(\r\n        CALCULATE(\r\n            FIRSTNONBLANK('Data integrity audit'[Order ID], 1),\r\n            FILTER( ALLEXCEPT('Data integrity audit','Data integrity audit'[Order ID] ),\r\n                              'Data integrity audit'[Most recent Planned Delivery Date] = 'Data integrity audit'[File's date] )\r\n        )\r\n    ) = FALSE,\r\n    \"Yes\",\r\n    \"No\"\r\n)\r\n```\r\n\r\n![Applied DAX structure to match different date columns and apply to all records per group](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-02-27--05.png?raw=true)\r\n\r\n![Successfully applied DAX column added to table 2](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-02-27--06.png?raw=true)\r\n\r\nOther more complex columns can then be built on top of this one, in my case, creating a conditional column that categorises the different patterns to highlight areas where data integrity may require review.\r\n\r\n![Complex conditional DAX query built on top of the aforementioned queries](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-02-27--07.png?raw=true)\r\n\r\n![Example categorisation from previous query added to a matrix](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-02-27--08.png?raw=true)\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2022-02-27.md)** for this post's markdown file in GitHub.\r\n"},{"id":1641906000,"title":"International Marketplace dataset created with Integration Services","tag":"logo-sqlserver","date":"January 12, 2022","content":"\r\nSQL Server Integration Services (SSIS) was used to create this fictional dataset, by merging Microsoft's *Wide World Importers* database and *Contoso* data warehouse, with Tableau's *Sample - APAC Superstore* dataset, with some data alterations.\r\n\r\n![International Marketplace SSIS package](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--01.png?raw=true)\r\n\r\n![International Marketplace in Power BI sales by city](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--00.png?raw=true)\r\n\r\n**Final output**\r\n\r\nYou can download Excel copies of the completed dataset:\r\n\r\n* Star schema (for Power BI data visualisation) **[download here](https://github.com/datamesse/data-visualisation-datasets/raw/main/International%20Marketplace%20sales/International%20Marketplace%20Normalised%20for%20Power%20BI.xlsx)**.\r\n\r\n![International Marketplace: Normalised star schema](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--02.png?raw=true)\r\n\r\n* Denormalised (for Tableau data visualisation) **[download here](https://github.com/datamesse/data-visualisation-datasets/raw/main/International%20Marketplace%20sales/International%20Marketplace%20Denormalised%20for%20Tableau.xlsx)**.\r\n\r\n![International Marketplace: Denormalised](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--03.png?raw=true)\r\n\r\nThis blog post outlines the planning and challenges in making the dataset, which was my first attempt at creating an SSIS package from scratch. For information on the data flows and SQL scripts used in the package, visit the **[GitHub repository](https://github.com/datamesse/data-visualisation-datasets/tree/main/International%20Marketplace%20sales)**.\r\n\r\n**REVIEWING THE ORIGINAL DATA SOURCES**\r\n\r\n**APAC Superstore**\r\n\r\nTableau's APAC Superstore dataset can be found and extracted from Tableau Desktop's *Saved Data Sources*.\r\n\r\n - Customers: 795 (by name), 5,220 (by name and city)\r\n - Products: 1,913\r\n - Countries: 23\r\n - Cities: 537\r\n - Sales records: 10,925\r\n - Sales years: 2018 to 2021\r\n\r\nThis is a succinct and denormalised dataset, where customers have no personal locations, as the same names are replicated across many countries. The names are also not very ethnically diverse, given the name of the dataset.\r\n\r\n![APAC Superstore order map](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--04.png?raw=true)\r\n\r\n![APAC Superstore data source](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--05.png?raw=true)\r\n\r\nAt the time of writing this blog post, the order dates covered are from 2018 to 2021. I'm highlighting that because I suspect that unlike the other datasets, Tableau's dataset has its dates automatically changed over time, and the SSIS package we create needs to accommodate for that.\r\n\r\nThe structure of the dataset merge will be based on APAC Superstore, since it has the least dimensions and facts compared to the Microsoft ones, which are proper databases.\r\n\r\n\r\n**Wide World Importers**\r\n\r\nMicrosoft's Wide World Importers database backup (.bak) file can be downloaded from here:\r\n**[https://github.com/Microsoft/sql-server-samples/releases/tag/wide-world-importers-v1.0](https://github.com/Microsoft/sql-server-samples/releases/tag/wide-world-importers-v1.0)**\r\n\r\n - Customers: 663\r\n - Products: 227\r\n - Countries: 1\r\n - Cities: 655\r\n - Sales records: 228,265\r\n - Sales years: 2013 to 2016\r\n\r\nThis is an extensive normalised dataset with customers based only in the United States. Each customer only exists in one city, but there are customer names for the same corporation in different cities e.g.\r\n - Tailspin Toys (Arietta, NY)\r\n - Tailspin Toys (Trentwood, WA)\r\nThese make up 60.6% of the customer records (402 out of 663).\r\n\r\n![Wide World Importers customer map](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--06.png?raw=true)\r\n\r\nWhilst 263 customer records have them based in a specific city, most have invoices for multiple other cities, presumably reflecting B2B sales.\r\n\r\n![Wide World Importers tables](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--07.png?raw=true)\r\n\r\n\r\n**Contoso BI Demo Dataset for Retail Industry**\r\n\r\nMicrosoft's Contoso BI Demo Dataset for Retail Industry data warehouse .bak file can be downloaded from here:\r\n**[https://www.microsoft.com/en-us/download/details.aspx?id=18279](https://www.microsoft.com/en-us/download/details.aspx?id=18279)**\r\n\r\n - Customers: 18,785 (by name) 18,868 (by name and city)\r\n - Products: 2,516\r\n - Countries: 29\r\n - Cities: 476\r\n - Sales records: 3,324,410 (online sales only)\r\n - Sales years: 2007 to 2009\r\n\r\n![Contoso customer map](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--08.png?raw=true)\r\n\r\nThis is the largest of the 3 datasets in terms of the number of sales records. Each sales row represents a single unit for a product, meaning aggregations need to be calculated where the same product is sold multiple times under the same order number.\r\n\r\nSo a lot of record removal is needed to bring down the dataset to a more manageable size, only using the latest order number for each customer (retaining multiple order line rows), which will leave a usable 19 thousand records.\r\n\r\n - Customers: 18,674 (by name) 18,753 (by name and city)\r\n - Products: 463\r\n - Countries: 29\r\n - Cities: 476\r\n - Sales records: 19,419\r\n - Sales years: 2007 to 2009\r\n\r\nContoso also has a lot of trailing spaces in values, which need to be trimmed as part of the ETL's data cleaning. Geographic corrections also need to be made, for example, the city Perth is incorrectly listed under South Australia.\r\n\r\n![Contoso tables](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--09.png?raw=true)\r\n\r\nThe table below shows a summary of how the dataset columns align as-is, and the T-SQL needed to populate respective columns with existing data.\r\n\r\n![Comparing columns across the 3 datasets](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--10.png?raw=true)\r\n\r\n**CHALLENGES**\r\n\r\n**Region inconsistency**\r\n\r\nWide World Importers has a comprehensive Country table for Region and Subregion. We will remap the countries from APAC Superstore and Contoso to those regions for consistency.\r\n\r\n**Support for special characters**\r\n\r\nA large number of products use commas, and many names, states, cities and the like, use special characters, which I found to store accurately in Unicode tab-delimited text files, as opposed to some other file format types which cannot render the characters correctly. So both the APAC Superstore, and the remapping files use this file format.\r\n\r\n**Customer names are incoherent and do not reflect country-of-origin**\r\n\r\nThe APAC Superstore and Contoso datasets have customers outside the United States, but their names do not reflect those locations. Using my past experience with country-based name randomisation (plugging my previous blog post **[here](https://datamesse.github.io/#/post/1635598800)**), I created customer name re-mapping files **[here](https://github.com/datamesse/data-visualisation-datasets/tree/main/International%20Marketplace%20sales/text%20files%20for%20value%20remapping%20in%20SSIS)**.\r\n\r\nContoso also has company names with a missing space before the \"Company\" suffix. As these are just the city with \"Company\" suffixed, they are remapped with a person's name instead.\r\n\r\n**Mostly United States sales**\r\n\r\nWide World Importers is a United States sales dataset, so the U.S. sales data from Contoso will be remapped to other countries, to give the final dataset more global reach. The geographic remapping file for this is **[here](https://github.com/datamesse/data-visualisation-datasets/tree/main/International%20Marketplace%20sales/text%20files%20for%20value%20remapping%20in%20SSIS)**.\r\n\r\n**Product, Category/Subcategory, and Supplier remapping**\r\n\r\nThere are shared _Supplier_ names across Contoso and Wide World Importers, but the spelling needs to be amended so they align. World Wide Importers uses _Stock Groups_, where each product item can belong to multiple groups (1:M), whereas APAC Superstore and Contoso have 1:1 Category and Sub-Category for their products. So all of these have been remapped and re-classified as a whole. Remapping files are **[here](https://github.com/datamesse/data-visualisation-datasets/tree/main/International%20Marketplace%20sales/text%20files%20for%20value%20remapping%20in%20SSIS)**.\r\n\r\n**Customer segmentation**\r\n\r\nAPAC Superstore defines its segmentation by Consumer, Home Office, and Corporate. International Marketplace merges Home Office with Consumer, as Contoso uses Person and Company for its segmentations. So these are easily remapped:\r\n\r\n```\r\ncase\r\n   when c.CustomerType = 'Person' then N'Consumer'\r\n   when c.CustomerType = 'Company' then N'Corporate'\r\n   else null end as \"Segment\"\r\n```\r\n\r\nWorld Wide Importers made the remapping simple as company names are suffixed with the office's city in parenthesis:\r\n\r\n```\r\ncase\r\n   when c.CustomerName like '%(%' then N'Corporate'\r\n   else N'Consumer' end as \"Segment\"\r\n```\r\n\r\n**Ship Date missing**\r\n\r\nOnly APAC Superstore has this column, with no decent equivalents in Contoso and World Wide Importers. So we used T-SQL randomisation to populate this column.\r\n\r\nAs Contoso is a regional dataset, the _Ship Date_ is set to randomise up to 17 days after the _Order Date_.\r\n\r\n```\r\nconvert(varchar, (dateadd(day,(abs(checksum(newid())) % 18),s.Datekey)),103) \"Ship Date\"\r\n```\r\n\r\nTo give some variation toward shorter ship times, World Wide Importers randomises up to 7 days after the _Order Date_.\r\n\r\n```\r\nconvert(varchar,dateadd(day,(abs(checksum(newid())) % 8),o.OrderDate),103) \"Ship Date\",\r\n```\r\n\r\n**Ship Mode**\r\n\r\nLike _Ship Date_, only APAC Superstore has this column. This value is randomised in different phases for Contoso and Wide World Importers.\r\n\r\nThe first pass involves checking where the _Order Date_ and _Ship Date_ are the same value, and approximately 66.6% of them are randomised to be treated as \"Same Day\" shipment.\r\n\r\n```\r\nupdate zs2\r\n   set zs2.FromShipMode = 'Same Day'\r\n   from z_sales zs2\r\n   inner join\r\n      (select cast((ABS(CHECKSUM(NewId()))%3) as bigint) as \"random\", zs1.SalesStagingID\r\n       from  z_sales zs1\r\n       where zs1.FromOrderDate = zs1.FromShipDate and zs1.FromShipMode is null) as rzs\r\n      on zs2.SalesStagingID = rzs.SalesStagingID\r\n   where rzs.random < 2;\r\n```\r\n\r\nAs randomisation in SQL Server seems to result in near-equal value distribution, all remaining sales are allocated _Ship Mode_ based on a 0 to 9 randomisation with categories disproportionately assigned to integer ranges. \r\n\r\n```\r\nupdate zs2\r\n   set zs2.FromShipMode =\r\n      case\r\n         when rzs.random between 0 and 5 then N'Standard Class'           \r\n         when rzs.random between 6 and 8 then N'Second Class'\r\n         else N'First Class' end\r\n   from z_sales zs2\r\n   inner join\r\n      (select cast((ABS(CHECKSUM(NewId()))%10) as bigint) as \"random\", zs1.SalesStagingID\r\n       from  z_sales zs1\r\n       where zs1.FromShipMode is null) as rzs\r\n      on zs2.SalesStagingID = rzs.SalesStagingID;\r\n```\r\n\r\n**Different date ranges**\r\n\r\nAligning the order dates across the datasets to cover the same years is done with an initial randomised approximately 50:50 split of the sales data to be moved to 2022.\r\n\r\n```\r\nupdate zs2\r\n   set\r\n      zs2.toshipdate = dateadd(year,(2022-cast(convert(varchar(4),zs2.fromshipdate,112) as int)),zs2.fromshipdate),\r\n      zs2.toorderdate = dateadd(year,(2022-cast(convert(varchar(4),zs2.fromorderdate,112) as int)),zs2.fromorderdate)\r\n   from z_sales zs2\r\n   inner join\r\n      (select cast((ABS(CHECKSUM(NewId()))%2) as bigint) as \"random\", zs1.SalesStagingID\r\n       from  z_sales zs1 where zs1.toorderdate is null and zs1.toshipdate is null) as rzs\r\n       on zs2.SalesStagingID = rzs.SalesStagingID\r\n   where rzs.random < 1;\r\n```\r\n\r\nWith the remainder set to the following year. In hindsight, it would have been better to implement this as a package-scoped variable to allow the user to place any year they wanted.\r\n\r\n```\r\nupdate zs\r\n   set\r\n      zs.toshipdate = dateadd(year,(2022-cast(convert(varchar(4),zs.fromshipdate,112) as int)+1),zs.fromshipdate),\r\n      zs.toorderdate = dateadd(year,(2022-cast(convert(varchar(4),zs.fromorderdate,112) as int)+1),zs.fromorderdate)\r\n   from z_sales zs\r\n   where zs.toorderdate is null and zs.toshipdate is null;\r\n```\r\n\r\n**OTHER LESSONS LEARNED**\r\n\r\n**Creating bins for histograms**\r\n\r\nAPAC Superstore uses a Profit (bin) field to round down profits to nearest $200, including going into negatives. To apply that to the invoice line profit field in Wide World Importers, the basic select for this would be:\r\n\r\n```\r\nfloor( il.LineProfit / 200 ) * 200\r\n```\r\n\r\n**For Flat File connections, Visual Studio may retain cached file loads**\r\n\r\nSimply running the Play button won't reflect changes to the file, but I found closing and reopening Visual Studio before hitting play to work in some cases.\r\n\r\n**Unicode and non-Unicode conversion**\r\n\r\nFor this error message that appears when using OLE DB Source: _\"Column cannot convert between unicode and non-unicode string data types.\"_\r\nRight-click the OLE DB Source element > Show Advanced Editor > navigate to \"Input and Output Properties\" tab > expand \"OLE DB Source Output\" > expand \"Output Columns\" > check the DataType field for each column reported, and adjust where needed.\r\n\r\n**Flat file data sources, with double quotations as string delimiter**\r\n\r\nTo get rid of them, the following can be used in Derived Column transformations **[https://stackoverflow.com/questions/65176461/ssis-flat-file-source-get-rid-of-some-embedded-unwanted-double-quotes](https://stackoverflow.com/questions/65176461/ssis-flat-file-source-get-rid-of-some-embedded-unwanted-double-quotes)**.\r\n\r\n```\r\nREPLACE([4-2-27  Data Conversion 3].Product,\"\\\"\",\"\")\r\n```\r\n**Note:** There's a backspace before the second double quotation mark, in order to escape the double quotation. Which is ironic since markdown files do the same thing any you may not see it in the code above.\r\n\r\n\r\n**SSIS Derived Columns can truncate flat file columns**\r\n\r\nThis can be resolved by right-clicking the Derived Column element > Show Advanced Editor, see above Unicode error for same steps, but this time involves changing the length for Derived Column Output **[https://nishantrana.me/2019/05/08/error-the-derived-column-failed-because-truncation-occurred-and-the-truncation-row-disposition-on-derived-column-outputsderived-column-output-columnsfilepath-specifies-failure-on-truncat/](https://nishantrana.me/2019/05/08/error-the-derived-column-failed-because-truncation-occurred-and-the-truncation-row-disposition-on-derived-column-outputsderived-column-output-columnsfilepath-specifies-failure-on-truncat/)**.\r\n\r\n**Creating text file-based remapping files may not work if NULLs are involved**\r\n\r\nRemapping files for Wide World Importers had to be separated from the other 2 data sources because it alone had null on categories. Otherwise produced whitespace that could not be trimmed because it used an ASCII 194 160 combination **[https://stackoverflow.com/questions/42424555/trim-whitespace-ascii-character-194-from-string](https://stackoverflow.com/questions/42424555/trim-whitespace-ascii-character-194-from-string)**.\r\n\r\n**Data Flow encapsulation**\r\n\r\nI noticed a strange behaviour where putting the flows to populate both the main State and City tables inside a single dataflow populates just State, but not the City table, but if the flows were separated into their own data flows, then they each populate. Currently suspecting it may be because they share the same OLE DB Source (though different SQL was used).\r\n\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2022-01-12.md)** for this post's markdown file in GitHub."},{"id":1639746000,"title":"How to create a free Azure account to post reports to the Power BI Gallery","tag":"logo-azure","date":"December 18, 2021","content":"\r\nIf your work or school account does not provide you access to Power BI (for the purposes of publishing to Web, specifically the Community Gallery), you can create your own. \r\n\r\nYou can check your existing account by navigating to **[https://powerbi.microsoft.com/](https://powerbi.microsoft.com/)**, then clicking \"Have an account? Sign in\".\r\n\r\n![Check Power BI sign in](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-12-18--01.png?raw=true)\r\n\r\nIf you receive the following message:\r\n\r\n*\"Sorry, we can't sign you up as ...*\r\n\r\n*Your IT department has turned off signup for Microsoft Power BI. Contact them to complete signup.\"*\r\n\r\n![Power BI sign up disabled](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-12-18--02.png?raw=true)\r\n\r\n...and have no sway over your IT department to give you access, you can sign up for a new account instead by clicking \"No account? Create one!\", and set up your new email and password.\r\n\r\n![Azure Sign in create account](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-12-18--03.png?raw=true)\r\n\r\n![Azure create a new account](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-12-18--04.png?raw=true)\r\n\r\nYou can then start your Azure free trial. Fill out your details, which will require providing a mobile number for identity verification.\r\n\r\n![Azure portal](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-12-18--05.png?raw=true)\r\n\r\nWith your account set up, you can now begin creating your own \"organisation\" or tenant, which would have Power BI enabled. Begin by going to Azure Active Directory.\r\n\r\n![Azure Active Directory](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-12-18--06.png?raw=true)\r\n\r\nClick \"Manage tenants\", and create your new tenant.\r\n![Azure Active Directory Manage tenants](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-12-18--07.png?raw=true)\r\n\r\nThen create a new user under that tenant domain e.g. yourname@yourtennt.onmicrosoft.com, and ensure that new user is added to the Administrators group of the tenant.\r\n\r\n![Azure Active Directory Add User](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-12-18--08.png?raw=true)\r\n\r\n![Azure Active Directory Add User Assignment](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-12-18--08a.png?raw=true)\r\n\r\nFor that new user account, assign the roles Power BI Administrator and Power Platform admin, which you can do from **[https://portal.office.com](https://portal.office.com)**.\r\n\r\n![Microsoft 365 admin center](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-12-18--09.png?raw=true)\r\n\r\nThen you can configure \"Publish to Web\" from the Admin portal at **[https://app.powerbi.com](https://app.powerbi.com)**. There are plenty of other resources for setup recommendations. For example, this post from **[Radacad](https://radacad.com/power-bi-administrator-tenant-settings-configuration-you-dont-dare-to-miss)**.\r\n\r\n![Power BI Admin portal](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-12-18--10.png?raw=true)\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2021-12-18.md)** for this post's markdown file in GitHub."},{"id":1635598800,"title":"Customer support ticket update generator and sample dataset","tag":"logo-excel","date":"October 31, 2021","content":"\r\nThis is a sample dataset created using Excel randomisation, and you can create your own using the file generator.\r\n\r\nYou can download or connect to the sample dataset from **[here](https://github.com/datamesse/data-visualisation-datasets/blob/main/Support%20ticket%20updates/Support%20ticket%20updates.xlsx?raw=true)**.\r\n\r\nThe Github repository with the agent photos can be found **[here](https://github.com/datamesse/data-visualisation-datasets/tree/main/Support%20ticket%20updates/agents)**.\r\n\r\nAn Excel random person name and business generator was also used to create this dataset, downloadable **[here](https://github.com/datamesse/data-visualisation-datasets/blob/main/Support%20ticket%20updates/Random%20name%20and%20business%20generator.xlsx?raw=true)**.\r\n\r\nThe dataset contains:\r\n - 5000 support tickets\r\n - 27780 support ticket update records\r\n - 29 agents across 8 countries and 12 cities\r\n - 1233 end users across 27 countries and 65 cities\r\n - Date/timestamps are based on Sydney, Australia time (AEST GMT+10/AEDT GMT+11)\r\n\r\nThe first worksheet \"Updates\" has the back-and-forth update records for each ticket, indicating if the update is a public user message or agent reply, or an internal message by an Agent.\r\n\r\n![Support ticket updates](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-31--01.png?raw=true)\r\n\r\nThe second worksheet \"Assignment\" has the ticket created versus ticket assigned date/time data.\r\n\r\n![Support ticket assignments](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-31--02.png?raw=true)\r\n\r\nThe third worksheet \"Agents\" has photo IDs that correlate to the images in this **[Github folder](https://github.com/datamesse/data-visualisation-datasets/tree/main/Support%20ticket%20updates/agents)**.\r\n\r\n![Support ticket agents](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-31--03.png?raw=true)\r\n\r\n![Support ticket agent photos](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-31--04.png?raw=true)\r\n\r\n\r\n**Why this dataset was created?**\r\n\r\nWorking in a global support team providing follow-the-sun (FTS) customer service, I wanted to be able to visualise the teamwork across staff members.\r\n\r\nAs I needed a fake dataset to test ideas on, I created an Excel file that randomly generated support ticket update information (e.g. when a end-user opens a ticket, when an agent first replies to the end-user, and the back-and-forth until the ticket becomes solved). The columns used in the dataset are meant to be similar to those seen in CRM systems that provide reporting on such ticket updates (e.g. Zendesk).\r\n\r\nI noticed my random generator did not account for the ticket IDs being in a realistic sequence, which needs to be amended after extracting the data from the generator. Also, the generator is limited in providing \"follow-the-sun\" support update records, so I spent some time manually editing records to demonstrate this in the sample dataset.\r\n\r\n\r\n**Disclaimer**\r\n\r\nThis dataset is free to use and alter as you need, and no attribution is required, though would be appreciated.\r\n\r\nAll names in this dataset are fictional and not based on real-life people. The random name generator that was used to create them can be found **[here](https://github.com/datamesse/excel-support-ticket-update-generator/blob/main/Random_name_and_business_generator.xlsx?raw=true)**.\r\n\r\nPhotographs were taken from **[Pixabay.com](https://pixabay.com/service/license/)** and **[Pexels.com](https://www.pexels.com/license/)** for non-commercial use, edited to fit the appearance of an organisational profile photo, and direct URL attribution included in the dataset for each photo.\r\n\r\nThe GitHub repository for this is **[here](https://github.com/datamesse/data-visualisation-datasets/tree/main/Support%20ticket%20updates)**.\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2021-10-31.md)** for this post's markdown file in GitHub."},{"id":1634994000,"title":"Calculate aggregate for grouped rows based on column value, DAX version","tag":"logo-powerbi","date":"October 24, 2021","content":"\r\nHow to use DAX to find the aggregate value for rows grouped by a column value in Power BI.\r\n\r\nIn a __[previous post](https://datamesse.github.io/#/post/1634389200/)__, I wrote on window aggregation equivalents in Power Query, going on the notion that categorical columns are generally better off done in Power Query using M code, as opposed to done in DAX.\r\n\r\nHowever, in a Power BI report I am working on, it seemed re-implementing it using DAX had faster report load times than Power Query, which I attributed to existing Power Query merges needed for the model, which slowed down processing. It also required having 2 columns in the model (one for the aggregation, and the other for the conditional result), unlike DAX which only required one.\r\n\r\nThat M code looked like this:\r\n\r\n**Power Query M structure**\r\n```\r\n    #\"Added Custom 1\" = Table.NestedJoin(#\"Previous step\",\r\n                                      {\"Column A to join on\"},\r\n                                      Table.Group(Table.SelectRows(#\"Changed Type\", each ([Column C] = \"Agent\")),\r\n                                                  {\"Column A to join on\"},\r\n                                                  {{\"Result of aggregation\",\r\n                                                  each List.Min([#\"Column B to aggregate on\"]), type nullable datetime}}),\r\n                                      {\"Column A to join on\"},\r\n                                      \"Merged group by table\",\r\n                                      JoinKind.Inner),\r\n    #\"Expanded Merged group by table\" = Table.ExpandTableColumn(#\"Added Custom 1\", \"Merged group by table\", {\"Result of aggregation\"}, {\"Result of aggregation\"}),\r\n    #\"Added Custom 2\" = Table.AddColumn(#\"Expanded Merged group by table\", \"M column result\", each if [#\"Column B to aggregate on\"] = [#\"Result of aggregation\"] then \"Yes\" else \"No\")\r\n```\r\n\r\nIn this example we're still using randomised support ticket update data, and trying to find which agent reply to the user is the first for each Ticket ID.\r\n\r\n![Sample dataset with only 15 rows of data](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--01.png?raw=true)\r\n\r\n\r\n**Power Query implementation**\r\n```\r\n    #\"Added Custom\" = Table.NestedJoin(#\"Changed Type\",\r\n                                      {\"Ticket ID\"},\r\n                                      Table.Group(Table.SelectRows(#\"Changed Type\", each ([Updater role] = \"Agent\")),\r\n                                                  {\"Ticket ID\"},\r\n                                                  {{\"Earliest date/time by Ticket ID\",\r\n                                                  each List.Min([#\"Update - Timestamp\"]), type nullable datetime}}),\r\n                                      {\"Ticket ID\"},\r\n                                      \"Merged group by table\",\r\n                                      JoinKind.Inner),\r\n    #\"Expanded Merged group by table\" = Table.ExpandTableColumn(#\"Added Custom\", \"Merged group by table\", {\"Earliest date/time by Ticket ID\"}, {\"Earliest date/time by Ticket ID\"}),\r\n    #\"Added Custom1\" = Table.AddColumn(#\"Expanded Merged group by table\", \"1st reply?\", each if [#\"Update - Timestamp\"] = [#\"Earliest date/time by Ticket ID\"] then \"Yes\" else \"No\")\r\nin\r\n    #\"Added Custom1\"\r\n```\r\n\r\n![Sample dataset with only 15 rows of data Power Query M code](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--02.png?raw=true)\r\n\r\n\r\n![Sample dataset with only 15 rows of data Power Query performance](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--03.png?raw=true)\r\n\r\nNow let's look at the DAX alternative.\r\n\r\nIn DAX, we've coded it as a new column, where an initial if condition is used to pre-determine the result for invalid rows (such as messages from client), and the nested false condition checks the remaining canidate rows (messages from agents) to see if the aggregate value (i.e. the earliest *Update - Timestamp*) matches for the existing row for each Ticket ID group.\r\n\r\n![Sample dataset with only 15 rows of data DAX Add New Column](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--04.png?raw=true)\r\n\r\n**DAX structure**\r\n```\r\nDAX column = IF([Column C] = \"Column value of rows that should be excluded from aggregation\",\r\n                \"No\",\r\n                IF(Tablename[Column B to aggregate on] = \r\n                    CALCULATE (\r\n                               MIN ( Tablename[Column B to aggregate on] ),\r\n                               FILTER(ALLEXCEPT (Tablename, Tablename[Column A to join on]), Tablename[Column C] = \"Column value of rows to aggregate on\" ))\r\n                              ,\"Yes\"\r\n                              ,\"No\"\r\n                )\r\n             )\r\n```\r\n\r\n**DAX implementation**\r\n```\r\n1st reply? = IF([Updater role] = \"Client\",\r\n                \"No\",\r\n                IF(Updates1[Update - Timestamp] = \r\n                    CALCULATE (\r\n                               MIN ( Updates1[Update - Timestamp] ),\r\n                               FILTER(ALLEXCEPT (Updates1, Updates1[Ticket ID]), Updates1[Updater role] = \"Agent\" ))\r\n                              ,\"Yes\"\r\n                              ,\"No\"\r\n                )\r\n             )\r\n```\r\n\r\n![Sample dataset with only 15 rows of data DAX code](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--05.png?raw=true)\r\n\r\n\r\n![Sample dataset with only 15 rows of data DAX performance](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--06.png?raw=true)\r\n\r\n\r\nComparing the initial table refreshes from the screenshots above seems to indicate Power Query in this scenario is more performant than DAX.\r\n* DAX: 224ms\r\n* Power Query: 94ms\r\n\r\nHowever, this is a flat dataset with only 15 rows. So I set out to test it with a larger flat dataset (27780 rows), and if the results still show Power Query is more performant than DAX for categorical window aggregation, I would test a second time with that larger dataset, but using pre-existing merges to more closer reflect my model.\r\n\r\n\r\n**COMPARING DAX AND M FOR CATEGORICAL WINDOW AGGREGATION**\r\n\r\nThis blog post compares DAX vs Power Query (M) implementation of this scenario against:\r\n1. flat dataset using M for the window aggregation column\r\n2. flat dataset using DAX for the window aggregation column\r\n3. dataset with existing merge using M for the window aggregation column\r\n4. dataset with existing merge using DAX for the window aggregation column\r\n\r\n**Test 1: Flat dataset using M for the aggregation column**\r\n\r\n![Large flat dataset using M window aggregation M code](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--07.png?raw=true)\r\n\r\nNote: Refresh 1 is when the column is first added to the visual.\r\n\r\n![Large flat dataset using M performance](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--09.png?raw=true)\r\n\r\n\r\n**Test 2: Flat dataset using DAX for the aggregation column**\r\n\r\n![Large flat dataset using M window aggregation DAX code](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--08.png?raw=true)\r\n\r\n\r\n![Large flat dataset using DAX](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--10.png?raw=true)\r\n\r\nNow we've applied a simple merge to the dataset to display additional columns.\r\n![Large dataset with existing merge](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--11.png?raw=true)\r\n\r\n\r\n**Test 3: Merged dataset using M for the aggregation column**\r\n\r\n![Large dataset with existing merge using M performance](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--12.png?raw=true)\r\n\r\n\r\n![Large dataset with existing merge using DAX performance](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--15.png?raw=true)\r\n\r\n\r\n**Test 4: Merged dataset using DAX  for the aggregation column**\r\n\r\n![Large dataset with existing merge using DAX](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--13.png?raw=true)\r\n\r\n\r\n![Large dataset with existing merge using DAX performance](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--14.png?raw=true)\r\n\r\n\r\n![Large dataset with existing merge using DAX performance](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--16.png?raw=true)\r\n\r\nReviewing these table refresh times, it appears that for data sources involving a merge, using DAX for window aggregations is more performant than using Power Query, whereas it seems to be the reverse for data sources not involving merges.\r\n\r\n![Comparing table refresh times](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--17.png?raw=true)\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2021-10-24.md)** for this post's markdown file in GitHub."},{"id":1634389200,"title":"Find aggregate value for grouped rows based on column value","tag":"logo-powerbi","date":"October 17, 2021","content":"\r\nHow to use Power Query to find the aggregate value for rows grouped by a column value in Power BI.\r\n\r\nT-SQL uses the concept of window functions to perform aggregations across groupings of rows, which are based on a specific column's values. More info on this can be found on [Pinal Dave's blog](https://blog.sqlauthority.com/2015/05/28/sql-server-what-are-t-sql-window-functions-notes-from-the-field-082/).\r\n\r\n```\r\nTable.NestedJoin(#\"Previous step in your Power Query code\", \r\n                 {\"Column(s) for left side of the join\"}, \r\n                Table.Group(#\"Previous step in your Power Query code\", \r\n                             {\"Column(s) to group by\"}, \r\n                             {{\"New column name for the aggregation result\", \r\n                             each List.Min([#\"Column to apply aggregation on\"]), \r\n                             type nullable datatypeofyouraggregation}}),\r\n                 {\"Column(s) for right side of the join\"}, \r\n                 \"New merged group by table name\",\r\n                 JoinKind.Inner)\r\n```\r\n\r\nAs a basic example, say your dataset has rows representing quarterly sales and you need to calculate the proportion of sales which quarter represents for the whole year i.e. Quarterly Sales ÷ Annual Sales.\r\n\r\nYou can do this by using a preliminary window function to first calculate annual sales by summing the rows based on shared year column values, providing that same value for each row of the group. Then for each row, calculate the proportion from there.\r\n\r\n![Example concept of Window function](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-17--01.png?raw=true)\r\n\r\nThe equivalent of Window functions in Power BI would typically be used for quantitative measures, and are thus more likely to be implemented using DAX calculations.\r\n\r\nHowever, there can be scenarios where window functions need to employed in a more categorical nature. Going by the general principle that custom measures (i.e. quantitative calculations) should be done in DAX, and that custom columns (typically categorical) should be done in Power Query, the latter is what we will employ here.\r\n\r\nIn this example scenario's dataset, we have 5 unique support ticket numbers, with each row representing an instance where a support agent has sent a response to the end-user for a ticket, as indicated by the date/timestamp.\r\n\r\n![Example categorical scenario of support ticket response date/timestamps](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-17--02.png?raw=true)\r\n\r\nOur objective is to create a column that identifies which rows of data represent the \"first response\" to the end-user for each ticket, with that intended 1st response flag being a rudimentary marker to help calculate other support agent statistics we may want (e.g. proportion of support tickets where the 1st responder is the assignee).\r\n\r\nThis is the equivalent of the previous example's proportion of annual sales that quarterly sales represents, in that the creation of a preliminary window function is needed. In our scenario, instead of doing a sum of sales based on rows sharing the same year, we will get the minimum (and hence the first) date/timestamps based on rows sharing the same ticket number.\r\n\r\nThe approach taken here is to create a new column whose definition is the combination of two common queries:\r\n1. The Table.NestedJoin function that's commonly seen in *Merge Queries* to combine your existing dataset to the conceptual grouped dataset.\r\n2. The Table.Group function creates a conceptual grouped by table of the existing dataset with the desired aggregate result, and that conceptual table being passed as the right-joined table parameter into the Table.NestedJoin.\r\n3. Then expand the merged table to display the aggregate result for each row of the original dataset.\r\n\r\n**Code structure**\r\n\r\n```\r\nTable.NestedJoin(#\"Previous step in your Power Query code\", \r\n                 {\"Column(s) for left side of the join\"}, \r\n                Table.Group(#\"Previous step in your Power Query code\", \r\n                             {\"Column(s) to group by\"}, \r\n                             {{\"New column name for the aggregation result\", \r\n                             each List.Min([#\"Column to apply aggregation on\"]), \r\n                             type nullable datatypeofyouraggregation}}),\r\n                 {\"Column(s) for right side of the join\"}, \r\n                 \"New merged group by table name\",\r\n                 JoinKind.Inner)\r\n```\r\n\r\n**Example**\r\n\r\nApplying the code structure above, the M code would be as below:\r\n\r\n![M code with merged](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-17--03.png?raw=true)\r\n\r\n```\r\n    #\"Added Custom\" = Table.NestedJoin(#\"Changed Type\", \r\n                                       {\"Ticket ID\"},\r\n                                       Table.Group(#\"Changed Type\",\r\n                                                   {\"Ticket ID\"},\r\n                                                   {{\"1st response\",\r\n                                                   each List.Min([#\"Update - Timestamp\"]), type nullable datetime}}),\r\n                                       {\"Ticket ID\"},\r\n                                       \"Merged group by table\",\r\n                                       JoinKind.Inner)\r\n```\r\n\r\nThe result of the new column addition (which is a merged table), will appear as below.\r\n\r\n![Power Query with new column for merged Group By table](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-17--04.png?raw=true)\r\n\r\nThen you simply need to expand out the aggregate column from the merge.\r\n\r\n![Power Query expand merged Group By table to display the aggregate column 1](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-17--05.png?raw=true)\r\n\r\n![Power Query expand merged Group By table to display the aggregate column 2](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-17--06.png?raw=true)\r\n\r\nNow that the preliminary window function is complete, we can address the example objective of creating a custom column to indicate which data row per Ticket ID represents the first support agent response to an end-user, which is a simple if condition to compare the \"Update - Timestamp\" and \"1st response\" columns.\r\n\r\n![Power Query if condition for example custom column](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-17--07.png?raw=true)\r\n\r\n![Power Query final example custom column](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-17--08.png?raw=true)\r\n\r\nIn this way, regardless of how the data is sorted in Power Query, the records representing the 1st responses will remain.\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2021-10-17.md)** for this post's markdown file in GitHub."},{"id":1633784400,"title":"Dynamically apply time zone and daylight savings on date/times in Power BI","tag":"logo-powerbi","date":"October 10, 2021","content":"\r\nHow to use Power Query to apply time zone offsets based on daylight savings \"anchors\" on date/times using a parameter, and without needing a separate calendar table in Power BI.\r\n\r\nThe final product is being able to use a Parameter to select a desired time zone, and apply it to your dataset's \"Date/Time\" column, and produce a \"Date/Time/Zone\" value.\r\n![Power BI Tokyo example](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--01.png?raw=true)\r\n\r\nWhat makes this different from the DateTime.Zone function alone, is that this method respects when the UTC offset changes based on time zone by creating custom functions to do this.\r\n\r\nThis post incorporates my previous posts on [how to import time zone and daylight saving observations from Wikipedia](https://datamesse.github.io/#/post/1633183200), which indicate when different offsets are applied (e.g. first Sunday of October), and my post on [how to find the nth day of a month](https://datamesse.github.io/#/post/1632578400) to convert those into usable \"anchors\" to determine the offset value for the date/times.\r\n\r\n**Note:** This is not an appropriate solution to time zone application in terms of data accuracy, processing efficiency, and coding involved. Ideally a predefined dataset or an API with actual date/time values for observation period start/ends would be best.\r\n\r\nA good example of this can be found in [a blog post by John White](https://whitepages.unlimitedviz.com/2020/10/dynamic-time-zone-conversion-using-power-bi/)\r\n\r\nThis post shows how date/time anchor values can be used as an alternative way to solve this problem, which does not use calendar tables nor APIs.\r\n\r\nThis is the **[sample date/time dataset](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/Date_times_to_convert.xlsx?raw=true)** we will dynamically apply time zone offsets to via parameter selection.\r\n\r\nNote there is no time zone in the data itself, so assumptions made by any application (e.g. user's machine time zone) may be incorrect.\r\n![Sample date time dataset in Excel](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--02.png?raw=true)\r\n\r\n![Sample date time dataset imported into Power Query](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--03.png?raw=true)\r\n\r\nAfter importing the sample date/time dataset, we next import the combined time zone offset and daylight savings observation **[dataset](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/Time_zone_offsets_and_DST_observations.xlsx?raw=true)** created in this [post](https://datamesse.github.io/#/post/1633183200).\r\n\r\nAt this point, if you only need certain time zones to select from, you can filter for them here before proceeding.\r\n\r\n![Offset and observation dataset imported into Power Query](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--04.png?raw=true)\r\n\r\nNext, we will create a list from the Timezone column, to be used as the available selections of the parameter.\r\n\r\n![Power Query Convert to List Part A](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--05.png?raw=true)\r\n\r\n![Power Query Convert to List Part B](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--06.png?raw=true)\r\n\r\nThen set up the parameter itself to pull from that list.\r\n\r\n![Power Query Create new parameter](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--07.png?raw=true)\r\n\r\n![Power Query Manage Parameters Part A](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--08.png?raw=true)\r\n\r\n![Power Query Manage Parameters Part B](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--09.png?raw=true)\r\n\r\nNext, we create a new column in the sample dataset whose value is the parameter selection.\r\n\r\n![Power Query custom column for Parameter value Part A](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--10.png?raw=true)\r\n\r\n![Power Query custom column for Parameter value Part B](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--11.png?raw=true)\r\n\r\nThen merge the two datasets using that new custom column.\r\n![Power Query Merge Queries](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--12.png?raw=true)\r\n\r\n![Power Query Merge Queries](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--13.png?raw=true)\r\n\r\nWhilst expanding the merged table, we can prefix the column names, which may be useful if intending to merge multiple times, e.g. parameter for data source's actual time zone vs parameter for desired time zone. We will only do the merge once, in this example.\r\n\r\n![Power Query expand merge queries Part A](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--14.png?raw=true)\r\n\r\n![Power Query expand merge queries Part B](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--15.png?raw=true)\r\n\r\nOur next step is to create a custom column to convert our Date/Time value to match the time zone of the parameter selection. This is not as simple as appending an offset to our Date/Time value, because of these considerations\r\n\r\na. The need to account for different Date/Time offsets based on standard vs daylight savings.\r\n\r\nb. The anchor dataset (which determines whether or not daylight savings is applied) has a mix of data structures e.g. anchor date/times can be either UTC or local time-based, and can either have a specific date of the month, or relative day position of the month.\r\n\r\nc. The datasets' standard and daylight saving offset values are in a text based structure e.g. \"+10:00\", rather than straight numbers, which are more easily consumed by Power Query functions (e.g. *DateTime.AddZone()*).\r\n\r\nBefore we create the custom column, we will need 3 custom functions.\r\n1. A simple suffix of the standard or daylight daylight offset to the Date/Time value to make it a DateTimeZone value, which we'll name **DatetimeToDatetimezone**.\r\n2. A slightly more complex function that pulls in all date anchor values to convert to an actual date. But it only lets single parameters to pass for the time anchor (which could be UTC or local time) and offset (standard or daylight saving), which we'll name **AnchorToDatetimezone**.\r\n3. The complex function that applies the time zone to the Date/Time value, factoring in daylight saving and standard time observation by using the previous two functions, which we'll name **DatetimeAppendZone**.\r\n\r\nBeginning with the simple **DatetimeToDatetimezone** custom function, which is meant to resolve consideration *c)*.\r\n\r\n![Power Query create custom function Part A](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--16.png?raw=true)\r\n\r\n![Power Query create custom function Part B](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--17.png?raw=true)\r\n\r\n```\r\nlet\r\n  DatetimeToDatetimezone = (DateTimestamp as datetime, Offset as nullable text) => \r\n    DateTimeZone.FromText(DateTime.ToText(DateTimestamp) & \" \" & Offset)\r\nin\r\n  DatetimeToDatetimezone\r\n```\r\n\r\n![Power Query create 1st custom function](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--18.png?raw=true)\r\n\r\nNext we create the second custom function **AnchorToDatetimezone**, which takes in the dependent date/time value to get the year, and nullable parameters for time, offset, and the anchors for month, day, and *n*th occurrence of the day within the month. It checks the data structure to create the anchor's date, and appends the time and offset values passed into it.\r\n\r\n* The first if condition checks that there are no incorrect nor incomplete data structures.\r\n* The first else if condition creates the date time zone if there is a date anchor e.g. \"21st\" of given month.\r\n* The second and third else if conditions convert to date time zones based on 1st, 2nd, 3rd, or 4th occurrence of specified day anchors, and are based on this previous [post](https://datamesse.github.io/#/post/1632578400).\r\n* The fourth and fifth if else conditions convert based on the last occurrence of the specified day anchor, based on the last section of that same [post](https://datamesse.github.io/#/post/1632578400).\r\n\r\n```\r\nlet\r\n  AnchorToDatetimezone = (DateTimestamp as datetime, MonthAnchor as nullable number, DayAnchor as nullable number, PositionAnchor as nullable number, DateAnchor as nullable number, Time as nullable time, Offset as nullable text) => \r\n  /* Error-handling based on insufficient data or incorrect value combination */\r\n  if (MonthAnchor = null) \r\n    or (DateAnchor = null and DayAnchor = null)\r\n    or (DateAnchor <> null and DayAnchor <> null)\r\n    or (DayAnchor <> null and PositionAnchor = null)\r\n    or (DayAnchor = null and PositionAnchor <> null)\r\n    or (Time = null)\r\n    or (Offset = null)\r\n  then \"Incomplete data\"\r\n  /* Applying time zone to DateTimestamp, with separate conditions for position anchor = 9 i.e. \"Last\" */\r\n  else if DateAnchor <> null\r\n    then Text.From(Date.Year(DateTimestamp)) & \"/\" & Text.From(MonthAnchor)  & \"/\" & Text.From(DateAnchor) & \" \" & Text.From(Time) & Offset\r\n  else if DayAnchor < 6 and PositionAnchor > 0 and PositionAnchor < 5\r\n  /* Optional parameter in Date.DayOfWeek 1 = Day.Monday will get Sunday, hence DayAnchor (Sunday = 0) + 1 */\r\n    then Text.From(Date.Year(DateTimestamp)) & \"/\" & Text.From(MonthAnchor) & \"/\" & Text.From( (7 - Date.DayOfWeek(Date.FromText(Text.From(Date.Year(DateTimestamp)) & \"/\" & Text.From(MonthAnchor) & \"/1\"), DayAnchor + 1)) + (-7 + (7 * PositionAnchor)) ) & \" \" & Text.From(Time) & Offset\r\n  /* Need to pass Day.Sunday to get Saturday */\r\n  else if DayAnchor = 6 and PositionAnchor > 0 and PositionAnchor < 5\r\n    then Text.From(Date.Year(DateTimestamp)) & \"/\" & Text.From(MonthAnchor) & \"/\" & Text.From( (7 - Date.DayOfWeek(Date.FromText(Text.From(Date.Year(DateTimestamp)) & \"/\" & Text.From(MonthAnchor) & \"/1\"), Day.Sunday )) + (-7 + (7 * PositionAnchor)) ) & \" \" & Text.From(Time) & Offset\r\n  /* handling for last specific day of month */\r\n  else if DayAnchor = 0 and PositionAnchor = 9\r\n    then Text.From(Date.AddDays(Date.EndOfMonth(Date.From(Text.From(Date.Year(DateTimestamp)) & \"/\" & Text.From(MonthAnchor) & \"/1\")),(-1 * Number.From(Date.DayOfWeek(Date.EndOfMonth(Date.From(Text.From(Date.Year(DateTimestamp)) & \"/\" & Text.From(MonthAnchor) & \"/1\")), Day.Sunday))))) & \" \" & Text.From(Time) & Offset\r\n  else if DayAnchor > 0 and PositionAnchor = 9\r\n    then Text.From(Date.AddDays(Date.EndOfMonth(Date.From(Text.From(Date.Year(DateTimestamp)) & \"/\" & Text.From(MonthAnchor) & \"/1\")),(-1 * (Number.From(Date.DayOfWeek(Date.EndOfMonth(Date.From(Text.From(Date.Year(DateTimestamp)) & \"/\" & Text.From(MonthAnchor) & \"/1\")), Day.Sunday)) + ( 7 - DayAnchor ))))) & \" \" & Text.From(Time) & Offset\r\n  else null\r\nin\r\n  AnchorToDatetimezone\r\n```\r\n\r\n![Power Query create 2nd custom function](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--19.png?raw=true)\r\n\r\nThe third custom function **DatetimeAppendZone** uses the **AnchorToDatetimezone** custom function to create date/time anchors for the start and end of daylight savings, then compares them with the Date/Time value to determine if standard or daylight savings offsets should be suffixed to the Date/Time value, using the **DatetimeToDatetimezone** custom function.\r\n\r\nSome data sources may be incorrectly failing to account for daylight savings differences. In this third custom function, you can use the \"Difference\" parameter to add/subtract the missed hour(s) to compensate for those. The code below does not do that, but you can add it in, if required.\r\n\r\n```\r\nlet\r\n  DatetimeAppendZone = (DateTimestamp as datetime, Difference as number, StandardOffset as nullable text, DaylightOffset as nullable text, DSTstartAncDate as nullable number, DSTstartAncPosition as nullable number, DSTstartAncDay as nullable number, DSTstartAncMonth as nullable number, DSTstartAncUTC as nullable time, DSTstartAncLocal as nullable time, DSTendAncDate as nullable number, DSTendAncPosition as nullable number, DSTendAncDay as nullable number, DSTendAncMonth as nullable number, DSTendAncUTC as nullable time, DSTendAncLocal as nullable time) => \r\n  /* Validation to ensure same time anchor types for start and end are used */\r\n  if Difference <> 0 and ( (DSTstartAncLocal = null and DSTstartAncUTC = null) or (DSTendAncLocal = null and DSTendAncUTC = null) )\r\n    then \"Incomplete data\"\r\n\r\n  /* Where DST is not observed, just append Standard UTC offset */\r\n  else if Difference = 0\r\n    then DatetimeToDatetimezone(DateTimestamp, StandardOffset)\r\n\r\n  /* From this point on, if using a data source that doesn't properly account for Daylight Savings offsets, you can factor those into the calculations */\r\n\r\n  /* Where DST is observed with Standard time result */\r\n  else if Difference <> 0\r\n    and (\r\n          (\r\n           /* Where local offset is used, 1 DST period in same year, datetimestamp is outside daylight savings */\r\n           DSTstartAncLocal <> null and DSTstartAncMonth < DSTendAncMonth\r\n           and ( DatetimeToDatetimezone(DateTimestamp,StandardOffset) < DateTimeZone.From(AnchorToDatetimezone(DateTimestamp, DSTstartAncMonth, DSTstartAncDay, DSTstartAncPosition, DSTstartAncDate, DSTstartAncLocal, StandardOffset))\r\n                or DatetimeToDatetimezone(DateTimestamp,StandardOffset) > DateTimeZone.From(AnchorToDatetimezone(DateTimestamp, DSTendAncMonth, DSTendAncDay, DSTendAncPosition, DSTendAncDate, DSTendAncLocal, StandardOffset)) )\r\n          )\r\n      or  (\r\n           /* Where local offset is used, 2 DST periods in same year, datetimestamp is outside both daylight savings periods */\r\n           DSTstartAncLocal <> null and DSTstartAncMonth > DSTendAncMonth \r\n           and Date.Month(DateTimestamp) >= DSTendAncMonth and Date.Month(DateTimestamp) <= DSTstartAncMonth\r\n           and DatetimeToDatetimezone(DateTimestamp,DaylightOffset) > DateTimeZone.From(AnchorToDatetimezone(DateTimestamp, DSTendAncMonth, DSTendAncDay, DSTendAncPosition, DSTendAncDate, DSTendAncLocal, DaylightOffset))\r\n           and DatetimeToDatetimezone(DateTimestamp,DaylightOffset) < DateTimeZone.From(AnchorToDatetimezone(Date.AddYears(DateTimestamp,1), DSTstartAncMonth, DSTstartAncDay, DSTstartAncPosition, DSTstartAncDate, DSTstartAncLocal, DaylightOffset))           \r\n          )\r\n      or  (\r\n           /* Where UTC offset is used, 1 DST period in same year, datetimestamp is inside daylight savings */\r\n           DSTstartAncUTC <> null and DSTstartAncMonth < DSTendAncMonth\r\n           and ( DateTimeZone.ToUtc(DatetimeToDatetimezone(DateTimestamp,StandardOffset)) <  DateTimeZone.FromText(AnchorToDatetimezone(DateTimestamp, DSTstartAncMonth, DSTstartAncDay, DSTstartAncPosition, DSTstartAncDate, DSTstartAncUTC, \"+00:00\"))\r\n                 or DateTimeZone.ToUtc(DatetimeToDatetimezone(DateTimestamp,StandardOffset)) > DateTimeZone.FromText(AnchorToDatetimezone(DateTimestamp, DSTendAncMonth, DSTendAncDay, DSTendAncPosition, DSTendAncDate, DSTendAncUTC, \"+00:00\")) )\r\n          )\r\n    )\r\n    then DatetimeToDatetimezone(DateTimestamp, StandardOffset)\r\n\r\n  /* Where DST is observed with Dayliht Saving time result */\r\n  else if Difference <> 0\r\n    and (\r\n          (\r\n           /* Where local offset is used, 1 DST period within same year, datetimestamp is inside daylight savings */    \r\n           DSTstartAncLocal <> null and DSTstartAncMonth < DSTendAncMonth\r\n           and ( DatetimeToDatetimezone(DateTimestamp,StandardOffset) >= DateTimeZone.From(AnchorToDatetimezone(DateTimestamp, DSTstartAncMonth, DSTstartAncDay, DSTstartAncPosition, DSTstartAncDate, DSTstartAncLocal, StandardOffset))\r\n                or DatetimeToDatetimezone(DateTimestamp,StandardOffset) <= DateTimeZone.From(AnchorToDatetimezone(DateTimestamp, DSTendAncMonth, DSTendAncDay, DSTendAncPosition, DSTendAncDate, DSTendAncLocal, StandardOffset)) )\r\n          )\r\n      or  (\r\n           /* Where local offset is used, 2 DST periods in same year, datetimestamp is inside 1st daylight savings period */\r\n           DSTstartAncLocal <> null and DSTstartAncMonth > DSTendAncMonth and Date.Month(DateTimestamp) <= DSTendAncMonth\r\n           and DatetimeToDatetimezone(DateTimestamp,DaylightOffset) <= DateTimeZone.From(AnchorToDatetimezone(DateTimestamp, DSTendAncMonth, DSTendAncDay, DSTendAncPosition, DSTendAncDate, DSTendAncLocal, DaylightOffset))\r\n          )\r\n      or  (\r\n           /* Where local offset is used, 2 DST periods in same year, datetimestamp is inside 2nd daylight savings period */\r\n           DSTstartAncLocal <> null and DSTstartAncMonth > DSTendAncMonth and Date.Month(DateTimestamp) >= DSTendAncMonth\r\n           and DatetimeToDatetimezone(DateTimestamp,DaylightOffset) >= DateTimeZone.From(AnchorToDatetimezone(DateTimestamp, DSTstartAncMonth, DSTstartAncDay, DSTstartAncPosition, DSTstartAncDate, DSTstartAncLocal, DaylightOffset))\r\n          )\r\n      or  (\r\n           /* Where UTC offset is used, 1 DST period in same year, datetimestamp is outside daylight savings */\r\n           DSTstartAncUTC <> null and DSTstartAncMonth < DSTendAncMonth\r\n           and DateTimeZone.ToUtc(DatetimeToDatetimezone(DateTimestamp,StandardOffset)) >=  DateTimeZone.From(AnchorToDatetimezone(DateTimestamp, DSTstartAncMonth, DSTstartAncDay, DSTstartAncPosition, DSTstartAncDate, DSTstartAncUTC, \"+00:00\"))\r\n           and DateTimeZone.ToUtc(DatetimeToDatetimezone(DateTimestamp,StandardOffset)) <= DateTimeZone.From(AnchorToDatetimezone(DateTimestamp, DSTendAncMonth, DSTendAncDay, DSTendAncPosition, DSTendAncDate, DSTendAncUTC, \"+00:00\")) \r\n          )\r\n    )\r\n    then DatetimeToDatetimezone(DateTimestamp, DaylightOffset)\r\n  else null\r\nin\r\n  DatetimeAppendZone\r\n```\r\n\r\n![Power Query create 3rd custom function](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--20.png?raw=true)\r\n\r\nNow that our functions are complete, we can use the third **DatetimeAppendZone** to create the custom column which converts Date/Time to a datetimezone value, by passing in all the respective anchors as parameters.\r\n\r\n```\r\nDatetimeAppendZone([#\"Date/Time\"],[#\"Daylight offset - Standard offset\"],[Standard UTC offset],[Daylight Saving UTC offset],[#\"DST start (date anchor)\"],[#\"DST start (position anchor)\"],[#\"DST start (day anchor)\"],[#\"DST start (month anchor)\"],[#\"DST start (UTC time anchor)\"],[#\"DST start (local time anchor)\"],[#\"DST end (date anchor)\"],[#\"DST end (position anchor)\"],[#\"DST end (day anchor)\"],[#\"DST end (month anchor)\"],[#\"DST end (UTC time anchor)\"],[#\"DST end (local time anchor)\"])\r\n```\r\n\r\n![Power BI Create report](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--21.png?raw=true)\r\n\r\nIf we now create a table in the Power BI's report designer, we can see the appropriate suffixing of standard vs daylight saving offsets to our Date/Time values as we change the parameter. Then we can test to see if, regardless of the data structure used for the daylight savings anchoring, that the appropriate offset is applied to our Date/Time dataset.\r\n\r\nAustralia, Sydney is a time zone that uses local time and non-\"last position\" for its daylight savings start and end anchors.\r\n![Power BI Sydney example Part A](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--22a.png?raw=true)\r\n\r\n![Power BI Sydney example Part B](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--22.png?raw=true)\r\n\r\nEurope, Dublin is a time zone that uses UTC time and \"last position\" (indicated by the 9), for its anchors.\r\n![Power BI Dublin example Part A](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--23a.png?raw=true)\r\n\r\n![Power BI Dublin example Part B](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--23.png?raw=true)\r\n\r\nAfrica, Casablanca is a time zone that uses a fixed date of the month for its anchors.\r\n![Power BI Casablanca example Part A](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--24a.png?raw=true)\r\n\r\n![Power BI Casablanca example Part B](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--24.png?raw=true)\r\n\r\nSo now we have created a parameterised way of defining our data source's time zone, with potential to compensate for skipped daylight savings conversions it may have, by using the concept of anchor values to construct relative date/times.\r\n\r\nUse cases for this include:\r\n1. Dynamically converting date/times to another time zone e.g. a report reader from one time zone wanting to know the date/times from the perspective of another time zone.\r\n2. The admittedly rare instance where the time zone of date/times may vary at access or export when creating a datset.\r\n\r\nAn example where I encountered use case #2, was manually exporting data from Explore, the reporting tool for the Zendesk customer service platform, where date/times are automatically converted to match the time zone of the extractor's Zendesk login.\r\n\r\nIn that scenario, if multiple people from different time zones are creating or maintaining dashboards not made in the native Explore tool (e.g. via Power BI), their extract date times can be inconsistent.\r\n\r\nThere are simple ways around this:\r\n* Creating a shared account fixed to a specific time zone for data extracts.\r\n* Having access to the data source's API.\r\n\r\nYou can take this further for other solutions, such as hard-coding the desired time zone, or make it based on the values of another column e.g. time zone is based on country or city values.\r\n\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2021-10-10.md)** for this post's markdown file in GitHub."},{"id":1633183200,"title":"Import time zone offsets and observations from Wikipedia in Power BI","tag":"logo-powerbi","date":"October 3, 2021","content":"\r\nHow to use Power BI to scrape Wikipedia pages and create a data source for UTC time zone offsets and daylight saving observation anchors (e.g. the first Sunday of October).\r\n\r\nHere we will be importing Wikipedia table data from 2 different pages. The first example contains structured data values requiring minimal data cleaning. The second contains data which requires disaggregation of qualitative information to make it more quantitative.\r\n\r\nTime zone offset hours\r\n[https://en.wikipedia.org/wiki/List_of_tz_database_time_zones](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones)\r\n\r\n![Wikipedia List of tz database time zones](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--01.png?raw=true)\r\n\r\nDaylight Saving observation period anchors\r\n[https://en.wikipedia.org/wiki/Daylight_saving_time_by_country](https://en.wikipedia.org/wiki/Daylight_saving_time_by_country)\r\n\r\n![Wikipedia Daylight saving time by country](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--02.png?raw=true)\r\n\r\n**Exercise 1:**\r\n\r\nBeginning with the time zone offset hours, we Get Data from Web and provide the URL.\r\n\r\n![Power BI Import data from a web page](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--03.png?raw=true)\r\n\r\nSelect the Basic option. The intention is to export the results to Excel, as opposed to a live ongoing connection. This is to mitigate problems regarding source page changes and connection delays.\r\n\r\n![Power BI Import data Basic setting and set URL](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--04.png?raw=true)\r\n\r\nThe HTML table we are after is the list containing the offsets.\r\nTick it, then click Transform Data.\r\n\r\n![Power BI Import data web page table selection](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--05.png?raw=true)\r\n\r\nNext we Use First Row as Headers to assign the column names.\r\n\r\n![Power Query Use First Row as Headers](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--06.png?raw=true)\r\n\r\nThen we add new custom columns to substitute existing columns to clean the data.\r\n\r\nFirst we add a new column to substitute the TZ database name column, replacing the single forward slashes “/” with forward slashes surrounded by spaces “ / “, and replace the underscores “_” with spaces “ “, for readability.\r\n\r\n```\r\nText.Replace(Text.Replace([TZ database name],\"/\", \", \"),\"_\",\" \")\r\n```\r\n\r\n![Power Query Replace text to make more readable](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--07.png?raw=true)\r\n\r\nSecondly, the data source’s offsets use a different dash character \"−\" (slightly longer) from the mathematical operator \"-\" (shorter), so we need to create custom columns to substitute the longer dash with the shorter one.\r\n\r\nFor the Standard UTC offset:\r\n\r\n```\r\nText.Replace([#\"UTC offset ±hh:mm\"],\"−\",\"-\")\r\n```\r\n![Power Query Custom Column: Standard UTC offset](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--08.png?raw=true)\r\n\r\nFor the Daylight Saving UTC offset:\r\n\r\n```\r\nText.Replace([#\"UTC DST offset ±hh:mm\"],\"−\",\"-\")\r\n```\r\n![Power Query Custom Column: Daylight Saving UTC offset](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--09.png?raw=true)\r\n\r\nNow to add a column that shows the difference between the standard and daylight savings offsets.\r\nThe nature of the data means you cannot subtract them in a simple way.\r\nComments are included in the code to explain what is occurring.\r\n\r\n```\r\n/* If the offsets are identical, it may imply no Daylight Saving observed */\r\nif [Standard UTC offset] = [Daylight Saving UTC offset]\r\nthen 0\r\n\r\n/* If minutes are the same and aren't zero, just subtract hours */\r\nelse if (Text.End([Standard UTC offset],2) <> \"00\" or \r\nText.End([Daylight Saving UTC offset],2) <> \"00\") and Text.End([Standard UTC offset],2) = Text.End([Daylight Saving UTC offset],2)\r\nthen Number.FromText(Text.Range([Daylight Saving UTC offset],0,3)) - Number.FromText(Text.Range([Standard UTC offset],0,3))\r\n\r\n/* If minutes are different and either of them aren't zero, convert minutes to proper decimals, subtract, then convert minutes back */\r\nelse if (Text.End([Standard UTC offset],2) <> \"00\" or \r\nText.End([Daylight Saving UTC offset],2) <> \"00\") and Text.End([Standard UTC offset],2) <> Text.End([Daylight Saving UTC offset],2)\r\nthen (Number.FromText(Text.Range([Daylight Saving UTC offset],1,2)) + (Number.FromText(Text.End([Daylight Saving UTC offset],2)) / 60)) - (Number.FromText(Text.Range([Standard UTC offset],1,2)) + (Number.FromText(Text.End([Standard UTC offset],2)) / 60))\r\n\r\n/* Standard expectation that difference is only in the hour values */\r\nelse Number.FromText(Text.Range([Daylight Saving UTC offset],1,2)) - Number.FromText(Text.Range([Standard UTC offset],1,2))\r\n```\r\n\r\n![Power Query Custom Column: Offset difference](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--10.png?raw=true)\r\n\r\nNext we filter out the data rows not required, starting with only including Canonical status offsets\r\n\r\n![Power Query Filter for Canonical records](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--11.png?raw=true)\r\n\r\nThen we filter for time zones that have a country code.\r\n\r\n![Power Query Filter for country codes](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--12.png?raw=true)\r\n\r\nLastly, remove columns that won’t be needed, depending on what you need for your data source.\r\n\r\n![Power Query Remove other columns](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--13.png?raw=true)\r\n\r\nIn my scenario, I want to retain this data separately in an Excel file, so I create a table in Power BI with all the columns, then Export.\r\n\r\n![Power BI Export table results](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--14.png?raw=true)\r\n\r\n\r\n\r\n**Exercise 2:**\r\n\r\nFor our second dataset, we need to retrieve the relative anchors for daylight saving periods using the second URL: [https://en.wikipedia.org/wiki/Daylight_saving_time_by_country](https://en.wikipedia.org/wiki/Daylight_saving_time_by_country)\r\n\r\n![Power BI Import data Basic setting and set URL](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--15.png?raw=true)\r\n\r\nAgain, click Transform Data and Use First Row as Headers.\r\n\r\n![Power BI Import data web page table selection](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--16.png?raw=true)\r\n\r\n![Power Query Use First Row as Headers](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--17.png?raw=true)\r\n\r\nNext we filter for records with a valid current DST start value.\r\n\r\n![Power Query Filter for valid DST start](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--18.png?raw=true)\r\n\r\nThe problem with this dataset is that the DST start and DST end columns are not quantified at a low enough level to be easily worked with. Revising these columns, we can see value commonalities that can be separated out into custom columns as declartive “anchors” for each daylight saving period’s start and end.\r\n\r\nThis includes:\r\n* Positions (i.e. first, second, third, fourth, last)\r\n* Weekday names\r\n* Month names\r\n* “UTC” prefixed with a specific UTC time (e.g. 01:00 UTC), or prefixed with a non-UTC time (e.g. 002:00 AST (UTC-4)\r\n* Phrases “local standard time” and “local daylight saving time” prefixed with a time\r\n\r\n![Power Query Exploring qualitative data values](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--19.png?raw=true)\r\n\r\nThe custom columns are base conditions that check for substrings and substitute them with numerical data where possible, so they can be referenced by other custom functions.\r\n\r\nWe’ll begin with the position-related column.\r\n\r\nEvery day has at least four occurrences in each month, but the “last” position could either be the fourth or fifth occurrence of that day. I chose to use an arbitrary value of 9 for the output of last, given 5 could possibly be used for the fifth instance of the day. Note: With this particular dataset, neither Fourth nor Fifth occur, so they can be omitted here if you want.\r\n\r\n```\r\nif Text.Contains([DST start], \"First\")\r\nthen 1\r\nelse if Text.Contains([DST start], \"Second\")\r\nthen 2\r\nelse if Text.Contains([DST start], \"Third\")\r\nthen 3\r\nelse if Text.Contains([DST start], \"Fourth\")\r\nthen 4\r\nelse if Text.Contains([DST start], \"Fifth\")\r\nthen 5\r\nelse if Text.Contains([DST start], \"Last\")\r\nthen 9\r\nelse null\r\n```\r\n\r\n![Power Query Custom Column: DST start position anchor](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--20.png?raw=true)\r\n\r\nNext to create a custom column for the weekdays, using Power Query’s Day functions, which translate as numbers from 0 for Sunday to 6 for Saturday.\r\n\r\n```\r\nif Text.Contains([DST start], \"Sunday\")\r\nthen Day.Sunday\r\nelse \r\nif Text.Contains([DST start], \"Monday\")\r\nthen Day.Monday\r\nelse \r\nif Text.Contains([DST start], \"Tuesday\")\r\nthen Day.Tuesday\r\nelse \r\nif Text.Contains([DST start], \"Wednesday\")\r\nthen Day.Wednesday\r\nelse \r\nif Text.Contains([DST start], \"Thursday\")\r\nthen Day.Thursday\r\nelse \r\nif Text.Contains([DST start], \"Friday\")\r\nthen Day.Friday\r\nelse \r\nif Text.Contains([DST start], \"Saturday\")\r\nthen Day.Saturday\r\nelse null\r\n```\r\n\r\n![Power Query Custom Column: DST start day anchor](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--21.png?raw=true)\r\n\r\nWe repeat with similar logic for the DST start field’s month anchor. At time of writing, Power Query has a function that converts month numbers to month names, but not the other way around.\r\n\r\n```\r\nif Text.Contains([DST start], \"January\")\r\nthen 1\r\nelse if Text.Contains([DST start], \"February\")\r\nthen 2\r\nelse if Text.Contains([DST start], \"March\")\r\nthen 3\r\nelse if Text.Contains([DST start], \"April\")\r\nthen 4\r\nelse if Text.Contains([DST start], \"May\")\r\nthen 5\r\nelse if Text.Contains([DST start], \"June\")\r\nthen 6\r\nelse if Text.Contains([DST start], \"July\")\r\nthen 7\r\nelse if Text.Contains([DST start], \"August\")\r\nthen 8\r\nelse if Text.Contains([DST start], \"September\")\r\nthen 9\r\nelse if Text.Contains([DST start], \"October\")\r\nthen 10\r\nelse if Text.Contains([DST start], \"November\")\r\nthen 11\r\nelse if Text.Contains([DST start], \"December\")\r\nthen 12\r\nelse null\r\n```\r\n\r\n![Power Query Custom Column: DST start month anchor](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--22.png?raw=true)\r\n\r\nLastly, we will pull where a UTC time is specified. There are entries where a local time with its UTC offset value is provided, but since these entries are few and complex to manage, I will edit the export result afterward to account for these. It’s a cost vs benefit juggle.\r\n\r\n```\r\nif Text.Contains([DST start], \" UTC\") then Text.Range([DST start], Text.PositionOf([DST start],\" UTC\") - 5, 5) else null\r\n```\r\n\r\n![Power Query Custom Column: DST start UTC time anchor](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--23.png?raw=true)\r\n\r\nRinse-and-repeat the creation of those anchor columns. An alternative is creating a custom function to make it easier to manage later on.\r\n\r\n![Power Query Custom Columns for DST end](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--24.png?raw=true)\r\n\r\nThen we can retain the columns we need, such as Country/Territory, Notes, and the custom columns we created.\r\n\r\n![Power Query Remove other columns](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--25.png?raw=true)\r\n\r\nLastly, as with the previous dataset, we will export this to Excel, and clean up the file from there, e.g. accounting for records that have a different data structure for their anchors, such as an exact date for day and month per year, and records that include local time, etc.\r\n\r\n![Power BI Export table results](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--26.png?raw=true)\r\n\r\nAt this point, we typically would merge these datasets, similar to left outer joins in SQL. Unfortunately, the first dataset uses an incoherent structure for its time zone name values, e.g. _country, city_ or _region, city_ or _region, country, city_ etc., as opposed to the second data set, which only lists country.\r\n\r\n![Power Query Merge Queries](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--27.png?raw=true)\r\n\r\nI tried fuzzy matching, but as at time of writing, it cannot connect a high enough number of the records, regardless of adjustments made to the accuracy.\r\n\r\n![Power Query Merge using fuzzy matching](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--28.png?raw=true)\r\n\r\nAn alternative solution would be to create a list based on the second dataset’s county column, but this would neglect the _region, city_ joins from the first dataset. Another would be to find a third dataset to extend the other datasets and formulate a common column for the merge.\r\n\r\nIn my scenario, it would be more time efficient to do the mapping manually, as this dataset is small, and intended for a niche non-scaled need. \r\n\r\nFind a copy of the end product to download as an Excel file **[here](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/Time_zone_offsets_and_DST_observations.xlsx?raw=true)**.\r\n\r\nAs a reminder, this is strictly an exercise file, and its data is not comprehensive nor accurate, so please be mindful of that if using it.\r\n\r\n![Manually cleaned output](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--29.png?raw=true)\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2021-10-03.md)** for this post's markdown file in GitHub."},{"id":1632578400,"title":"Find date for the nth day of a month in Power BI","tag":"logo-powerbi","date":"September 26, 2021","content":"\r\nHow to use Power Query to find the date for the nth day of a month/year based on another date column (e.g. 3rd Tuesday of October 2021).\r\n\r\nIn Power BI this can be used for the conditional logic of other Custom Columns. For example, to create indicators that data rows occur on or fall between relative date ranges (e.g. Black Friday sales). The following involves adding a Custom Column in Power Query i.e. M code, not DAX.\r\n\r\nThis finds the first Monday of the month, where our dependent date column is OurDateField.\r\n\r\n```\r\nDate.FromText(Text.From(Date.Year([OurDateField])) & \"/10/\" & Text.From((7 - (Date.DayOfWeek(Date.FromText(Text.From(Date.Year([OurDateField])) & \"/10/1\"),Day.Monday)))))\r\n```\r\n\r\n![Power Query: 1st Sunday of month](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--01.png?raw=true)\r\n\r\n**How it works**\r\n\r\nTo find the first Sunday of a specific month/year relative to another date, first establish the start of the month e.g. 1/10 (1st October), passing in the date field you are using e.g. [OurDateField], to append its year.\r\n\r\n```\r\nDate.FromText(Text.From(Date.Year([OurDateField])) & \"/10/1\")\r\n```\r\n\r\nIn this example, we are hard-coding October regardless of OurDateField’s month value, but if you need it to be relative to its month too, simply add an extra concatenation for month in the same way year is treated, i.e. using Date.Month().\r\n\r\nNow we need to identify what day of the week that this first day of the month is, using Date.DayOfWeek, and setting the optional parameter for what the start of the week is, as Day.Monday\r\n\r\n```\r\nDate.DayOfWeek(Date.FromText(Text.From(Date.Year([OurDateField])) & \"/10/1\"),Day.Monday)\r\n```\r\n\r\n![Power Query: Day of week number](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--02.png?raw=true)\r\n\r\n \r\nIn this example, 1st October 2021 is a Friday, and Friday’s day number is 4 (with Monday being 0).\r\n\r\n![Calendar: Weekday of 1st day of month](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--03.png?raw=true)\r\n\r\nIf you do not provide the Day.Monday parameter, it will default to Day.Monday in the background. If another parameter is used e.g. Day.Sunday, then the assignment numbers will change.\r\n\r\nNow we subtract the weekday number 4 from 7, and get 3, which is the first Sunday’s date.\r\n\r\n```\r\n7 - (Date.DayOfWeek(Date.FromText(Text.From(Date.Year([OurDateField])) & \"/10/1\"),Day.Monday))\r\n```\r\n\r\n![Power Query: Date of 1st Sunday](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--04.png?raw=true)\r\n\r\nThen concatenate this with the year month retrieved earlier\r\n\r\n```\r\nDate.FromText(Text.From(Date.Year([OurDateField])) & \"/10/\" & Text.From((7 - (Date.DayOfWeek(Date.FromText(Text.From(Date.Year([OurDateField])) & \"/10/1\"),Day.Monday)))))\r\n```\r\n\r\n![Power Query: Concatenate the month year to the date](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--05.png?raw=true)\r\n\r\nIf you need to change the weekday that Power Query needs to find, simply increment the Day.Monday parameter to the following day of the desired weekday.\r\n\r\nFor example, if you want to find the first Wednesday, change the parameter to Day.Thursday.\r\n\r\n```\r\nDate.FromText(Text.From(Date.Year([OurDateField])) & \"/10/\" & Text.From((7 - (Date.DayOfWeek(Date.FromText(Text.From(Date.Year([OurDateField])) & \"/10/1\"),Day.Thursday)))))\r\n```\r\n\r\n![Power Query: 1st Wednesday of month](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--06.png?raw=true)\r\n\r\n![Calendar: 1st Wednesday of the month](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--07.png?raw=true)\r\n\r\nIf you need to change the position from first, to second, third, or fourth Sunday, simply add 7 for the second, 14 for the third, and 21 for the fourth.\r\n\r\nFor example, we will retrieve the 3rd Sunday.\r\n\r\n```\r\nDate.FromText(Text.From(Date.Year([OurDateField])) & \"/10/\" & Text.From((7 - (Date.DayOfWeek(Date.FromText(Text.From(Date.Year([OurDateField])) & \"/10/1\"),Day.Monday)) + 14 )))\r\n```\r\n\r\n![Calendar: 3rd Sunday of the month](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--08.png?raw=true)\r\n\r\n\r\n\r\n**Edit: 9th October 2021**\r\n\r\nIf the last occurrence of a specific day in a month needs to be retrieved, it can possibly be the 4th or 5th instance of that day. Retrieving this may be required for conditional or other custom column dependencies. \r\n\r\nAs an example, this Power Query code finds the last Sunday of the month, where our dependent date column is OurDateField.\r\n\r\n```\r\nDate.AddDays(Date.EndOfMonth([OurDateField]),(-1 * Number.From(Date.DayOfWeek(Date.EndOfMonth([OurDateField]), Day.Sunday))))\r\n```\r\n![Power Query Day number of last of the month year Day.Sunday](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--12.png?raw=true)\r\n\r\n**How it works**\r\n\r\nWe will try to retrieve the last Sunday of a specific month/year, passing in our relative *OurDateField*.\r\n\r\n```\r\nDate.EndOfMonth([OurDateField])\r\n```\r\n![Power Query last day of the month year](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--09.png?raw=true)\r\n\r\nThen find out which day of the week it is.\r\n\r\n```\r\nDate.DayOfWeek(Date.EndOfMonth([OurDateField]), Day.Monday)\r\n```\r\n![Power Query Day number of last of the month year Day.Monday](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--10.png?raw=true)\r\n\r\nIn this example, 31st January 2021 is a Sunday, and Sunday’s day number is 6. This is if the optional parameter for start of the week is Day.Monday (which is the default, if not provided).\r\n\r\n![Calendar using Day.Monday](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--11.png?raw=true)\r\n\r\nNow we will swap that Day.Monday parameter out with Day.Sunday, so that the value for Sunday becomes 0 instead of 6.\r\n\r\n```\r\nDate.DayOfWeek(Date.EndOfMonth([OurDateField]), Day.Sunday)\r\n```\r\n![Power Query Day number of last of the month year Day.Sunday](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--12.png?raw=true)\r\n\r\n![Calendar using Day.Sunday](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--13.png?raw=true)\r\n\r\nWe then minus this number from the last date of the month, to get the last Sunday of the month, whih ironically is the same day i.e. Sunday 31/01/2021 - 0 = 31/01/2021. We do this using the Date.AddDays function and multiplying the number with -1.\r\n\r\n```\r\nDate.AddDays(Date.EndOfMonth([OurDateField]),(-1 * Number.From(Date.DayOfWeek(Date.EndOfMonth([OurDateField]), Day.Sunday))))\r\n```\r\n![Power Query last date Sunday](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--14.png?raw=true)\r\n\r\nLastly, if you need to pick any other last specific day of the month, just increment by one for each day you want to go earlier in the week e.g. Saturday is +1, Friday is +2 etc.\r\n\r\nFor example, the last Friday of the month year.\r\n\r\n```\r\nDate.AddDays(Date.EndOfMonth([OurDateField]),(-1 * (Number.From(Date.DayOfWeek(Date.EndOfMonth([OurDateField]), Day.Sunday)) + 2)))\r\n```\r\n![Power Query last date Friday](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--15.png?raw=true)\r\n\r\n![Power Query last date Friday](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--16.png?raw=true)\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2021-09-26.md)** for this post's markdown file in GitHub."}]