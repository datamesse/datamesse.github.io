(window.webpackJsonp=window.webpackJsonp||[]).push([[0],[,function(e,t,a){},,,,,,,,function(e){e.exports=[{id:1669381200,title:"How to use geographic maps in Power BI with AppSource Deneb",tag:"logo-powerbi",date:"26 November 2022",content:'\r\nThis post shows how to create country-level maps with AppSource Deneb in Power BI, and circumventing the external data source restriction by embedding the map data directly in your visual.\r\n\r\nThe final Power BI report *"AppSource Deneb Maps"* is available here:\r\n* **[https://community.powerbi.com/t5/Data-Stories-Gallery/AppSource-Deneb-Maps/m-p/2930366](https://community.powerbi.com/t5/Data-Stories-Gallery/AppSource-Deneb-Maps/m-p/2930366)**\r\n\r\n![My Power BI visualisation](https://github.com/datamesse/datamesse.github.io/blob/main/src/assets-portfolio/img-2022-11-power-bi-appsource-deneb-maps.gif?raw=true)\r\n\r\n\r\nMany visualisations from the Vega and Vega-Lite Example Galleries can be copy-and-pasted into Power BI with Deneb.\r\n\r\n![Vega-Lite: Simple Bar Chart](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--01.png?raw=true)\r\n\r\n![Deneb: Simple Bar Chart](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--02.png?raw=true)\r\n\r\nA key exception are geographic maps because those templates connect to a web-hosted data file, and AppSource visuals are not allowed to access *"external services or resources"*.\r\n\r\n* **[https://learn.microsoft.com/en-us/power-bi/developer/visuals/power-bi-custom-visuals-certified](https://learn.microsoft.com/en-us/power-bi/developer/visuals/power-bi-custom-visuals-certified)**\r\n\r\n![Vega-Lite: Choropleth of Unemployment Rate per County](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--03.png?raw=true)\r\n\r\n![Deneb: Choropleth of Unemployment Rate per County template](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--04.png?raw=true)\r\n\r\nLogically, the workaround should be to embed the map data into the visual itself. But there are notable issues:\r\n\r\n* The map data may not have everything you need (e.g. country names) to map back to your Power BI dataset.\r\n\r\n![Vega-Lite: Example Gallery map data has no Country information](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--05.png?raw=true)\r\n\r\n* Copying-and-pasting map data from the Example Gallery as-is without understanding the data structure may not work.\r\n\r\n![Deneb: Copy-and-pasting American state map data](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--06.png?raw=true)\r\n\r\n* Performance issues editing the specification whilst the map data is embedded in it.\r\n\r\n\r\n## CREATE A COUNTRY-LEVEL VECTOR MAP DATASET\r\n\r\n\r\nDrawing shapes in Deneb (Vega/Vega-Lite) involves mapping out coordinates for each point of the shape a.k.a. "feature".\r\n\r\nA basic example of this can be copy-and-pasted from this Stack Overflow answer by Chris Woods **[(@ChrisWoodsSays)](https://twitter.com/ChrisWoodsSays)**.\r\n\r\n* **[https://stackoverflow.com/questions/71254436/loading-feature-data-into-vega-lite](https://stackoverflow.com/questions/71254436/loading-feature-data-into-vega-lite)**\r\n\r\n![Deneb: Copy-and-pasting American state map data](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--07.png?raw=true)\r\n\r\nSo all we need is a dataset that has these coordinates that outline each country\'s shape.\r\n\r\n\r\n### Step 1. Retrieve a vector map data file\r\n\r\n*GeoJSON Maps of the globe* **[https://geojson-maps.ash.ms](https://geojson-maps.ash.ms)** by Ash Kyd **[(@ashkyd)](https://twitter.com/ashkyd)** is an excellent source for this.\r\n\r\nYou can choose how detailed the map is, and which regions to include.\r\n\r\nIn this example, we choose the lowest resolution (so it\'s less of a burden on the visual processing-wise), and all regions.\r\n\r\n![GeoJSON Maps of the globe: Part 1](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--08.png?raw=true)\r\n\r\nClick the *"Build Custom GeoJSON"* button to download your custom.geo.json file.\r\n\r\n![GeoJSON Maps of the globe: Part 2](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--09.png?raw=true)\r\n\r\n**Important!** This dataset is not comprehensive as it is missing a number of countries, e.g. Singapore. You can try seeking out other similar datasets as alternatives, as long as their structure is similar.\r\n\r\n\r\n### Step 2. Identify properties you do not need\r\n\r\nThis file has prolific properties you should review in case there are any you want to keep for your visual (highlighted in red), as well as the coordinates for the country shapes themselves (highlighted blue).\r\n\r\nIn this case, we will only be retaining the country names and coordinates, which would remove up to 2 thirds of the data.\r\n\r\nUsing Notepad++, you can use the JsonTools plugin to reformat the file\'s data into *"Pretty-print"*.\r\n\r\n![custom.geo.json modification: Part 1](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--10.png?raw=true)\r\n\r\nYour data should now look like this:\r\n\r\n![custom.geo.json modification: Part 2](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--11.png?raw=true)\r\n\r\nAn essential property to keep is:\r\n\r\n```\r\n"type": "Polygon"\r\n```\r\n\r\nBut there are other "type" properties including "Sovereignty", "Sovereign country", and "Country".\r\nSo we need to be careful we remove these, but not the "Polygon" one.\r\n\r\n![custom.geo.json modification: Part 3](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--12.png?raw=true)\r\n\r\nThe property we will be keeping is "geounit", as it has the country name short form.\r\n\r\nWhereas other properties, like "fips_10", "formal_en", "homepart", etc. will need to be removed.\r\n\r\n![custom.geo.json modification: Part 4](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--13.png?raw=true)\r\n\r\nUltimately, you want to create a string array of wildcard properties that will be used to assess each line of the custom.geo.json file, and remove that line where found.\r\n\r\n```\r\n"abbrev","adm0_","admin","brk_","continent","economy","fclass_","featurecla","filename","fips_","formal_","gdp_","geou_dif","gu_a3","homepart","income_grp","iso_","label_","labelrank", "level","long_len","mapcolor","max_", "min_","name","ne_id","note_","pop_","postal","region_","scalerank","sov_","sovereignt","su_","subregion","subunit","tiny","tlc","un_","wb_","wikidataid","woe_","ountry","Dependency","Sovereignty","Disputed","Indeterminate"\r\n```\r\n\r\nWe will use Python to do just that.\r\n\r\n\r\n### Step 3. Use Python to remove the unnecessary properties\r\n\r\nIn this example, we use Visual Studio Code and the Python extension to run the script needed to create a new file that only has the data we want.\r\n\r\nInstructions on how to create a Python environment are mentioned at the start of this previous post:\r\n\r\n* **[https://datamesse.github.io/#/post/1650117600](https://datamesse.github.io/#/post/1650117600)**\r\n\r\n\r\nThe Python script is as below:\r\n\r\n```\r\nimport os\r\ndef should_remove_line(line, stop_properties):\r\n    return any([word in line for word in stop_properties])\r\n\r\n# This lists the text strings that need to be detected for, and the lines they belong in removed\r\n\r\nstop_properties = ["abbrev","adm0_","admin","brk_","continent","economy","fclass_","featurecla","filename","fips_",\r\n                   "formal_","gdp_","geou_dif","gu_a3","homepart","income_grp","iso_","label_","labelrank", "level",\r\n                   "long_len","mapcolor","max_", "min_","name","ne_id","note_","pop_","postal","region_","scalerank",\r\n                   "sov_","sovereignt","su_","subregion","subunit","tiny","tlc","un_","wb_","wikidataid","woe_",\r\n                   "ountry","Dependency","Sovereignty","Disputed","Indeterminate"]\r\n\r\n# This code generates a temp file which removes nearly all of the unnecessary properties\r\n# But it leaves behind a single property "geounit", which has a comma at line end with no following property\r\n\r\nwith open(r"custom.geo.json") as f, open(r"stage1_output.json", "w") as stage1:    \r\n    for line in f:   \r\n        if not should_remove_line(line, stop_properties):  \r\n            stage1.write(line)\r\n\r\n# This code detects rows with the last remaining property, removes their commas, and writes to a second file\r\n\r\nstop_word = "geounit"\r\n\r\ndef remove_comma_after_name(line, stop_word):\r\n    return any([word in line for word in stop_word])\r\n\r\nwith open(r"stage1_output.json") as f, open(r"stage2_output.json", "w") as stage2:    \r\n    for line in f:\r\n        if stop_word in line:\r\n            line = line.replace(\',\',\'\',1)\r\n            stage2.write(line)\r\n        else:\r\n            stage2.write(line)\r\n\r\n```\r\n\r\nYour environment would have both the Python script and custom.geo.json, as per screenshot below, then you can just *Run Python File*.\r\n\r\n![Python: Part 1](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--14.png?raw=true)\r\n\r\nThe script creates an initial file named *stage1_output.json*, where the "geounit" property has a comma at the end. If this is the only or last property, this will cause issues.\r\n\r\n![Python: Part 2](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--15.png?raw=true)\r\n\r\nSo a second file is created named *stage2_output.json* to remove the comma. We will use the contents of this file for our Deneb visual.\r\n\r\n![Python: Part 3](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--16.png?raw=true)\r\n\r\n\r\n### Step 4. Extract country names from the dataset\r\n\r\nOne way to extract country names is to Ctrl + F for all instances of "geounit" in the file via Notepad++, then copy-and-paste the results into Excel, and use formulas to remove the excess data such as line number, "geounit", etc.\r\n\r\n![Extract country names](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--17.png?raw=true)\r\n\r\nYou can download this country list as an Excel file from here:\r\n\r\n* **[https://github.com/datamesse/data-visualisation-datasets/raw/main/AppSource%20Deneb%20maps/Countries.xlsx](https://github.com/datamesse/data-visualisation-datasets/raw/main/AppSource%20Deneb%20maps/Countries.xlsx)**\r\n\r\n\r\n### Step 5. Reformat map data file contents\r\n\r\nUsing Notepad++, you can use the JsonTools plugin to Compress the file.\r\n\r\n![Notepad++: Data compression Part 1](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--18.png?raw=true)\r\n\r\n![Notepad++: Data compression Part 2](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--19.png?raw=true)\r\n\r\nYou can additionally replace the "geounit" property name with "Country", to make it easily understood.\r\n\r\n![Notepad++: Data compression Part 3](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--20.png?raw=true)\r\n\r\nYou can download this trimmed down geographic JSON data from here:\r\n\r\n* **[https://github.com/datamesse/data-visualisation-datasets/blob/main/AppSource%20Deneb%20maps/compressed.geo.json](https://github.com/datamesse/data-visualisation-datasets/blob/main/AppSource%20Deneb%20maps/compressed.geo.json)**\r\n\r\n\r\n## CREATE POWER BI DATASET & MODEL\r\n\r\n\r\n### Step 1. Connect to the country dataset\r\n\r\nCreate a new Power BI report, then from Power Query, create a new Blank Query using the code below to connect to the list of countries extracted from the map dataset.\r\n\r\n**Power Query code**\r\n\r\n```\r\nlet\r\n    Source = Excel.Workbook(Web.Contents("https://github.com/datamesse/data-visualisation-datasets/blob/main/AppSource%20Deneb%20maps/Countries.xlsx?raw=true"), null, true),\r\n    tbl_Countries_Table = Source{[Item="tbl_Countries",Kind="Table"]}[Data],\r\n    #"Changed Type" = Table.TransformColumnTypes(tbl_Countries_Table,{{"Country", type text}})\r\nin\r\n    #"Changed Type"\r\n```\r\n\r\n![Data Source: Map Countries](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--21.png?raw=true)\r\n\r\n\r\n### Step 2. Connect to the city population dataset\r\n\r\nCreate another blank query for the sample city population dataset also using an Excel Workbook connection. This is a modified version of the **["World Cities Datasets" by Viswanathan C](https://www.kaggle.com/datasets/viswanathanc/world-cities-datasets)** hosted on my GitHub.\r\n\r\n**Power Query code**\r\n\r\n```\r\nlet\r\n    Source = Excel.Workbook(Web.Contents("https://github.com/datamesse/data-visualisation-datasets/blob/main/AppSource%20Deneb%20maps/Populations.xlsx?raw=true"), null, true),\r\n    tbl_CityPopulations_Table = Source{[Item="tbl_CityPopulations",Kind="Table"]}[Data],\r\n    #"Changed Type" = Table.TransformColumnTypes(tbl_CityPopulations_Table,{{"Country", type text}, {"City", type text}, {"Latitude", type number}, {"Longitude", type number}, \r\n    {"Capital?", type text}, {"Population", Int64.Type}})\r\nin\r\n    #"Changed Type"\r\n```\r\n\r\n\r\n### Step 3. Merge both datasets\r\n\r\nThe reason we import the country list from the map dataset is in case you want to use a dataset other than this city population one, and need to remap country names in your own dataset to match the country names from the map.\r\n\r\n![Data Source: City Populations](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--22.png?raw=true)\r\n\r\nIn this example, we will just merge the two datasets so that only records with the same country names are used. You can do this by creating a third blank query for this code:\r\n\r\n**Power Query code**\r\n\r\n```\r\nlet\r\n    Source = Table.NestedJoin(Countries, {"Country"}, Populations, {"Country"}, "Populations", JoinKind.Inner),\r\n    #"Expand dataset" = Table.ExpandTableColumn(Source, "Populations", {"City", "Latitude", "Longitude", "Capital?", "Population"}, {"City", "Latitude", "Longitude", "Capital?", "Population"})\r\nin\r\n    #"Expand dataset"\r\n```\r\n\r\n![Power Query: Merged dataset](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--23.png?raw=true)\r\n\r\nFrom Power BI\'s Column tools tab, ensure the Summarization and Data category values are properly set for the Longitude and Latitude fields\r\n\r\n![Power BI: Merged dataset](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--24.png?raw=true)\r\n\r\n\r\n### Step 4. Import the Deneb custom visual from AppSource\r\n\r\nDeneb can be downloaded from the Microsoft App Store:\r\n\r\n* **[https://appsource.microsoft.com/en-US/product/power-bi-visuals/coacervolimited1596856650797.deneb?tab=Overview](https://appsource.microsoft.com/en-US/product/power-bi-visuals/coacervolimited1596856650797.deneb?tab=Overview)**\r\n\r\nThe official website for Deneb:\r\n\r\n* **[https://deneb-viz.github.io/](https://deneb-viz.github.io/)**\r\n\r\n![Power BI: Import Deneb visual](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--25.png?raw=true)\r\n\r\nAdd the visual to your report.\r\n\r\n\r\n### Step 5. Add Country and Population to the Deneb visual\r\n\r\nCreate a measure that sums Population and add it to the Deneb visual, along with these fields:\r\n\r\n* Country\r\n* City\r\n* Latitude\r\n* Longitude\r\n\r\n```\r\nPopulation = SUM(\'Dataset\'[Population])\r\n```\r\n\r\n![Power BI: Add Deneb visual](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--26.png?raw=true)\r\n\r\nThat measure will be useful to the lowest field (e.g. Country or City, depending on which is added).\r\n\r\nA separate measure is needed to explicitly sum on Country for specifications that use both Country and City level.\r\n\r\n```\r\nPopulation by Country = CALCULATE(SUM(\'Dataset\'[Population]),ALLEXCEPT(\'Dataset\',\'Dataset\'[Country]))\r\n```\r\n\r\n\r\n## HOW TO ADD MAP DATA TO DENEB SPECIFICATION\r\n\r\nThe different downloadabe specifications below use this embedded map data approach.\r\n\r\nTo add the map data to each specification, copy from *"features"* at the start of the data, to *"type": "Feature"}]* at the end of the data, as per screenshot below.\r\n\r\n![Deneb: How to add map data to Vega-Lite](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--27.png?raw=true)\r\n\r\nThen paste it by replacing the *<< add map data here >>* placeholder in each of the specifications.\r\n\r\nIt is recommended to define your specification before pasting the map data, as editing the Specification can slow down a lot with that data in place.\r\n\r\nIf you must edit the specification with the map data in place, you can collapse it by clicking *hide*.\r\n\r\n![Deneb: How to add map data to Vega-Lite](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--28.png?raw=true)\r\n\r\n\r\n## MISSING DATA\r\n\r\nAs at time of writing, Deneb visuals are limited to displaying up to 30,000 rows of data.\r\n\r\nIf your map specification goes over that threshold, country geometries toward the end of your dataset may not appear onscreen.\r\n\r\nTo display them, try enabling the limiter override via the Visualizations pane > Format your visual section > Visual tab > expand Data Limit Options > Override Row Limit.\r\n\r\nFor example, Zimbabwe being at the end of the alphabet, does not appear when the limiter override is off, but appears when toggled on.\r\n\r\n![Deneb: Override Row Limit](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--29.png?raw=true)\r\n\r\n\r\n\r\nNext, let\'s create some standard map specifications based on Vega-Lite and Vega Example galleries by using the map data embedding we\'ve covered.\r\n\r\n\r\n\r\n## FILLED AREA MAP (FIXED)\r\n\r\nThis **Vega-Lite** map does not allow pan nor zoom.\r\n\r\nIt is based on the *"Choropleth of Unemployment Rate per County"* template, except this one is at country-level.\r\n\r\n* **[https://vega.github.io/vega-lite/examples/geo_choropleth.html](https://vega.github.io/vega-lite/examples/geo_choropleth.html)**\r\n\r\n**Preview**\r\n\r\n![Deneb: Filled area map fixed (Vega-Lite)](https://github.com/datamesse/data-visualisation-datasets/blob/main/AppSource%20Deneb%20maps/Power%20BI%20gif%20preview/Filled%20area%20map%20fixed.gif?raw=true)\r\n\r\n**Deneb specification**\r\n\r\n```\r\n{"background": "null",\r\n "view": {"fill": "#F0F8FF", \r\n          "fillOpacity": 1.0 },\r\n "width": 1000,\r\n "height": 500,\r\n "data": {\r\n    "name": "map",\r\n    "values": {\r\n      "type": "FeatureCollection",\r\n      << add map data here >>\r\n    },\r\n    "format": {"type": "json", "property": "features"}\r\n },\r\n "transform": [{\r\n    "lookup": "properties.Country",\r\n    "from": {\r\n      "data": {"name": "dataset"},\r\n        "key": "Country",\r\n        "fields": ["Population"]\r\n    }\r\n }],\r\n "projection": { "type": "mercator" },\r\n "mark": {"type": "geoshape", \r\n          "strokeWidth": "0.5",\r\n          "stroke": "#000000"},\r\n "encoding": {\r\n    "tooltip": [{"field": "properties.Country", \r\n                 "title": "Country"}, \r\n                {"field": "Population"}],\r\n    "color": {\r\n      "field": "Population",\r\n      "type": "quantitative",\r\n      "scale": {"range": ["#EBF7BB", "#1D368A"]}\r\n    }\r\n }\r\n}\r\n```\r\n\r\n\r\n## COORDINATE POINT MAP (FIXED)\r\n\r\nThis **Vega-Lite** map does not allow pan nor zoom.\r\n\r\nIt is based on the *"One Dot per Airport in the U.S. Overlayed on Geoshape"* template.\r\n\r\n* **[https://vega.github.io/vega-lite/examples/geo_layer.html](https://vega.github.io/vega-lite/examples/geo_layer.html)**\r\n\r\n**Preview**\r\n\r\n![Deneb: Coordinate point map fixed (Vega-Lite)](https://github.com/datamesse/data-visualisation-datasets/blob/main/AppSource%20Deneb%20maps/Power%20BI%20gif%20preview/Coordinate%20point%20map%20fixed.gif?raw=true)\r\n\r\n**Deneb specification**\r\n\r\n```\r\n{"background": "null",\r\n "view": {"fill": "#F0F8FF", \r\n          "fillOpacity": 1.0 },\r\n "width": 1000,\r\n "height": 500,\r\n "layer": [\r\n    { "data": {\r\n        "values": {\r\n          "type": "FeatureCollection",\r\n          << add map data here >>\r\n        },\r\n        "format": {"type": "json", "property": "features"}\r\n      },\r\n      "mark": {"type": "geoshape", \r\n               "strokeWidth": "0.5",\r\n               "stroke": "#000000",\r\n               "fill": "#E8F5B9" },\r\n      "encoding": {\r\n        "tooltip": {"field": "properties.Country", "title": "Country"},\r\n        "shape": {"type": "geojson"}\r\n      }\r\n    },\r\n    { "data": { "name": "dataset" },\r\n      "projection": { "type": "mercator"},\r\n      "mark": "circle",\r\n      "encoding": {\r\n        "tooltip": [{"field": "Country"},\r\n                    {"field": "City"},\r\n                    {"field": "Latitude"},\r\n                    {"field": "Longitude"},\r\n                    {"field": "Population"}],\r\n        "longitude": {\r\n          "field": "Longitude",\r\n          "type": "quantitative"\r\n        },\r\n        "latitude": {\r\n          "field": "Latitude",\r\n          "type": "quantitative"\r\n        },\r\n        "size": {"value": 5},\r\n        "color": {"value": "red"}\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n\r\n## FILLED AREA WITH COORDINATE POINT MAP (FIXED)\r\n\r\nThis **Vega-Lite** map does not allow pan nor zoom.\r\n\r\nIt is based on a combination of both *"Choropleth of Unemployment Rate per County"* and *"One Dot per Airport in the U.S. Overlayed on Geoshape"* templates.\r\n\r\n* **[https://vega.github.io/vega-lite/examples/geo_choropleth.html](https://vega.github.io/vega-lite/examples/geo_choropleth.html)**\r\n\r\n* **[https://vega.github.io/vega-lite/examples/geo_layer.html](https://vega.github.io/vega-lite/examples/geo_layer.html)**\r\n\r\n**Preview**\r\n\r\n![Deneb: Filled area with coordinate point map fixed (Vega-Lite)](https://github.com/datamesse/data-visualisation-datasets/blob/main/AppSource%20Deneb%20maps/Power%20BI%20gif%20preview/Filled%20area%20with%20coordinate%20point%20map%20fixed.gif?raw=true)\r\n\r\n**Deneb specification**\r\n\r\n```\r\n{\r\n  "background": "null",\r\n  "view": {\r\n    "fill": "#F0F8FF",\r\n    "fillOpacity": 1\r\n  },\r\n  "width": 1000,\r\n  "height": 500,\r\n  "layer": [\r\n    {\r\n      "data": {\r\n        "values": {\r\n          "type": "FeatureCollection",\r\n          << add map data here >>\r\n        },\r\n        "format": {\r\n          "type": "json",\r\n          "property": "features"\r\n        }\r\n      },\r\n      "transform": [\r\n        {\r\n          "lookup": "properties.Country",\r\n          "from": {\r\n            "data": {"name": "dataset"},\r\n            "key": "Country",\r\n            "fields": [\r\n              "Population",\r\n              "Population by Country"\r\n            ]\r\n          }\r\n        }\r\n      ],\r\n      "projection": {\r\n        "type": "mercator"\r\n      },\r\n      "mark": {\r\n        "type": "geoshape",\r\n        "strokeWidth": "0.5",\r\n        "stroke": "#000000"\r\n      },\r\n      "encoding": {\r\n        "tooltip": [\r\n          {\r\n            "field": "properties.Country",\r\n            "title": "Country"\r\n          },\r\n          {\r\n            "field": "Population by Country",\r\n            "title": "Population"\r\n          }\r\n        ],\r\n        "color": {\r\n          "field": "Population by Country",\r\n          "type": "quantitative",\r\n          "scale": {\r\n            "range": [\r\n              "#EBF7BB",\r\n              "#1D368A"\r\n            ]\r\n          }\r\n        }\r\n      }\r\n    },\r\n    {\r\n      "data": {"name": "dataset"},\r\n      "projection": {\r\n        "type": "mercator"\r\n      },\r\n      "mark": "circle",\r\n      "encoding": {\r\n        "tooltip": [\r\n          {"field": "Country"},\r\n          {"field": "City"},\r\n          {"field": "Latitude"},\r\n          {"field": "Longitude"},\r\n          {"field": "Population"}\r\n        ],\r\n        "longitude": {\r\n          "field": "Longitude",\r\n          "type": "quantitative"\r\n        },\r\n        "latitude": {\r\n          "field": "Latitude",\r\n          "type": "quantitative"\r\n        },\r\n        "size": {"value": 5},\r\n        "color": {"value": "red"}\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n\r\n## MAP PROJECTION EXPLORER (FIXED)\r\n\r\nThis **Vega-Lite** map does not allow pan nor zoom.\r\n\r\nThis allows exploring various map projection types in Vega-Lite.\r\n\r\nIt is based on the *"Projection explorer"* template.\r\n\r\n* **[https://vega.github.io/vega-lite/examples/geo_params_projections.html](https://vega.github.io/vega-lite/examples/geo_params_projections.html)**\r\n\r\n**Preview**\r\n\r\n![Deneb: Map projection explorer fixed (Vega-Lite)](https://github.com/datamesse/data-visualisation-datasets/blob/main/AppSource%20Deneb%20maps/Power%20BI%20gif%20preview/Map%20projection%20explorer%20fixed.gif?raw=true)\r\n\r\n**Deneb specification**\r\n\r\n```\r\n{"background": "null",\r\n "view": {"fill": "#F0F8FF", \r\n          "fillOpacity": 1.0 },\r\n "width": 1000,\r\n "height": 500,\r\n "params": [\r\n    { "name": "projection",\r\n      "value": "albers",\r\n      "bind": {\r\n        "input": "select",\r\n        "options": [\r\n          "albers",\r\n          "albersUsa",\r\n          "azimuthalEqualArea",\r\n          "azimuthalEquidistant",\r\n          "conicConformal",\r\n          "conicEqualArea",\r\n          "conicEquidistant",\r\n          "equalEarth",\r\n          "equirectangular",\r\n          "gnomonic",\r\n          "mercator",\r\n          "naturalEarth1",\r\n          "orthographic",\r\n          "stereographic",\r\n          "transverseMercator"\r\n        ]\r\n      }\r\n    }\r\n ],\r\n "layer": [\r\n    { "data": {\r\n        "values": {\r\n          "type": "FeatureCollection",\r\n          << add map data here >>\r\n        },\r\n        "format": {"type": "json", "property": "features"}\r\n      },\r\n      "transform": [{\r\n        "lookup": "properties.Country",\r\n          "from": {\r\n            "data": {"name": "dataset"},\r\n                     "key": "Country",\r\n                     "fields": ["Population", "Population by Country"]\r\n          }\r\n      }],\r\n      "projection": {"type": {"expr": "projection"}},\r\n      "mark": {"type": "geoshape",\r\n               "strokeWidth": "0.5",\r\n               "stroke": "#000000"\r\n      },\r\n      "encoding": {\r\n        "tooltip": [{"field": "properties.Country", \r\n                     "title": "Country"}, \r\n                    {"field": "Population by Country"}],\r\n        "color": {\r\n          "field": "Population by Country",\r\n          "type": "quantitative",\r\n          "scale": {"range": ["#EBF7BB", "#1D368A"]}\r\n        }\r\n      }\r\n    },\r\n    { "data": { "name": "dataset" },\r\n      "projection": {"type": {"expr": "projection"}},\r\n      "mark": "circle",\r\n      "encoding": {\r\n        "tooltip": [{"field": "Country"},\r\n                    {"field": "City"},\r\n                    {"field": "Latitude"},\r\n                    {"field": "Longitude"},\r\n                    {"field": "Population"}],\r\n        "longitude": {\r\n          "field": "Longitude",\r\n          "type": "quantitative"\r\n        },\r\n        "latitude": {\r\n          "field": "Latitude",\r\n          "type": "quantitative"\r\n        },\r\n        "size": {"value": 5},\r\n        "color": {"value": "red"}\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n\r\n## SLIDER INTERACTIVE GLOBE (FIXED)\r\n\r\nThis **Vega-Lite** globe map does not allow pan nor zoom, but can rotate using sliders.\r\n\r\nIt is based on the *"Earthquakes Example"* template.\r\n\r\n* **[https://vega.github.io/vega-lite/examples/interactive_geo_earthquakes.html](https://vega.github.io/vega-lite/examples/interactive_geo_earthquakes.html)**\r\n\r\n**Preview**\r\n\r\n![Deneb: Slider interactive globe fixed (Vega-Lite)](https://github.com/datamesse/data-visualisation-datasets/blob/main/AppSource%20Deneb%20maps/Power%20BI%20gif%20preview/Slider%20interactive%20globe%20fixed.gif?raw=true)\r\n\r\n**Deneb specification**\r\n\r\n```\r\n{\r\n  "width": 500,\r\n  "height": 500,\r\n  "projection": {\r\n    "type": "orthographic",\r\n    "rotate": {"expr": "[rotate0, rotate1, 0]"}\r\n  },\r\n  "params": [\r\n    {\r\n      "name": "rotate0",\r\n      "value": 0,\r\n      "bind": {"input": "range", "min": -90, "max": 90, "step": 1}\r\n    },\r\n    {\r\n      "name": "rotate1",\r\n      "value": 0,\r\n      "bind": {"input": "range", "min": -90, "max": 90, "step": 1}\r\n    },\r\n    {\r\n      "name": "populationSize",\r\n      "value": 2.0,\r\n      "bind": {"input": "range", "min": 0.1, "max": 4, "step": 0.1}\r\n    }\r\n  ],\r\n  "layer": [\r\n    {\r\n      "data": {"sphere": true},\r\n      "mark": {"type": "geoshape", "fill": "#F0F8FF"}\r\n    },\r\n    {\r\n      "data": { "graticule": true },\r\n      "mark": {\r\n        "type": "geoshape",\r\n        "stroke": "#B0D8FF",\r\n        "strokeWidth": 0.25\r\n      }\r\n    },\r\n    {\r\n      "data": {\r\n        "values": {\r\n          "type": "FeatureCollection",\r\n          << add map data here >>\r\n        },\r\n        "format": {"type": "json", "property": "features"}\r\n      },\r\n      "transform": [{\r\n        "lookup": "properties.Country",\r\n          "from": {\r\n            "data": {"name": "dataset"},\r\n            "key": "Country",\r\n            "fields": ["Population", "Population by Country"]\r\n          }\r\n      }],\r\n      "mark": {"type": "geoshape",\r\n               "strokeWidth": "0.5",\r\n               "stroke": "#000000"\r\n      },\r\n      "encoding": {\r\n        "shape": {"type": "geojson"},\r\n        "tooltip": [{"field": "properties.Country", \r\n                     "title": "Country"}, \r\n                    {"field": "Population by Country",\r\n                     "title": "Population"\r\n                    }],\r\n        "color": {\r\n          "field": "Population by Country",\r\n          "type": "quantitative",\r\n          "scale": {"range": ["#EBF7BB", "#1D368A"]}\r\n        }\r\n      }\r\n    },\r\n    {\r\n      "data": { "name": "dataset" },\r\n      "transform": [\r\n        {"calculate": "datum.Longitude", "as": "longitude"},\r\n        {"calculate": "datum.Latitude", "as": "latitude"},\r\n        {"filter": "(rotate0 * -1) - 90 < datum.longitude && datum.longitude < (rotate0 * -1) + 90 && (rotate1 * -1) - 90 < datum.latitude && datum.latitude < (rotate1 * -1) + 90"},\r\n        {"calculate": "abs(datum.Longitude)", "as": "magnitude"}\r\n      ],\r\n      "mark": {"type": "circle", "color": "red", "opacity": 1.0},\r\n      "encoding": {\r\n        "longitude": {"field": "longitude", "type": "quantitative"},\r\n        "latitude": {"field": "latitude", "type": "quantitative"},\r\n        "size": {\r\n          "legend": null,\r\n          "field": "magnitude",\r\n          "type": "quantitative",\r\n          "scale": {\r\n            "type": "sqrt",\r\n            "domain": [0, 100],\r\n            "range": [0, {"expr": "pow(populationSize, 3)"}]\r\n          }\r\n        },\r\n        "tooltip": [{"field": "Country", "name": "Country"},\r\n                    {"field": "City"},\r\n                    {"field": "Latitude"},\r\n                    {"field": "Longitude"},\r\n                    {"field": "Population"}]\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n\r\n## INTERACTIVE MAP (PAN & ZOOM)\r\n\r\nThis **Vega** map allows pan and zoom.\r\n\r\nIt is based on the *"Zoomable World Map Example"* template, but mine takes it a couple of steps further by actually connecting the geographic shapes to your country data, and has an extra layer for latitude and longitude coordinates.\r\n\r\n* **[https://vega.github.io/vega/examples/zoomable-world-map/](https://vega.github.io/vega/examples/zoomable-world-map/)**\r\n\r\n**Preview**\r\n\r\n![Deneb: Interactive map pan & zoom (Vega-Lite)](https://github.com/datamesse/data-visualisation-datasets/blob/main/AppSource%20Deneb%20maps/Power%20BI%20gif%20preview/Interactive%20map%20pan%20and%20zoom.gif?raw=true)\r\n\r\n**Deneb specification**\r\n\r\n```\r\n{\r\n  "width": 1100,\r\n  "height": 700,\r\n  "autosize": "true",\r\n  "background": "#EEFFFF",\r\n  "signals": [\r\n    { "name": "tx", "update": "width / 2" },\r\n    { "name": "ty", "update": "height / 2" },\r\n    { "name": "scale",\r\n      "value": 150,\r\n      "on": [{\r\n        "events": {"type": "wheel", "consume": true},\r\n        "update": "clamp(scale * pow(1.0005, -event.deltaY * pow(16, event.deltaMode)), 150, 3000)"\r\n      }]\r\n    },\r\n    { "name": "angles",\r\n      "value": [0, 0],\r\n      "on": [{\r\n        "events": "mousedown",\r\n        "update": "[rotateX, centerY]"\r\n      }]\r\n    },\r\n    { "name": "cloned",\r\n      "value": null,\r\n      "on": [{\r\n        "events": "mousedown",\r\n        "update": "copy(\'projection\')"\r\n      }]\r\n    },\r\n    { "name": "start",\r\n      "value": null,\r\n      "on": [{\r\n        "events": "mousedown",\r\n        "update": "invert(cloned, xy())"\r\n      }]\r\n    },\r\n    { "name": "drag",\r\n      "value": null,\r\n      "on": [{\r\n        "events": "[mousedown, window:mouseup] > window:mousemove",\r\n        "update": "invert(cloned, xy())"\r\n      }]\r\n    },\r\n    { "name": "delta", "value": null,\r\n      "on": [{\r\n        "events": {"signal": "drag"},\r\n        "update": "[drag[0] - start[0], start[1] - drag[1]]"\r\n      }]\r\n    },\r\n    { "name": "rotateX", "value": 0,\r\n      "on": [{\r\n        "events": {"signal": "delta"},\r\n        "update": "angles[0] + delta[0]"\r\n      }]\r\n    },\r\n    { "name": "centerY", "value": 0,\r\n      "on": [{\r\n        "events": {"signal": "delta"},\r\n        "update": "clamp(angles[1] + delta[1], -60, 60)"\r\n      }]\r\n    }\r\n  ],\r\n\r\n  "projections": [\r\n    {\r\n      "name": "projection",\r\n      "type": "mercator",\r\n      "scale": {"signal": "scale"},\r\n      "rotate": [{"signal": "rotateX"}, 0, 0],\r\n      "center": [0, {"signal": "centerY"}],\r\n      "translate": [{"signal": "tx"}, {"signal": "ty"}]\r\n    }\r\n  ],\r\n\r\n  "data": [\r\n    { "name": "world",\r\n      "values": {\r\n        "type": "FeatureCollection",\r\n        << add map data here >>\r\n      },\r\n      "format": {"type": "json", "property": "features"}\r\n    },\r\n    { "name": "dataset", "format": {} },\r\n    { "name": "graticule",\r\n      "transform": [ { "type": "graticule", "step": [15, 15] } ]\r\n    },\r\n    { "name": "world_map",\r\n      "source": "world",\r\n      "transform": [\r\n        { "type": "formula",\r\n          "expr": "datum[\\"properties\\"] && datum[\\"properties\\"][\\"Country\\"]",\r\n          "as": "properties.Country"\r\n        },\r\n        { "type": "lookup",\r\n          "from": "dataset",\r\n          "key": "Country",\r\n          "fields": ["properties.Country"],\r\n          "values": ["Population", "Population by Country"]\r\n        },\r\n        { "type": "filter",\r\n          "expr": "isValid(datum[\\"Population\\"]) && isFinite(+datum[\\"Population\\"])"\r\n        }\r\n      ]\r\n    },\r\n    { "name": "powerbi_dataset",\r\n      "source": "dataset",\r\n      "transform": [\r\n        { "type": "geojson",\r\n          "fields": ["Longitude", "Latitude"],\r\n          "signal": "layer_1_geojson_0"\r\n        },\r\n        { "type": "geopoint",\r\n          "projection": "projection",\r\n          "fields": ["Longitude", "Latitude"],\r\n          "as": ["layer_1_x", "layer_1_y"]\r\n        }\r\n      ]\r\n    }\r\n  ],\r\n\r\n  "marks": [\r\n    { "type": "shape",\r\n      "from": {"data": "graticule"},\r\n      "encode": {\r\n        "enter": {\r\n          "strokeWidth": {"value": 0.25},\r\n          "stroke": {"value": "#88D5FF"},\r\n          "fill": {"value": "#88D5FF"}\r\n        }\r\n      },\r\n      "transform": [ { "type": "geoshape", "projection": "projection" } ]\r\n    },\r\n    { "type": "shape",\r\n      "style": ["geoshape"],\r\n      "from": {"data": "world_map"},\r\n      "encode": {\r\n        "enter": {\r\n          "strokeWidth": {"value": 0.5},\r\n          "stroke": {"value": "#000000"}\r\n        },\r\n        "update": {\r\n          "fill": {"scale": "color", "field": "Population by Country"},\r\n            "tooltip": {\r\n              "signal": "{\\"Country\\": isValid(datum[\\"properties.Country\\"]) ? datum[\\"properties.Country\\"] : \\"\\"+datum[\\"properties.Country\\"], \\"Population\\": isValid(datum[\\"Population by Country\\"]) ? datum[\\"Population by Country\\"] : \\"\\"+datum[\\"Population by Country\\"]}"\r\n            },\r\n            "ariaRoleDescription": {"value": "geoshape"},\r\n            "description": {\r\n              "signal": "\\"Population: \\" + (format(datum[\\"Population\\"], \\"\\")) + \\"; Country: \\" + (isValid(datum[\\"properties.Country\\"]) ? datum[\\"properties.Country\\"] : \\"\\"+datum[\\"properties.Country\\"])"\r\n            }\r\n        }\r\n      },\r\n      "transform": [ { "type": "geoshape", "projection": "projection" } ]\r\n    },\r\n    { "type": "symbol",\r\n      "style": ["circle"],\r\n      "from": {"data": "powerbi_dataset"},\r\n      "encode": {\r\n        "update": {\r\n          "opacity": {"value": 1.0},\r\n          "fill": {"value": "red"},\r\n          "tooltip": {\r\n            "signal": "{\\"Country\\": isValid(datum[\\"Country\\"]) ? datum[\\"Country\\"] : \\"\\"+datum[\\"Country\\"], \\"City\\": isValid(datum[\\"City\\"]) ? datum[\\"City\\"] : \\"\\"+datum[\\"City\\"], \\"Population\\": isValid(datum[\\"Population\\"]) ? datum[\\"Population\\"] : \\"\\"+datum[\\"Population\\"]}"\r\n          },\r\n          "ariaRoleDescription": {"value": "circle"},\r\n          "description": {\r\n            "signal": "\\"Longitude: \\" + (format(datum[\\"Longitude\\"], \\"\\")) + \\"; Latitude: \\" + (format(datum[\\"Latitude\\"], \\"\\")) + \\"; Country: \\" + (isValid(datum[\\"Country\\"]) ? datum[\\"Country\\"] : \\"\\"+datum[\\"Country\\"]) + \\"; City: \\" + (isValid(datum[\\"City\\"]) ? datum[\\"City\\"] : \\"\\"+datum[\\"City\\"])"\r\n          },\r\n          "x": {"field": "layer_1_x"},\r\n          "y": {"field": "layer_1_y"},\r\n          "size": {"value": 10},\r\n          "shape": {"value": "circle"}\r\n        }\r\n      }\r\n    }\r\n  ],\r\n  "scales": [\r\n    { "name": "color",\r\n      "type": "linear",\r\n      "domain": {"data": "world_map", "field": "Population by Country"},\r\n      "range":  ["#EBF7BB", "#1D368A"],\r\n      "interpolate": "hcl",\r\n      "zero": false\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nThis is by no means an efficient way to create geographic maps with AppSource Deneb, but it does prove that it can be done, if for whatever reason you are not allowed to use the standalone Deneb version.\r\n\r\nIf you want to design maps in a particular style, like the futuristic outline ones you see in movies and video games, then it is doable with this approach.\r\n\r\n![Example futuristic map style](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-11-26--30.png?raw=true)\r\n\r\nExample futuristic map from **[Reddit](https://www.reddit.com/r/imaginarymaps/comments/wado40/the_world_of_the_new_order_last_days_of_europe_in/)**.\r\n\r\nSince geographic maps just use coordinate points, it also means it is feasible to draw all kinds of custom shapes.\r\n\r\nThe sky\'s the limit in what you can do with Deneb.\r\n\r\n\r\n### References\r\n\r\n* Microsoft Power BI Community Forum Question **[Deneb Vega Lite Map not displaying](https://community.powerbi.com/t5/Desktop/Deneb-Vega-Lite-Map-not-displaying/m-p/2487853)**\r\n\r\n* Stack Overflow Question **[Loading feature data into Vega-Lite](https://stackoverflow.com/questions/71254436/loading-feature-data-into-vega-lite)**\r\n\r\n* Website **[GeoJSON Maps of the globe](https://geojson-maps.ash.ms)**\r\n\r\n* Website **[Vega-Lite Example Gallery](https://vega.github.io/vega-lite/examples/)**\r\n\r\n* Website **[Vega Example Gallery](https://vega.github.io/vega/examples/)**\r\n\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2022-11-26.md)** for this post\'s markdown file in GitHub.'},{id:1666962e3,title:"Comparing manual forecasting with Excel's built-in FORECAST.ETS function",tag:"logo-excel",date:"29 October 2022",content:'\r\nThis post discusses my learnings from comparing manual forecasting in Excel using a ratio-to-moving averages method to estimate seasonal indices, versus using Excel\'s out-of-the-box FORECAST.ETS() function.\r\n\r\nThis began from my wanting to create an Excel variation of my Power BI International Marketplace profit report.\r\n\r\n![Excel profit forecast dashboard](https://github.com/datamesse/datamesse.github.io/blob/main/src/assets-portfolio/img-2022-10-excel-international-marketplace-profit-forecast.gif?raw=true)\r\n\r\n* **[https://datamesse.github.io/#/project/ExcelInternationalMarketplaceProfitForecast](https://datamesse.github.io/#/project/ExcelInternationalMarketplaceProfitForecast)**\r\n\r\nOne key component of that report, is that it integrated Python into its data model via Power Query (typically not best practice) to forecast seasonal profits using exponential smoothing via Python. See my **[blog post](https://datamesse.github.io/#/post/1650117600)** for more information on that.\r\n\r\nFor my Excel variation, I wanted to keep the forecasting within Excel without relying on an additional tool, and began learning how to do it manually via Wayne Winston\'s 2014 course on LinkedIn Learning; *"Excel Data Analysis: Forecasting"* **[https://www.linkedin.com/learning/excel-data-analysis-forecasting](https://www.linkedin.com/learning/excel-data-analysis-forecasting)**.\r\n\r\nI later found Microsoft released the FORECAST.ETS() function only a couple of years later from Excel 2016:\r\n\r\n* **[https://support.microsoft.com/en-us/office/forecast-ets-function-15389b8b-677e-4fbd-bd95-21d464333f41](https://support.microsoft.com/en-us/office/forecast-ets-function-15389b8b-677e-4fbd-bd95-21d464333f41)**\r\n\r\nSo I tried applying both methods against my International Marketplace dataset:\r\n\r\n* **[https://github.com/datamesse/data-visualisation-datasets/blob/main/International%20Marketplace%20sales/normalised%20flat%20files/](https://github.com/datamesse/data-visualisation-datasets/blob/main/International%20Marketplace%20sales/normalised%20flat%20files/)**\r\n\r\n\r\n### Download Excel file\r\n\r\nYou can download the completed Excel workbook comparing the two methods below:\r\n\r\n* **[https://github.com/datamesse/data-visualisation-datasets/raw/main/International%20Marketplace%20sales/International%20Marketplace%20FORECAST%20ETS%20vs%20ratio-to-moving%20point%20average.xlsx](https://github.com/datamesse/data-visualisation-datasets/raw/main/International%20Marketplace%20sales/International%20Marketplace%20FORECAST%20ETS%20vs%20ratio-to-moving%20point%20average.xlsx)**\r\n\r\n\r\n## Data modelling\r\n\r\nBefore getting into the forecasting, here\'s a bit of an explanation on how you can forecast values for dates with order/sales records that don\'t exist yet.\r\n\r\n\r\n### Step 1. Create a Year-Month table\r\n\r\nTo model the dates to account for both:\r\n\r\na) new records added to the dataset and\r\nb) keeping a consistent date buffer to forecast on, based on the latest record\r\n\r\nuse Power Query to create multiple references to the data source to find the earliest and latest Order Dates.\r\n\r\n![Power Query: Earliest and latest OrderDates](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--01.png?raw=true)\r\n\r\n**OrderDate (earliest)**\r\n\r\n```\r\nlet\r\n    Source = FactSales,\r\n    #"remove other columns" = Table.SelectColumns(#"Source",{"OrderDate"}),\r\n    #"earliest date" = List.Min(#"remove other columns"[OrderDate])\r\nin\r\n    #"earliest date"\r\n```\r\n\r\n**OrderDate (latest)**\r\n\r\n```\r\nlet\r\n    Source = FactSales,\r\n    #"remove other columns" = Table.SelectColumns(#"Source",{"OrderDate"}),\r\n    #"removed duplicates" = Table.Distinct(#"remove other columns"),\r\n    #"latest date" = List.Max(#"removed duplicates"[OrderDate])\r\nin\r\n    #"latest date"\r\n```\r\n\r\n**Note:** It is very important to ensure that the data source is referenced separately. Otherwise, using Power Query\'s Reference on the existing sales query would lead to circular dependencies that prevent processing in the next step.\r\n\r\nThen using those dates, create a Year-Month calendar list. In this example, the query below takes the latest order date (31/12/2022) and it finds the 31st December for the following year and defines it as the last date for your calendar. Then change it to only include Year and Month numbers, and add an index column.\r\n\r\n![Power Query: Year-Month Index](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--02.png?raw=true)\r\n\r\n**Year-Month Index**\r\n\r\n```\r\nlet\r\n    Source = List.Dates(#"OrderDate (earliest)", Duration.Days(Date.From(#datetime(Date.Year(#"OrderDate (latest)")+1, 12, 31, 0, 0, 0))-#"OrderDate (earliest)")+1, #duration(1, 0, 0, 0)),\r\n    #"convert to table" = Table.FromList(Source, Splitter.SplitByNothing(), null, null, ExtraValues.Error),\r\n    #"change type" = Table.TransformColumnTypes(#"convert to table",{{"Column1", type date}}),\r\n    #"rename to Date column" = Table.RenameColumns(#"change type",{{"Column1", "Date"}}),\r\n    #"add Year column" = Table.AddColumn(#"rename to Date column", "Year", each Date.Year([Date]), Int64.Type),\r\n    #"add Month column" = Table.AddColumn(#"add Year column", "Month", each Date.Month([Date]), Int64.Type),\r\n    #"remove other columns" = Table.SelectColumns(#"add Month column",{"Year", "Month"}),\r\n    #"remove duplicates" = Table.Distinct(#"remove other columns"),\r\n    #"add Index column" = Table.AddIndexColumn(#"remove duplicates", "Index", 1, 1, Int64.Type)\r\nin\r\n    #"add Index column"\r\n```\r\n\r\n\r\n### Step 2. Create a Date table\r\n\r\nFollowing similar principles in the previous step, create a main Date table, and merge the Year-Month table to it, so that the latter\'s index is added. In this example, we have named our date table as "Calendar".\r\n\r\n![Power Query: Merge Calendar table with Year-Month Index table](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--03.png?raw=true)\r\n\r\n**Calendar**\r\n\r\n```\r\nlet\r\n    Source = List.Dates(#"OrderDate (earliest)", Duration.Days(Date.From(#datetime(Date.Year(#"OrderDate (latest)")+1, 12, 31, 0, 0, 0))-#"OrderDate (earliest)")+1, #duration(1, 0, 0, 0)),\r\n    #"conver to table" = Table.FromList(Source, Splitter.SplitByNothing(), null, null, ExtraValues.Error),\r\n    #"change type" = Table.TransformColumnTypes(#"conver to table",{{"Column1", type date}}),\r\n    #"rename to Date column" = Table.RenameColumns(#"change type",{{"Column1", "Date"}}),\r\n    #"add Year column" = Table.AddColumn(#"rename to Date column", "Year", each Date.Year([Date]), Int64.Type),\r\n    #"add Quarter column" = Table.AddColumn(#"add Year column", "Quarter", each "Q" & Text.From(Date.QuarterOfYear([Date])), type text),\r\n    #"add Month column" = Table.AddColumn(#"add Quarter column", "Month", each Date.Month([Date]), Int64.Type),\r\n    #"add Month name column" = Table.AddColumn(#"add Month column", "Month name", each Text.Upper(Text.Start(Date.MonthName([Date]),3)), type text),\r\n    #"add Day column" = Table.AddColumn(#"add Month name column", "Day", each Date.Day([Date]), Int64.Type),\r\n    #"add Day name column" = Table.AddColumn(#"add Day column", "Day name", each Text.Upper(Text.Start(Date.DayOfWeekName([Date]),3)), type text),\r\n    #"merge with Year-Month Index" = Table.NestedJoin(#"add Day name column", {"Year", "Month"}, #"Year-Month Index", {"Year", "Month"}, "Year-Month Index", JoinKind.Inner),\r\n    #"expand Year-Month Index" = Table.ExpandTableColumn(#"merge with Year-Month Index", "Year-Month Index", {"Index"}, {"Index"})\r\nin\r\n    #"expand Year-Month Index"\r\n```\r\n\r\nRemember to Mark as Date Table via Power Pivot.\r\n\r\nThen add the relationship between the sales table and the date table. \r\n\r\n![Power Query: Relationship between Calendar table and FactSales data](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--04.png?raw=true)\r\n\r\n\r\n### Step 3. Create DAX measure and a pivot table\r\n\r\nIn the example above, we have an additional relationship from the sales table\'s ShipDate field to the calendar. This means needing to define USERELATIONSHIP() when creating our DAX query to sum profit:\r\n\r\n```\r\n=VAR currentyear = YEAR(CALCULATE(MAX(FactSales[OrderDate]), ALL(FactSales)))\r\nRETURN CALCULATE(SUM(FactSales[Profit]), YEAR(FactSales[OrderDate]) = currentyear, USERELATIONSHIP(\'Calendar\'[Date], FactSales[OrderDate]))\r\n```\r\n\r\n![Power Query: DAX measure](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--05.png?raw=true)\r\n\r\n\r\nNext, add a pivot table from the Data Model, using the Index from the Calendar date table, year and month fields, and the DAX measure.\r\n\r\nBy default, Excel will only show the records that exist for the DAX measure.\r\n\r\n![Pivot table: Add calendar fields, index, and DAX measure](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--06.png?raw=true)\r\n\r\nTo show the rows for unforecasted months, configure the *PivotTable Options* to *Show items with no data on rows*\r\n\r\n![Pivot table: Enable Show items with no data on rows](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--07.png?raw=true)\r\n\r\nNow the rows without data which will be forecasted for, display in the pivot. If a new year\'s data is added, then the additional months will update in accordance with that.\r\n\r\n\r\n**Note:** As at the time of writing, FORECAST.ETS() cannot be called from within DAX, so ultimately, you would need to reference the pivot table\'s cells, and perform the calculations from normal cells.\r\n\r\n![Pivot table: Pivot table with extra year-months for predictions](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--08.png?raw=true)\r\n\r\n\r\nFinally, onto the forecasting...\r\n\r\n\r\n## Manual: Ratio-to-moving averages method\r\n\r\nBeginning with the manual ratio-to-moving averages method, these are the general steps:\r\n\r\n\r\n### Step 1. 3-period moving average\r\n\r\nCreate a column for the moving average. You can choose how many periods cover the average and redo your analysis to try diffserent ones. In the case of the LinkedIn Learning course, 4 periods were used, but I\'ve used 3 here to reflect the number of months per quarter.\r\n\r\nBecause of the nature of the averaging and need to maintain a consistent pattern, the range for the result periods will be less than the range of the period data you have.\r\n\r\n![Ratio-to-moving average: 3-period moving average](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--09.png?raw=true)\r\n\r\n\r\n### Step 2. Centred moving average (CMA)\r\n\r\nCreate a column for the centred moving average, which averages the previous and current period\'s 3-period moving average.\r\n\r\nIn this case, there\'ll just be one less period in the results from the start versus the input.\r\n\r\n![Ratio-to-moving average: Centred moving average](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--10.png?raw=true)\r\n\r\n\r\n### Step 3. Actual \xf7 CMA\r\n\r\nCreate a column to divide the actual profit value by the centred moving average.\r\n\r\n![Ratio-to-moving average: Divide actual by CMA](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--11.png?raw=true)\r\n\r\n\r\n### Step 4. Seasonal average based on month number\r\n\r\nCreate a separate reference table that averages the Actual \xf7 CMA results based on their month number as a seasonal index.\r\n\r\n![Ratio-to-moving average: Seasonal index](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--12.png?raw=true)\r\n\r\nThen average the seasonal index values.\r\n\r\n![Ratio-to-moving average: Average the seasonal indexes](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--13.png?raw=true)\r\n\r\n\r\n### Step 5. Normalised seasonal index\r\n\r\nAdd a new column to that reference table to normalise seasonal index values by dividing each with the overall average of them. This normalised index value is what will be used to forecast the later months.\r\n\r\n![Ratio-to-moving average: Normalise each seasonal indexes](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--14.png?raw=true)\r\n\r\nHowever, incorporating those exact cells into the forecast will cause a circular reference, and thus not work. So the results need to be copy-and-pasted as plain values into new cells, which will be used for forecasting.\r\n\r\n![Ratio-to-moving average: Copy-and-paste normalised index](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--15.png?raw=true)\r\n\r\n\r\n### Step 6. SLOPE() and INTERCEPT() of CMA\r\n\r\nNext, create 2 cells to each calculate the slope and intercepts of the CMA values based on the overall index value.\r\n\r\n![Ratio-to-moving average: Slope and intercept](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--16.png?raw=true)\r\n\r\n\r\n### Step 7. Trendline level\r\n\r\nFor each month that will be forecasted, create a column to calculate trendline level.\r\nThis calculation will be the INTERCEPT() result + SLOPE() result x overall index\r\n\r\n![Ratio-to-moving average: Trendline level](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--17.png?raw=true)\r\n\r\n\r\n### Step 8. Forecast using the trendline level and normalised index\r\n\r\nLastly, calculate the forecast by multiplying the trendline value for the month number, with a VLOOKUP of the normalised index.\r\n\r\n![Ratio-to-moving average: Forecast](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--18.png?raw=true)\r\n\r\n\r\nNext we\'ll look at how we can forecast more simply and with more accuracy.\r\n\r\n\r\n\r\n## Excel\'s FORECAST.ETS() function\r\n\r\nYou will only need the overall index to serve as a continuous reference, and the profit columns only.\r\nSelect them, including the indexes being forecasted for.\r\n\r\nThen from the Data tab > Forecast Sheet\r\n\r\n![FORECAST.ETS function: Forecast Sheet](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--19.png?raw=true)\r\n\r\nIn the Create Forecast Worksheet prompt, specify where the forecast starts and ends based on the index number, and the seasonality.\r\n\r\n![FORECAST.ETS function: Forecast Sheet prompt](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--20.png?raw=true)\r\n\r\nNow you have both the forecasted values and a line chart to show its accuracy.\r\n\r\n![FORECAST.ETS function: Forecast Sheet result](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--21.png?raw=true)\r\n\r\n\r\n\r\n## Compare forecasting accuracies of both methods\r\n\r\nThe mean absolute percentage error (MAPE) is used to check how much error the forecast could have. For example, a MAPE score of 5% means the forecast can be off by about 5%.\r\n\r\nHow to calculate the MAPE:\r\n\r\n### Step 1. Apply your forecasting method to actuals\r\n\r\nFor each of the methods above, create a new column that calculates the forecasts for the actual values you already have.\r\n\r\n* **Ratio-to-moving average**\r\n\r\nThe formula used to forecast the later months is easily adapted to existing months.\r\n\r\n![Forecast on actuals: Ratio-to-moving average](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--22.png?raw=true)\r\n\r\n* **Forecast sheet**\r\n\r\nFor the Forecast Sheet method, rather than clicking on the Forecast Sheet button, simply apply the same formula in the forecasted cells.\r\n\r\n![Forecast on actuals: Forecast Sheet](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--23.png?raw=true)\r\n\r\n\r\n### Step 2. Create the ABS % of the error\r\n\r\nCreate a new column to calculate the absolute percentage of the error between the actual and the forecast for it.\r\n\r\nThis is calculated as ABS((Actual - Forecast) \xf7 Actual)\r\n\r\n![Forecast on actuals: Absolute percentage of the error](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--24.png?raw=true)\r\n\r\n\r\n### Step 3. Create the MAPE\r\n\r\nTake the average of these absolute percentages of errors to get the MAPE.\r\n\r\nComparing the two methods above, the MAPE scores are:\r\n\r\n* manual ratio-to-moving average method: 14.95%\r\n* Excel\'s FORECAST.ETS function: 1.16%\r\n\r\nSo clearly, the FORECAST.ETS function (at least on this dataset), is far less off in terms of forecasting accuracy than ratio-to-moving average. The performance of Excel\'s algorithm over the manual method is also self-evident by comparing the line charts.\r\n\r\n![Comparing MAPE scores of different methods](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--25.png?raw=true)\r\n\r\n\r\n## Manual: Checking for autocorrelation\r\n\r\nHowever, the presence of autocorrelation (which seeks to detect the infrequency of + and - sign changes in errors) can negate the findings from MAPE. So as a final step, we can check for that in our model.\r\n\r\n\r\n### Step 1. Visualise the linear trendline coefficients for actuals\r\n\r\nAs an optional step, plot the actuals on a line chart, then add a linear trendline.\r\nThis will help you visually validate the values against the next step.\r\n\r\n![Excel: Linear trendline of actuals](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--26.png?raw=true)\r\n\r\n\r\n### Step 2. Use the LINEST() function to get the coefficients\r\n\r\nPass the actual profit values, and the overall index as parameters into the LINEST() function. Be sure to click Citrl + Shift + Enter when completing the formula.\r\n\r\nThe first rows contains the slope and the y-intercept seen in the line chart\'s linear trendline.\r\n\r\n![Excel: LINEST() function](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--27.png?raw=true)\r\n\r\n\r\n### Step 3. Calculate the + or - error values\r\n\r\nCreate a new column to calculate the positive or negative error values using the following equation:\r\n\r\nactual - (slope x month index + y-intercept)\r\n\r\n![Excel: + or - error values](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--28.png?raw=true)\r\n\r\n\r\n### Step 4. Check for positive or negative values\r\n\r\nCreate a new column that performs a binary check to see if the multiplication of the previous period\'s + or - error value is less than 0.\r\n\r\n![Excel: Binary sign change check](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--29.png?raw=true)\r\n\r\n\r\n### Step 5. Count the number of actual observations\r\n\r\nCreate a new reference cell to count the number of actual observations.\r\n\r\n![Autocorrelation: count observations](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--30.png?raw=true)\r\n\r\n\r\n### Step 6. Sum the number of sign changes\r\n\r\nThe result of Step 4. should be each actual observation having a 1 or 0 based on the sign of the previous value. So summing these will show the number of changes.\r\n\r\n![Autocorrelation: sum sign changes](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--31.png?raw=true)\r\n\r\n\r\n### Step 7. Calculate cut-off, then compare with sum of sign changes\r\n\r\nThe cut-off value indicates the minimum number of sign changes to not be impacted by autocorrelation.\r\n\r\nThe formula for this is:\r\n\r\n((observation count - 1) \xf7 2) - square root of (observation count - 1)\r\n\r\n![Autocorrelation: cut-off](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-10-29--32.png?raw=true)\r\n\r\nIn this case, the sum of sign changes (8) is higher than the cut-off (6), so autocorrelation does not undermine the MAPE check performed earlier.\r\n\r\n\r\n\r\n### References\r\n\r\n* LinkedIn Learning Course **[Excel Data Analysis: Forecasting by Wayne Winston](https://www.linkedin.com/learning/excel-data-analysis-forecasting)**\r\n\r\n* YouTube video **[Forecasting in Excel Made SIMPLE (include seasonality & make predictions) by Leila Gharani](https://www.youtube.com/watch?v=j22tLUQQDh4)**\r\n\r\n* Microsoft Office Community Question **[Using coefficients from trendline in Excel Chart](https://answers.microsoft.com/en-us/msoffice/forum/all/using-coefficients-from-trendline-in-excel-chart/16bb5160-1b3d-432e-86dc-8fb15b44100f)**\r\n\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2022-10-29.md)** for this post\'s markdown file in GitHub.'},{id:1663509600,title:"How to webscrape a YouTube channel's video titles using Python and Visual Studio Code",tag:"logo-python",date:"19 September 2022",content:"\r\nThis post shows how to webscrape all the video titles from a YouTube channel, despite the 50 result per request limit of the YouTube API version 3 (as at time of writing), using Visual Studio Code, and a mix of Python libraries.\r\n\r\n\r\n## Step 1. Log into console.cloud.google.com and create a new project.\r\n\r\nIn the example below, I created a new project named *\"Le Wagon YouTube Channel\"*, but the name you use may not particularly matter, as the purpose of creating the project is just to get an API key for web scraping.\r\n\r\nAs at the time of writing, you can get a Twitter Developer account off the back of an existing Twitter account or create a new one, but you will need to provide your mobile phone number for verification.\r\n\r\n* **[https://console.cloud.google.com/](https://console.cloud.google.com/)**\r\n\r\n![Google Cloud Console: new project](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--01.png?raw=true)\r\n\r\n\r\n## Step 2. Enable the YouTube Data API v3 and create an API key\r\n\r\nFrom within the project, go to the APIs and services section > Library > search for YouTube Data API v3 > Enable. \r\n\r\n![Google Cloud Console: enable YouTube API](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--02.png?raw=true)\r\n\r\nOnce enabled, go to the APIs and services section > Credentials > create API Key\r\n\r\n![Google Cloud Console: generate API key](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--03.png?raw=true)\r\n\r\n\r\n## Step 3. Install Python, create a virtual environment, and install libraries\r\n\r\nInstall Python\r\n* **[https://www.python.org/downloads/windows/](https://www.python.org/downloads/windows/)**\r\n\r\nCreate a folder for your Python project and open it in Visual Studio Code.\r\nIn this example, the folder name is *Project* and is on the Desktop.\r\n\r\nCreate your virtual environment as a subfolder of your project.\r\n\r\n```\r\npython -m venv C:\\Users\\Administrator\\Desktop\\Project\\venv\r\n```\r\n\r\nInstall the Python Extension for Visual Studio Code.\r\n\r\n![Visual Studio Code: create and activate virtual environment](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--04.png?raw=true)\r\n\r\nYou can find more detailed screenshot steps of the above from a previous blog post:\r\n* **[Add forecasts from Python using Visual Studio Code to Power BI](https://datamesse.github.io/#/post/1650117600)**\r\n\r\n\r\nFrom within the Python virtual environment, install the required dependencies for the script.\r\n\r\n```\r\npip install google-api-python-client\r\npip install pandas\r\npip install numpy\r\n```\r\n\r\n![Visual Studio Code: install libraries](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--05.png?raw=true)\r\n\r\n\r\n## Step 4. Find the YouTube channel's ID via browser source code\r\n\r\nNavigate to the main page for the YouTube channel you want to scrape, then right-click > View page source.\r\n\r\n![Browser: YouTube channel](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--06.png?raw=true)\r\n\r\nPress Ctrl + F to find the *externalId* attribute, as its value is the channel ID.\r\n\r\n![Browser: YouTube channel ID](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--07.png?raw=true)\r\n\r\nIn this example, Le Wagon's channel ID is **'UChCDYcBCrb8tuPAO6e0P-Hw'**\r\n\r\nBefore creating the python file to add and run the script, I've outlined how it works below.\r\n\r\nIf you do not want an overview how the code works and just want to implement the code, then skip the *Code explanation* to Step 5, otherwise you can continue below:\r\n\r\n\r\n## Code explanation\r\n\r\nThe first half of the code is mostly based on this YouTube tutorial by techTFQ:\r\n\r\n* YouTube video **[\"Python Project to Scrape YouTube using YouTube Data API | Analyze and Visualize YouTube data\" by Thoufiq Mohammed](https://www.youtube.com/watch?v=SwSbnmqk3zY)**\r\n\r\nHowever, even though I copied the code from the tutorial, I found that I was unable to pull more than 50 records into a single file, despite the video showing it was capable of doing that.\r\n\r\nI was unsure if this was due to issues with the code demonstrated, or perhaps changes to YouTube's API, such as regarding immediate subsequent calls as part of the 50 result limit, that made the code no longer capable of pulling more than 50 videos worth of data.\r\n\r\nRather than spend time trying to figure that out, I instead took the approach of running a loop that would send separate API requests and store each result into individual temporary files, then use Python to append all the results into a single file.\r\n\r\n### Use the channel ID to retrieve the uploads ID\r\n\r\nThis is the first part of the code, where you would need to substitute the ?????????? with your API key and the channel ID retrieved in earlier steps. It uses that information to pull the uploads ID for the channel, which lists all the video IDs.\r\n\r\n```\r\nfrom googleapiclient.discovery import build\r\nimport pandas as pd\r\nimport numpy as np\r\nimport csv\r\nimport glob\r\nimport re\r\nimport json\r\n\r\napi_key = '??????????'\r\nservice = build('youtube', 'v3', developerKey=api_key)\r\nchannel_id = '??????????'\r\n\r\n\r\n\r\n# EXTRACT YOUTUBE CHANNEL'S UPLOADS ID FOR FULL PLAYLIST\r\n\r\ndef get_uploads_id(service, channel_id):\r\n\r\n    request = service.channels().list(\r\n        part='contentDetails',\r\n        id=channel_id\r\n    )\r\n    response = request.execute()\r\n\r\n    for channel in response['items']:\r\n        channelstats = channel['contentDetails']['relatedPlaylists']['uploads']\r\n\r\n    return channelstats\r\n\r\nuploadsID = (get_uploads_id(service, channel_id))\r\n```\r\n\r\n![Visual Studio Code: Python part 1](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--08.png?raw=true)\r\n\r\n\r\n### Store all the video ids from the channel\r\n\r\nThis second part of the code is pretty much straight from Thoufiq Mohammed's YouTube tutorial, and it retrieves all of the video IDs for the selected channel, then stores them into an array. In order to account for the 50 result per request limit, Thoufiq's code uses page tokens and a for loop to go through each \"page\" of 50 video IDs.\r\n\r\n* YouTube video **[\"Python Project to Scrape YouTube using YouTube Data API | Analyze and Visualize YouTube data\" by Thoufiq Mohammed](https://www.youtube.com/watch?v=SwSbnmqk3zY)**\r\n\r\n```\r\n# EXTRACT YOUTUBE VIDEO IDS TO SEPARATE CSV FILES\r\n\r\ndef get_video_ids(service, uploadsID):\r\n\r\n    request = service.playlistItems().list(\r\n        part='contentDetails',\r\n        playlistId = uploadsID,\r\n        maxResults = 50)\r\n    response = request.execute()\r\n\r\n    video_ids = []\r\n\r\n    for i in range(len(response['items'])):\r\n        video_ids.append(response['items'][i]['contentDetails']['videoId'])\r\n\r\n    next_page_token = response.get('nextPageToken')\r\n    more_pages = True\r\n\r\n    while more_pages:\r\n        if next_page_token is None:\r\n            more_pages = False\r\n        else:\r\n            request = service.playlistItems().list(\r\n                        part='contentDetails',\r\n                        playlistId = uploadsID,\r\n                        maxResults = 50,\r\n                        pageToken = next_page_token)\r\n            response = request.execute()   \r\n\r\n            for i in range(len(response['items'])):\r\n                video_ids.append(response['items'][i]['contentDetails']['videoId'])\r\n\r\n            next_page_token = response.get('nextPageToken')\r\n \r\n    return (video_ids)\r\n\r\nvideo_ids = get_video_ids(service, uploadsID)\r\n```\r\n\r\n![Visual Studio Code: Python part 2](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--09.png?raw=true)\r\n\r\n### Extract all the video ids into separate csv files\r\n\r\nThoufiq's code stores data for the API requests with variable arrays, as you would typically expect. But as it was not working for me, rather than figure out what other variation of similar in-memory based API requests would work, I took a primitive approach of separating the array into csv files with 50 video IDs each.\r\n\r\nWhether it's because of the different approach to pulling the data for the API requests, or its implied slower performance for subsequent API requests, I placed my chances on this allowing me to get what I needed.\r\n\r\nThis third part of the code is simple:\r\n\r\n```\r\nchunk_size = 50\r\n\r\nvideo_id_chunks = [video_ids[i:i + chunk_size] for i in range(0, len(video_ids), chunk_size)]\r\n\r\nfor i in range(len(video_id_chunks)):\r\n\r\n    filename = 'video_ids_' + str(i) + '.csv'\r\n    file = open(filename, 'w', newline='', encoding='utf-8')\r\n    writer = csv.writer(file)\r\n    writer.writerow(video_id_chunks[i])\r\n    file.close()\r\n```\r\n\r\n![Visual Studio Code: Python part 3a](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--10.png?raw=true)\r\n\r\nHere you can see an example of what the comma separated files look like:\r\n\r\n![Visual Studio Code: Python part 3b](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--11.png?raw=true)\r\n\r\n### Loop each csv for YouTube API requests then store results into separate json files\r\n\r\nThis fourth part of the code creates a glob variable to list all the temp files containing the video IDs, then loops it for each file, which in turn each file has all its videos looped through to retrieve the video details via the API such as title and publish date, and exports each result set to separate temporary json files.\r\n\r\nThere is one issue here that I have not yet solved. Note that the dictionary keys for count of views, likes, and comments are commented out.\r\n\r\nIt is possible to uncomment these and retrieve results. However, in my testing I found that some videos returned a KeyError that stopped the whole process. Typically the way to deal with this is to create a try excepts, but as I am still a Python novice, could not figure out how to design it for a dict(), and didn't really need these fields, I didn't spend time to try to solve that yet.\r\n\r\n```\r\n# EXTRACT YOUTUBE VIDEO DETAILS TO SEPARATE JSON FILES\r\n\r\nall_files_ids = glob.glob(\"video_ids_*.csv\")\r\n\r\nfor i in range(len(all_files_ids)):\r\n\r\n    with open(all_files_ids[i]) as csvfile:\r\n        video_ids_for_details = np.loadtxt(csvfile, delimiter=',', dtype='str')\r\n\r\n    def get_video_details(service, video_ids_for_details):\r\n\r\n        all_video_stats = []\r\n\r\n        for i in range(0, len(video_ids_for_details), 50):\r\n            request = service.videos().list(\r\n                part='snippet,statistics',\r\n                id=','.join(video_ids_for_details[i:i+50]))\r\n            response = request.execute()\r\n\r\n            for video in response['items']:\r\n                video_stats = dict(Title = video['snippet']['title'],\r\n                                   Video_id = video['id'],\r\n                                   # Views = video['statistics']['viewCount'],\r\n                                   # Likes = video['statistics']['likeCount'],\r\n                                   # Comments = video['statistics']['commentCount'],\r\n                                   Published_date = video['snippet']['publishedAt'])\r\n                all_video_stats.append(video_stats)\r\n\r\n            return all_video_stats\r\n\r\n    csv_video_list = pd.DataFrame(get_video_details(service, video_ids_for_details))\r\n    csv_video_list.to_json(re.sub('_ids_','_details_', re.sub('.csv', '', all_files_ids[i])) + '.json', orient='records')\r\n```\r\n\r\n![Visual Studio Code: Python part 4a](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--12.png?raw=true)\r\n\r\nHere you can see an example of what the json files look like (though honestly I applied the pretty-print function from Notepad++ to clean it up a little):\r\n\r\n![Visual Studio Code: Python part 4b](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--13.png?raw=true)\r\n\r\n### Combine the temporary json files into a single json file\r\n\r\nThe fifth and final step of the code is to combine all the data from each of the temporary json files into a single json file output. Looking back on this, I should have included a clean up script component to delete the temporary csv and json files.\r\n\r\n```\r\n# COMBINE YOUTUBE VIDEO DETAILS INTO A SINGLE JSON FILE\r\n\r\nall_files_details = glob.glob(\"video_details_*.json\")\r\n\r\ndef merge_JsonFiles(filename):\r\n    result = list()\r\n    for f1 in filename:\r\n        with open(f1, 'r') as infile:\r\n            result.extend(json.load(infile))\r\n\r\n    with open('compileddata.json', 'w') as output_file:\r\n        json.dump(result, output_file)\r\n\r\nmerge_JsonFiles(all_files_details)\r\n```\r\n\r\n![Visual Studio Code: Python part 5](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--14.png?raw=true)\r\n\r\n\r\n## Step 5. Install Python, create a virtual environment, and install libraries\r\n\r\nCreate a new Python file in your project directory, in my case it is named *extract-script.py*.\r\n\r\nThen copy-and-paste the completed code below (substituting your API key and channel ID), then click Run Python File.\r\n\r\n![Visual Studio Code: Run Python File](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--15.png?raw=true)\r\n\r\nThe completed json output is the file named *compileddata.json*.\r\n\r\n![Visual Studio Code: Output json file](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-09-19--16.png?raw=true)\r\n\r\n**Note:** As-is, this code only extracts the YouTube video IDs, titles, and publish date. You can try to uncomment the view, like, and comment count, but there is a chance the script may fail due to KeyErrors.\r\nThat caveat aside, you now have all the video titles for a YouTube channel, and can explore more fields to pull via the YouTube Data API v3 official site.\r\n\r\n```\r\nfrom googleapiclient.discovery import build\r\nimport pandas as pd\r\nimport numpy as np\r\nimport csv\r\nimport glob\r\nimport re\r\nimport json\r\n\r\napi_key = '??????????'\r\nservice = build('youtube', 'v3', developerKey=api_key)\r\nchannel_id = 'UChCDYcBCrb8tuPAO6e0P-Hw'\r\n\r\n\r\n\r\n# EXTRACT YOUTUBE CHANNEL'S UPLOADS ID FOR FULL PLAYLIST\r\n\r\ndef get_uploads_id(service, channel_id):\r\n\r\n    request = service.channels().list(\r\n        part='contentDetails',\r\n        id=channel_id\r\n    )\r\n    response = request.execute()\r\n\r\n    for channel in response['items']:\r\n        channelstats = channel['contentDetails']['relatedPlaylists']['uploads']\r\n\r\n    return channelstats\r\n\r\nuploadsID = (get_uploads_id(service, channel_id))\r\n\r\n\r\n\r\n# EXTRACT YOUTUBE VIDEO IDS TO SEPARATE CSV FILES\r\n\r\ndef get_video_ids(service, uploadsID):\r\n\r\n    request = service.playlistItems().list(\r\n        part='contentDetails',\r\n        playlistId = uploadsID,\r\n        maxResults = 50)\r\n    response = request.execute()\r\n\r\n    video_ids = []\r\n\r\n    for i in range(len(response['items'])):\r\n        video_ids.append(response['items'][i]['contentDetails']['videoId'])\r\n\r\n    next_page_token = response.get('nextPageToken')\r\n    more_pages = True\r\n\r\n    while more_pages:\r\n        if next_page_token is None:\r\n            more_pages = False\r\n        else:\r\n            request = service.playlistItems().list(\r\n                        part='contentDetails',\r\n                        playlistId = uploadsID,\r\n                        maxResults = 50,\r\n                        pageToken = next_page_token)\r\n            response = request.execute()   \r\n\r\n            for i in range(len(response['items'])):\r\n                video_ids.append(response['items'][i]['contentDetails']['videoId'])\r\n\r\n            next_page_token = response.get('nextPageToken')\r\n \r\n    return (video_ids)\r\n\r\nvideo_ids = get_video_ids(service, uploadsID)\r\n\r\nchunk_size = 50\r\n\r\nvideo_id_chunks = [video_ids[i:i + chunk_size] for i in range(0, len(video_ids), chunk_size)]\r\n\r\nfor i in range(len(video_id_chunks)):\r\n\r\n    filename = 'video_ids_' + str(i) + '.csv'\r\n    file = open(filename, 'w', newline='', encoding='utf-8')\r\n    writer = csv.writer(file)\r\n    writer.writerow(video_id_chunks[i])\r\n    file.close()\r\n\r\n\r\n\r\n# EXTRACT YOUTUBE VIDEO DETAILS TO SEPARATE JSON FILES\r\n\r\nall_files_ids = glob.glob(\"video_ids_*.csv\")\r\n\r\nfor i in range(len(all_files_ids)):\r\n\r\n    with open(all_files_ids[i]) as csvfile:\r\n        video_ids_for_details = np.loadtxt(csvfile, delimiter=',', dtype='str')\r\n\r\n    def get_video_details(service, video_ids_for_details):\r\n\r\n        all_video_stats = []\r\n\r\n        for i in range(0, len(video_ids_for_details), 50):\r\n            request = service.videos().list(\r\n                part='snippet,statistics',\r\n                id=','.join(video_ids_for_details[i:i+50]))\r\n            response = request.execute()\r\n\r\n            for video in response['items']:\r\n                video_stats = dict(Title = video['snippet']['title'],\r\n                                   Video_id = video['id'],\r\n                                   # Views = video['statistics']['viewCount'],\r\n                                   # Likes = video['statistics']['likeCount'],\r\n                                   # Comments = video['statistics']['commentCount'],\r\n                                   Published_date = video['snippet']['publishedAt'])\r\n                all_video_stats.append(video_stats)\r\n\r\n            return all_video_stats\r\n\r\n    csv_video_list = pd.DataFrame(get_video_details(service, video_ids_for_details))\r\n    csv_video_list.to_json(re.sub('_ids_','_details_', re.sub('.csv', '', all_files_ids[i])) + '.json', orient='records')\r\n\r\n\r\n\r\n# COMBINE YOUTUBE VIDEO DETAILS INTO A SINGLE JSON FILE\r\n\r\nall_files_details = glob.glob(\"video_details_*.json\")\r\n\r\ndef merge_JsonFiles(filename):\r\n    result = list()\r\n    for f1 in filename:\r\n        with open(f1, 'r') as infile:\r\n            result.extend(json.load(infile))\r\n\r\n    with open('compileddata.json', 'w') as output_file:\r\n        json.dump(result, output_file)\r\n\r\nmerge_JsonFiles(all_files_details)\r\n```\r\n\r\n\r\n\r\n### References\r\n\r\n* YouTube video **[\"Python Project to Scrape YouTube using YouTube Data API\" | Analyze and Visualize YouTube data by Thoufiq Mohammed](https://www.youtube.com/watch?v=SwSbnmqk3zY)**\r\n\r\n* YouTube video **[\"How to split a list into evenly sized chunks in Python | Python Tutorial\" by Jie Jenn](https://www.youtube.com/watch?v=SuEk_TBkReQ)**\r\n\r\n* YouTube video **[\"How to Read Multiple CSV Files in Python | For-Loop + 2 More\" by Business Science](https://www.youtube.com/watch?v=TN_Cvyq_rxE)**\r\n\r\n* Soft Hints blog post **[How to Merge Multiple JSON Files Into Pandas DataFrame blog post by John D K](https://softhints.com/merge-multiple-json-files-pandas-dataframe/)**\r\n\r\n* Stack Overflow Question **[KeyError: 'commentCount' using Youtube API in Python](https://stackoverflow.com/questions/70103870/keyerror-commentcount-using-youtube-api-in-python)**\r\n\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2022-09-19.md)** for this post's markdown file in GitHub."},{id:1661522400,title:"Creating a decorated scatterplot visual using Deneb and Power BI",tag:"logo-powerbi",date:"27 August 2022",content:'\r\nThis is my second attempt with Deneb, and this time I\'ve designed a more advanced and fancier zoomable scatterplot. The centre of the axis represents Earth, and the data points around it represent satellites, whereby their distance from the centre reflects their average orbital distance from Earth, and their circumferential position relative to the centre reflects the year they were launched.\r\n\r\n\r\nThe final Power BI report *"Satellite launch overview"* is available here:\r\n* **[https://community.powerbi.com/t5/Data-Stories-Gallery/Satellite-launch-overview/m-p/2730077](https://community.powerbi.com/t5/Data-Stories-Gallery/Satellite-launch-overview/m-p/2730077)**\r\n\r\n\r\n![My Power BI visualisation](https://github.com/datamesse/datamesse.github.io/blob/main/src/assets-portfolio/img-2022-08-satellite-launch-overview.gif?raw=true)\r\n\r\n### Requirements\r\n\r\nDeneb is a highly flexible custom visual developed by **[Daniel Marsh-Patrick](https://twitter.com/the_d_mp)** that can be used to create unique visualisations for Power BI reports with JSON-based syntaxes; Vega or Vega-lite.\r\n\r\nDeneb can be downloaded from the Microsoft App Store:\r\n\r\n* **[https://appsource.microsoft.com/en-US/product/power-bi-visuals/coacervolimited1596856650797.deneb?tab=Overview](https://appsource.microsoft.com/en-US/product/power-bi-visuals/coacervolimited1596856650797.deneb?tab=Overview)**\r\n\r\nThe official website for Deneb:\r\n\r\n* **[https://deneb-viz.github.io/](https://deneb-viz.github.io/)**\r\n\r\nTwo satellite datasets were merged together for my report, but this custom visual design only uses the one from the Union of Concerned Scientists:\r\n\r\n* **[https://www.ucsusa.org/resources/satellite-database](https://www.ucsusa.org/resources/satellite-database)**\r\n\r\nFor more information on my dataset merge using Power Query, see my Github:\r\n\r\n* **[https://github.com/datamesse/data-visualisation-datasets/tree/main/Satellite%20launch%20overview](https://github.com/datamesse/data-visualisation-datasets/tree/main/Satellite%20launch%20overview)**\r\n\r\n\r\n### Inspiration\r\n\r\nThe main inspirations for this custom visual are these Tableau Public community gallery visualisations:\r\n\r\n* [Yuli Wang\'s :](https://twitter.com/Yuli_Wg) **[50 years of Chinese Aerospace](https://public.tableau.com/app/profile/yuri.wg/viz/50_15655374759590/1)**\r\n\r\n![Tableau satellite report by Yuli Wang](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-08-27--01.png?raw=true)\r\n\r\n* [Wendy Shijia\'s :](https://twitter.com/ShijiaWendy) **[50 Years of China\'s Space Journey](https://public.tableau.com/app/profile/wendy.shijia/viz/50YearsofChinasSpaceJourney/50yrsChinasSpaceJourney)**\r\n\r\n![Tableau satellite report by Wendy Shijia](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-08-27--01a.png?raw=true)\r\n\r\n* [Jean-Paul Frenett\'s :](https://public.tableau.com/app/profile/jean.paul.frenett) **[Satellites of Earth](https://public.tableau.com/app/profile/jean.paul.frenett/viz/SatellitesofEarth/SatellitesinOrbit)**\r\n\r\n![Tableau satellite report by Jean-Paul Frenett](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-08-27--02.png?raw=true)\r\n\r\n* [Lubos Pribula\'s :](https://twitter.com/LubosPribula) **[Traffic Overhead](https://public.tableau.com/app/profile/lubospribula/viz/TrafficOverhead/TrafficOverhead)**\r\n\r\n![Tableau satellite report by Lubos Pribula](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-08-27--03.png?raw=true)\r\n\r\n\r\n## BUILDING THE SATELLITE CUSTOM VISUAL USING DENEB\r\n\r\n\r\n### Step 1: Set up the data and custom columns needed to plot each satellite \r\n\r\nFollow the Power Query code from my Github repository (mentioned in the Requirements above) to import the data into Power BI.\r\nThe end result should be a single dimension table.\r\n\r\nFrom Report view, add the following custom columns to that dimension table using DAX:\r\n\r\n**Cos**\r\n\r\n```\r\nCos = \r\nVAR EarliestLaunch = CALCULATE(MIN(Satellites[Launch date]), ALLEXCEPT(Satellites,Satellites[Record type]))\r\nVAR LatestLaunch = CALCULATE(MAX(Satellites[Launch date]), ALLEXCEPT(Satellites,Satellites[Record type]))\r\nVAR Angle = IF(ISBLANK([Orbit class]) = FALSE, \r\n               (1.5 * PI() *\r\n                (INT(Satellites[Launch date] - EarliestLaunch)) \r\n                 /\r\n                (INT(LatestLaunch - EarliestLaunch))),\r\n               BLANK()\r\n            )\r\nVAR OrbitRadius = \r\n    SWITCH([Orbit class],\r\n           "Low Earth Orbit", Satellites[Distance (km)],\r\n           "Medium Earth Orbit", Satellites[Distance (km)],\r\n           "Geostationary Transfer Orbit", Satellites[Distance (km)],\r\n           "Geosynchronous Equatorial Orbit", Satellites[Distance (km)],\r\n           BLANK()\r\n    )\r\nRETURN IF(ISBLANK(Satellites[Orbit class]) = FALSE, (COS(Angle) * (OrbitRadius)))\r\n```\r\n\r\n**Sin**\r\n\r\n```\r\nSin = \r\nVAR EarliestLaunch = CALCULATE(MIN(Satellites[Launch date]), ALLEXCEPT(Satellites,Satellites[Record type]))\r\nVAR LatestLaunch = CALCULATE(MAX(Satellites[Launch date]), ALLEXCEPT(Satellites,Satellites[Record type]))\r\nVAR Angle = IF(ISBLANK([Orbit class]) = FALSE, \r\n               (1.5 * PI() *\r\n                (INT(Satellites[Launch date] - EarliestLaunch)) \r\n                 /\r\n                (INT(LatestLaunch - EarliestLaunch))),\r\n               BLANK()\r\n            )\r\nVAR OrbitRadius = \r\n    SWITCH([Orbit class],\r\n           "Low Earth Orbit", Satellites[Distance (km)],\r\n           "Medium Earth Orbit", Satellites[Distance (km)],\r\n           "Geostationary Transfer Orbit", Satellites[Distance (km)],\r\n           "Geosynchronous Equatorial Orbit", Satellites[Distance (km)],\r\n           BLANK()\r\n    )\r\nRETURN IF(ISBLANK(Satellites[Orbit class]) = FALSE, (SIN(Angle) * (OrbitRadius)))\r\n```\r\n\r\nThe idea behind using Sin and Cos for the calculations to map the satellites as a scatterplot is based on Yuli Wang\'s Tableau Public report. That report additionally (and very cleverly) removes the scaled distance of outlier satellites with average distances tremendously greater than the others, and in doing so is able to fit all the satellites onscreen in a neat and ordered way.\r\n\r\nHowever, the approach we will take is to deliberately show that scaled distance, even if the outliers are not immediately seen. As at the time of writing, Tableau is unable to zoom freely to resize what is seen onscreen, whereas Power BI can via Deneb and Vega-Lite, and in a way enhances the user\'s experience to be able to see and interact with that scale.\r\n\r\nAs a next step, import Deneb into your report, adding an instance of it to the page, and resize it to the full screen. Add the fields from the dimension table to the deneb visual as per screenshot below.\r\n\r\n![Power BI: Adding dimension fields to the Deneb custom visual](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-08-27--04.png?raw=true)\r\n\r\n\r\n### Step 2: Create your planet Earth image base64 code\r\n\r\nNext find a .png image of Earth that will be used as the centre axis i.e. the origin (0,0).\r\n\r\nIn my case, I used this Earth image by PIRO4D from pixabay.com \r\n\r\n* **[https://pixabay.com/photos/earth-globe-sea-trenches-2314687/](https://pixabay.com/photos/earth-globe-sea-trenches-2314687/)**\r\n\r\n![Earth globe sea trenches](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-08-27--05.png?raw=true)\r\n\r\nIf the image you want to use doesn\'t have a transparent background, you can use an online tool such as **[https://onlinepngtools.com/crop-png](https://onlinepngtools.com/crop-png)** to trim out that background.\r\n\r\n![Trim out background part of the image](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-08-27--06.png?raw=true)\r\n\r\nNote that the larger the image, the more code it will generate when converted to base64, which in turn impacts the performance of the Deneb code for editing and rendering. You can use an online tool such as **[https://onlinepngtools.com/resize-png](https://onlinepngtools.com/resize-png)** to resize it, in my case to 50 x 50 pixels.\r\n\r\n![Resize the image](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-08-27--07.png?raw=true)\r\n\r\nThen lastly, use an online tool such as [https://www.browserling.com/tools/image-to-base64](https://www.browserling.com/tools/image-to-base64) to convert the trimmed .png image to base64 code.\r\n\r\n![Generate base64 code for the image](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-08-27--08.png?raw=true)\r\n\r\nCopy out the base64 code and save it e.g. to a .txt file, as we will use it a little later.\r\n\r\nIn my case, you can download the base64 code I used from my Github repository:\r\n\r\n* **[https://raw.githubusercontent.com/datamesse/data-visualisation-datasets/main/Satellite%20launch%20overview/images/base64code%20for%20earth-50x50.txt](https://raw.githubusercontent.com/datamesse/data-visualisation-datasets/main/Satellite%20launch%20overview/images/base64code%20for%20earth-50x50.txt)**\r\n\r\n\r\n### Step 3: Set a dark background for the entire report canvas\r\n\r\nYou can either set a solid black or other dark coloured background.\r\n\r\nIn my case, I used this galaxy wallpaper image by Adrien Olichon from pexels.com\r\n\r\n* **[https://www.pexels.com/photo/galaxy-wallpaper-2538107/](https://www.pexels.com/photo/galaxy-wallpaper-2538107/)**\r\n\r\n![Galaxy wallpaper](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-08-27--09.png?raw=true)\r\n\r\n![Power BI add background image](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-08-27--10.png?raw=true)\r\n\r\nThe colour palette we will use for the actual custom visual is lighter to contrast with the dark background.\r\n\r\n\r\n### Step 4: Begin setting up the core Vega-lite code for the Deneb visual\r\n\r\nThere will be multiple layers of Vega-lite code placed on top of each other to create this visual.\r\n\r\nBegin by clearing out the defaulted code, and replace it with the following core code:\r\n\r\n```\r\n{ "background": "null",\r\n  "view": {"fill": "#0B0D14", \r\n           "fillOpacity": 0 },\r\n  "data": {"name": "dataset"},\r\n  "layer": [\r\n  ],\r\n  "config": {\r\n    "legend": {"title": "null",\r\n               "titleColor": "#ffffff",\r\n               "labelColor": "#ffffff",\r\n               "labelFontSize": 10,\r\n               "labelFont": "Consolas",\r\n               "orient": "top-left",\r\n               "labelLimit": 260  }\r\n  }\r\n}\r\n```\r\n\r\n![Deneb: Adding the core code structure](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-08-27--11.png?raw=true)\r\n\r\nOur next steps will be to fill in the **"layer": [ ]** array with multiple layers to complete the visual.\r\n\r\nSo the layer code structure will appear like this:\r\n\r\n```\r\n{ "background": "null",\r\n  "view": {"fill": "#0B0D14", \r\n           "fillOpacity": 0 },\r\n  "data": {"name": "dataset"},\r\n  "layer": [\r\n            { JSON code for layer 5 e.g. axis label to display year of satellite launch },\r\n            { JSON code for layer 9 e.g. axis label to display average orbital distance from Earth },\r\n            { JSON code for layer 2 e.g. x-axis line },\r\n            { JSON code for layer 1 e.g. Earth image },\r\n            { JSON code for layer 4 e.g. scatterplot data points for the satellites },\r\n  ],\r\n  "config": {\r\n    "legend": {"title": "null",\r\n               "titleColor": "#ffffff",\r\n               "labelColor": "#ffffff",\r\n               "labelFontSize": 10,\r\n               "labelFont": "Consolas",\r\n               "orient": "top-left",\r\n               "labelLimit": 260  }\r\n  }\r\n}\r\n```\r\n\r\nThe logic behind how each layer is constructed is outlined below.\r\n\r\n\r\n### Step 5: Layer 1 - Adding planet Earth to the centre of the axis\r\n\r\nWe want to place the image in the centre of the axis i.e. at the origin, coordinates (0,0).\r\n\r\nWhilst building this visual, I\'ve tried providing the raw value of 0, but this does not appear to work, presumably because it does not reference an actual data record.\r\n\r\nThe workaround I came up with was to perform a generalised aggregate calculation across all records, in this case, the maximum value of the Sin field, and create a separate calculation to multiply that aggregate with zero, to get the zero-value needed for axis mapping.\r\n\r\n```\r\n{ "transform": [\r\n      { "aggregate": [{"op": "max", "field": "Sin", "as": "max_sin"}] },\r\n      { "calculate": "datum.max_sin * 0", "as": "origin zero" } ],\r\n```               \r\n\r\nThen we encode the data point\'s x and y axis to the calculation\'s zero value.\r\n\r\n```\r\n"encoding": { "x": { "aggregate": "median",\r\n                     "field": "origin zero",\r\n                     "type": "quantitative"},\r\n              "y": { "aggregate": "median",\r\n                     "field": "origin zero",\r\n                     "type": "quantitative"},\r\n```\r\n\r\nLastly, we embed the Earth image\'s base64 code.\r\n\r\n\r\n**Layer code to add to visual:**\r\n\r\n```\r\n{ "transform": [\r\n       { "aggregate": [{"op": "max", "field": "Sin", "as": "max_sin"}] },\r\n       { "calculate": "datum.max_sin * 0", "as": "origin zero" } ],\r\n  "mark": {"type": "image",\r\n           "width": 75,\r\n           "height": 75},\r\n  "encoding": { "x": { "aggregate": "median",\r\n                       "field": "origin zero",\r\n                       "type": "quantitative"},\r\n                "y": { "aggregate": "median",\r\n                       "field": "origin zero",\r\n                       "type": "quantitative"},\r\n                "url": { "datum": "data:image/png;base64, << replace this message with your base64 code >>",\r\n                         "type":  "nominal"}\r\n              }\r\n}\r\n```\r\n\r\n**Note:** Because the base64 code is long, I\'ve excluded it from the code above, but you would just copy-and-paste it in place of the << >> message, as per screenshot below\r\n\r\n![Deneb: Adding the Earth layer code](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-08-27--12.png?raw=true)\r\n\r\n\r\n### Step 6: Layers 2 & 3 - Adding the lines for the x and y axis\r\n\r\nFollowing the same multiply by zero calculation logic of the previous layer, we can create "rule"-type marks to draw out the lines for the x and y axis.\r\n\r\n**Layer code to add to visual:**\r\n\r\nAdd this code above the Earth layer, so that the lines are hidden behind the Earth image as they pass through it.\r\n\r\nThe further down you place an element within the **"layer": [ ]** array, the more it is "on top" of other layers.\r\n\r\n```\r\n{ "transform": [\r\n      { "aggregate": [{"op": "max", "field": "Sin", "as": "max_sin"}] },\r\n      { "calculate": "datum.max_sin * 0", "as": "x-axis zero for vertical rule" } ],\r\n  "mark": {"type": "rule",\r\n           "strokeDash": [1, 10],\r\n           "strokeDashOffset": 50000},\r\n  "encoding": { "x": { "field": "x-axis zero for vertical rule",\r\n                       "type": "quantitative"},\r\n                       "color": {"value": "#9DA4AC"},\r\n                       "size": {"value": 1} }\r\n  },\r\n{ "transform": [\r\n      { "aggregate": [{"op": "max", "field": "Sin", "as": "max_sin"}] },\r\n      { "calculate": "datum.max_sin * 0", "as": "y-axis zero for horizontal rule" } ],\r\n  "mark": {"type": "rule",\r\n           "strokeDash": [1, 10],\r\n           "strokeDashOffset": 50000},\r\n  "encoding": { "y": { "field": "y-axis zero for horizontal rule",\r\n                       "type": "quantitative"},\r\n                       "color": {"value": "#9DA4AC"},\r\n                       "size": {"value": 1} }\r\n},\r\n```\r\n\r\n![Deneb: Adding the x axis and y axis layer codes](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-08-27--13.png?raw=true)\r\n\r\n\r\n### Step 7: Layer 4 - Adding scatterplot data points for the satellite data\r\n\r\nThis is the most important layer of the visual, as it enables zoom in and out with the user\'s mouse scroll wheel, and is based on an official Vega-Lite example called the *Scatterplot Pan & Zoom*.\r\n\r\n* **[https://vega.github.io/vega-lite/examples/selection_translate_scatterplot_drag.html](https://vega.github.io/vega-lite/examples/selection_translate_scatterplot_drag.html)**\r\n\r\nThe concept is to bind the scales as a parameter, and use the axis\' **"scale": {"domain"}** attribute to define a default zoom setting, which the user can get back to after scrolling, by double-clicking the visual.\r\n\r\nThe x-axis and y-axis are mapped to the satellite\'s **Sin** and **Cos** values respectively\r\n\r\n```\r\n{ "params": [{ "name": "grid",\r\n               "select": "interval",\r\n               "bind": "scales"},\r\n             { "name": "legend",\r\n               "select": {"type": "point", "fields": ["Orbit class"]},\r\n               "bind": "legend"} ],\r\n  "mark": { "type": "point", \r\n            "shape": "circle",\r\n            "tooltip": true,\r\n            "filled": true,\r\n            "fillOpacity": 0.75},\r\n  "encoding": { "x": {"field": "Sin", \r\n                      "type": "quantitative",\r\n                      "title": false, \r\n                      "axis": {"disable": true,\r\n                               "grid": false,\r\n                               "labels": false},\r\n                               "scale": {"domain": [-70400,70050]}},\r\n                "y": {"field": "Cos", \r\n                      "type": "quantitative",\r\n                      "title": false, \r\n                      "axis": {"disable": true,\r\n                               "grid": false,\r\n                               "labels": false},\r\n                               "scale": {"domain": [-69540,60370]}},\r\n```\r\n\r\nIn this case, we\'ve additionally defined a legend bind as an extra **params** element so that selecting an orbit class in the legend highlights satellite data points for that class.\r\n\r\nBy default, Vega-Lite sorts a column\'s values alphabetically, but you can manually define the sort order, as well as the colour scheme.\r\n\r\n```\r\n"color": { "field": "Orbit class",\r\n           "type": "nominal",\r\n           "sort": ["Geostationary Transfer Orbit", "Geosynchronous Equatorial Orbit", "Medium Earth Orbit", "Low Earth Orbit"],\r\n           "scale": {"range": ["#C8F6FF", "#8DECFF", "#2DDBFF", "#11BBDE"]} },\r\n```\r\n\r\n\r\n**Layer code to add to visual:**\r\n\r\nAdd this code below the Earth layer, so it is the last element in the **"layer": [ ]** array. This allows users to easily hover over the data points and show tooltips without the interference of other layers.\r\n\r\n```\r\n{ "params": [{ "name": "grid",\r\n               "select": "interval",\r\n               "bind": "scales"},\r\n             { "name": "legend",\r\n               "select": {"type": "point", "fields": ["Orbit class"]},\r\n               "bind": "legend"} ],\r\n  "mark": { "type": "point", \r\n            "shape": "circle",\r\n            "tooltip": true,\r\n            "filled": true,\r\n            "fillOpacity": 0.75},\r\n  "encoding": { "x": {"field": "Sin", \r\n                      "type": "quantitative",\r\n                      "title": false, \r\n                      "axis": {"disable": true,\r\n                               "grid": false,\r\n                               "labels": false},\r\n                      "scale": {"domain": [-70400,70050]}},\r\n                "y": {"field": "Cos", \r\n                      "type": "quantitative",\r\n                      "title": false, \r\n                      "axis": {"disable": true,\r\n                               "grid": false,\r\n                               "labels": false},\r\n                               "scale": {"domain": [-69540,60370]}},\r\n                "size": {"value": 50},\r\n                "color": { "field": "Orbit class",\r\n                           "type":  "nominal",\r\n                           "sort": [ "Geostationary Transfer Orbit", "Geosynchronous Equatorial Orbit", "Medium Earth Orbit", "Low Earth Orbit"],\r\n                           "scale": {"range": ["#C8F6FF", "#8DECFF", "#2DDBFF", "#11BBDE" ]} },\r\n                "opacity": { "condition": {"param": "legend", "value": 1},\r\n                             "value": 0.025 },\r\n                "tooltip": [ {"field": "Satellite catalog number (NORAD)", "type": "nominal", "title": "NORAD"},\r\n                             {"field": "International designator (COSPAR)", "title": "COSPAR"},\r\n                             {"field": "Name", "title": "SATELLITE"},\r\n                             {"field": "Country", "title": "COUNTRY"},\r\n                             {"field": "Purpose", "title": "PURPOSE"},\r\n                             {"field": "Users", "title": "USERS"},\r\n                             {"field": "Launch date", "title": "LAUNCH DATE", "type": "temporal", "format": "%d-%m-%Y (DMY)"},\r\n                             {"field": "Status", "title": "STATUS"},\r\n                             {"field": "Orbit class", "title": "ORBIT CLASS"},\r\n                             {"field": "Distance (km)", "title": "AVERAGE DISTANCE (km)", "type": "nominal", "format": ",.0f"},\r\n                             {"field": "Apogee (km)", "title": "APOGEE (km)", "type": "nominal", "format": ",.0f"},\r\n                             {"field": "Perigee (km)", "title": "PERIGEE (km)", "type": "nominal", "format": ",.0f"},\r\n                             {"field": "Launch site", "title": "LAUNCH SITE"},\r\n                             {"field": "Launch vehicle", "title": "LAUNCH VEHICLE"}] }\r\n}\r\n```\r\n\r\n![Deneb: Adding the scatterplot for satellite data points](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-08-27--14.png?raw=true)\r\n\r\n\r\n### Step 8: Layers 5, 6, 7, and 8 - Adding year labels for the top, bottom, left, and right sides of the origin\r\n\r\nUnfortunately, I was unable to figure out how to programmatically determine the years that reflect each side of the x and y axis.\r\n\r\nFortunately, we have the satellite data points onscreen, so we can just hover over the satellites which sit on each of the 4 axis points, and manually hardcode the year values as labels.\r\n\r\nFor example, there is only one satellite sitting on the positive y-axis, and its tooltip shows that its launch year is 1974\r\n\r\n![Identify the year for positive y-axis](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-08-27--15.png?raw=true)\r\n\r\nIn this code we again use the multiply by zero calculation to align the position of the label, and define the distance of the label from the centre by applying the absolute function and multiply it with a fixed value of 50% the distance of the highest Sin value.\r\n\r\n```\r\n{ "transform": [\r\n       { "aggregate": [{"op": "max", "field": "Sin", "as": "max_sin"}] },\r\n       { "calculate": "datum.max_sin * 0", "as": "x-axis zero" },\r\n       { "calculate": "abs(datum.max_sin) * 0.5", "as": "y-axis top year" } ],\r\n  "mark": {"type": "text",\r\n           "style": "label",\r\n           "angle": 0,\r\n           "baseline": "top",\r\n           "align": "center",\r\n           "fontSize": 30 },\r\n  "encoding": { "x": { "field": "x-axis zero",\r\n                       "type": "quantitative"},\r\n                "y": { "field": "y-axis top year",\r\n                       "type": "quantitative"},\r\n                       "text": {"value": "1974"},\r\n                       "color": {"value": "#9DA4AC"} }\r\n},\r\n```\r\n\r\nThen rinse-and-repeat for the 3 other points from the origin.\r\n\r\n**Layer code to add to visual:**\r\n\r\nAdd these 4 layers for each of the 4 year labels, as the first elements in the **"layer": [ ]** array.\r\n\r\n```\r\n{ "transform": [\r\n       { "aggregate": [{"op": "max", "field": "Sin", "as": "max_sin"}] },\r\n       { "calculate": "datum.max_sin * 0", "as": "x-axis zero" },\r\n       { "calculate": "abs(datum.max_sin) * 0.5", "as": "y-axis top year" } ],\r\n  "mark": {"type": "text",\r\n           "style": "label",\r\n           "angle": 0,\r\n           "baseline": "top",\r\n           "align": "center",\r\n           "fontSize": 30 },\r\n  "encoding": { "x": { "field": "x-axis zero",\r\n                       "type": "quantitative"},\r\n                "y": { "field": "y-axis top year",\r\n                       "type": "quantitative"},\r\n                       "text": {"value": "1974"},\r\n                       "color": {"value": "#9DA4AC"} }\r\n},\r\n{ "transform": [\r\n    { "aggregate": [{"op": "max", "field": "Sin", "as": "max_sin"}] },\r\n                    { "calculate": "datum.max_sin * 0", "as": "x-axis zero" },\r\n                    { "calculate": "abs(datum.max_sin) * -1 * 0.6", "as": "y-axis bottom year" } ],\r\n      "mark": {"type": "text",\r\n               "style": "label",\r\n               "angle": 0,\r\n               "baseline": "bottom",\r\n               "align": "center",\r\n               "fontSize": 30 },\r\n      "encoding": { "x": {"field": "x-axis zero",\r\n                          "type":  "quantitative"},\r\n                    "y": {"field": "y-axis bottom year",\r\n                           "type":  "quantitative"},\r\n                    "text": {"value": "2006"},\r\n                    "color": {"value": "#9DA4AC"} }\r\n},\r\n{ "transform": [\r\n    { "aggregate": [{"op": "max", "field": "Sin", "as": "max_sin"}] },\r\n                    { "calculate": "datum.max_sin * 0", "as": "y-axis zero" },\r\n                    { "calculate": "abs(datum.max_sin) * 0.5", "as": "x-axis right year" } ],\r\n      "mark": {"type": "text",\r\n               "style": "label",\r\n               "angle": 0,\r\n               "baseline": "middle",\r\n               "align": "center",\r\n               "fontSize": 30 },\r\n      "encoding": { "y": {"field": "y-axis zero",\r\n                          "type":  "quantitative"},\r\n                    "x": {"field": "x-axis right year",\r\n                          "type":  "quantitative"},\r\n                    "text": {"value": "1990"},\r\n                    "color": {"value": "#9DA4AC"} }\r\n},\r\n{ "transform": [\r\n    { "aggregate": [{"op": "max", "field": "Sin", "as": "max_sin"}] },\r\n                    { "calculate": "datum.max_sin * 0", "as": "y-axis zero" },\r\n                    { "calculate": "abs(datum.max_sin) * -1 * 0.5", "as": "x-axis left year" } ],\r\n      "mark": {"type": "text",\r\n               "style": "label",\r\n               "angle": 0,\r\n               "baseline": "middle",\r\n               "align": "center",\r\n               "fontSize": 30 },\r\n      "encoding": { "y": {"field": "y-axis zero",\r\n                          "type":  "quantitative"},\r\n                    "x": {"field": "x-axis left year",\r\n                          "type":  "quantitative"},\r\n                    "text": {"value": "2021"},\r\n                    "color": {"value": "#9DA4AC"} }\r\n},\r\n```\r\n\r\n![Deneb: Adding year labels for each of the 4 points of the origin](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-08-27--16.png?raw=true)\r\n\r\n\r\n### Step 9: Layers 9 and more\r\n\r\nThe last part is to add multiple layers for each label along the axis to mark various satellite distances from Earth.\r\n\r\nIt is arbitrary how many labels you would like to add, but one idea is to include the minimum and maximum values for each Orbit class along each of the four axis parts, so that when a user zooms in, they have an idea of how many satellites fall within that min-max range.\r\n\r\nBecause the distance label will be marked along the axis, we need to filter the data by the year that part of the axis represents.\r\nIn this case, we are filtering by 2021 because we are using the negative part of the x-axis.\r\n\r\n```\r\n{ "timeUnit": "year", "field": "Launch date", "as": "date"},\r\n{ "filter": "year(datum.date) == 2021"},\r\n```\r\n\r\nTo suffix "km" to the distance value, simply concatenate it:\r\n\r\n```\r\n{ "calculate": "abs(round(datum.min_sin_GEO)) + \'km\'", "as": "display GEO max 2021 kms"},\r\n```\r\n\r\n**Layer code to add to visual:**\r\n\r\nAdd this code between the years layers and the axis layers within the **"layer": [ ]** array.\r\n\r\nThis is an example of a single label, for the satellite in Geosynchronous Equatorial Orbit with the lowest average distance from Earth and launched in 2021.\r\n\r\n```\r\n{ "transform": [\r\n          { "timeUnit": "year", "field": "Launch date", "as": "date"},\r\n          { "filter": "year(datum.date) == 2021"},\r\n          { "filter": {"field": "Orbit class", "equal": "Geosynchronous Equatorial Orbit"}},\r\n          { "aggregate": [{"op": "min", "field": "Sin", "as": "min_sin_GEO"}] },\r\n          { "calculate": "datum.min_sin_GEO * 0", "as": "y-axis zero" },\r\n          { "calculate": "abs(round(datum.min_sin_GEO)) + \'km\'", "as": "display GEO max 2021 kms"},\r\n          { "calculate": "abs(datum.min_sin_GEO) * -1", "as": "x-axis GEO max 2021 distance" } ],\r\n  "mark": {"type": "text", "style": "label", "angle": 0, "baseline": "bottom", "align": "center", "fontSize": 16 },\r\n  "encoding": { "y": {"field": "y-axis zero", "type":  "quantitative"},\r\n                "x": {"field": "x-axis GEO max 2021 distance", "type": "quantitative"},\r\n                "text": {"field": "display GEO max 2021 kms", "type": "nominal"},\r\n                "color": {"value": "#AAB6C4"} }\r\n},\r\n```\r\n\r\n![Deneb: Adding distance label](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-08-27--17.png?raw=true)\r\n\r\nYou can find more distance label layers in the completed code below.\r\n\r\nAn important note is to filter your data by Record type: "UCSUSA active observation", as the n2yo.com data does not have the average distance data, because most of those records are for decayed or unknown status satellites.\r\n\r\n![Power BI: Apply filter to the Deneb visual](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-08-27--18.png?raw=true)\r\n\r\n\r\n## FINAL DENEB CODE\r\n\r\nThis is the final Deneb code for the custom code:\r\n\r\n```\r\n{"background": "null",\r\n "view": {"fill": "#0B0D14", \r\n          "fillOpacity": 0 },\r\n "data": {"name": "dataset"},\r\n "layer": [\r\n     { "transform": [\r\n            { "aggregate": [{"op": "max", "field": "Sin", "as": "max_sin"}] },\r\n            { "calculate": "datum.max_sin * 0", "as": "x-axis zero" },\r\n            { "calculate": "abs(datum.max_sin) * 0.5", "as": "y-axis top year" } ],\r\n       "mark": {"type": "text",\r\n                "style": "label",\r\n                "angle": 0,\r\n                "baseline": "top",\r\n                "align": "center",\r\n                "fontSize": 30 },\r\n       "encoding": { "x": {"field": "x-axis zero",\r\n                           "type":  "quantitative"},\r\n                     "y": {"field": "y-axis top year",\r\n                           "type":  "quantitative"},\r\n                     "text": {"value": "1974"},\r\n                     "color": {"value": "#9DA4AC"} }\r\n     },\r\n     { "transform": [\r\n            { "aggregate": [{"op": "max", "field": "Sin", "as": "max_sin"}] },\r\n            { "calculate": "datum.max_sin * 0", "as": "x-axis zero" },\r\n            { "calculate": "abs(datum.max_sin) * -1 * 0.6", "as": "y-axis bottom year" } ],\r\n       "mark": {"type": "text",\r\n                "style": "label",\r\n                "angle": 0,\r\n                "baseline": "bottom",\r\n                "align": "center",\r\n                "fontSize": 30 },\r\n       "encoding": { "x": {"field": "x-axis zero",\r\n                           "type":  "quantitative"},\r\n                     "y": {"field": "y-axis bottom year",\r\n                           "type":  "quantitative"},\r\n                     "text": {"value": "2006"},\r\n                     "color": {"value": "#9DA4AC"} }\r\n     },\r\n     { "transform": [\r\n            { "aggregate": [{"op": "max", "field": "Sin", "as": "max_sin"}] },\r\n            { "calculate": "datum.max_sin * 0", "as": "y-axis zero" },\r\n            { "calculate": "abs(datum.max_sin) * 0.5", "as": "x-axis right year" } ],\r\n       "mark": {"type": "text",\r\n                "style": "label",\r\n                "angle": 0,\r\n                "baseline": "middle",\r\n                "align": "center",\r\n                "fontSize": 30 },\r\n       "encoding": { "y": {"field": "y-axis zero",\r\n                           "type":  "quantitative"},\r\n                     "x": {"field": "x-axis right year",\r\n                           "type":  "quantitative"},\r\n                     "text": {"value": "1990"},\r\n                     "color": {"value": "#9DA4AC"} }\r\n     },\r\n     { "transform": [\r\n            { "aggregate": [{"op": "max", "field": "Sin", "as": "max_sin"}] },\r\n            { "calculate": "datum.max_sin * 0", "as": "y-axis zero" },\r\n            { "calculate": "abs(datum.max_sin) * -1 * 0.5", "as": "x-axis left year" } ],\r\n       "mark": {"type": "text",\r\n                "style": "label",\r\n                "angle": 0,\r\n                "baseline": "middle",\r\n                "align": "center",\r\n                "fontSize": 30 },\r\n       "encoding": { "y": {"field": "y-axis zero",\r\n                           "type":  "quantitative"},\r\n                     "x": {"field": "x-axis left year",\r\n                           "type":  "quantitative"},\r\n                     "text": {"value": "2021"},\r\n                     "color": {"value": "#9DA4AC"} }\r\n     },\r\n     { "transform": [\r\n         { "timeUnit": "year", "field": "Launch date", "as": "date"},\r\n         { "filter": "year(datum.date) == 1974"},\r\n         { "filter": {"field": "Orbit class", "equal": "Low Earth Orbit"}},\r\n         { "aggregate": [{"op": "min", "field": "Cos", "as": "min_sin_LEO"}] },\r\n         { "calculate": "datum.min_sin_LEO * 0", "as": "x-axis zero" },\r\n         { "calculate": "abs(round(datum.min_sin_LEO)) + \'km\'", "as": "display LEO max 1974 kms"}, \r\n         { "calculate": "abs(datum.min_sin_LEO)", "as": "y-axis LEO max 1974 distance" } ],\r\n       "mark": { "type": "text", "style": "label", "angle": 0, "baseline": "bottom", "align": "left", "fontSize": 16 },\r\n       "encoding": { "y": {"field": "y-axis LEO max 1974 distance", "type":  "quantitative"},\r\n                     "x": {"field": "x-axis zero", "type":  "quantitative"},\r\n                     "text": {"field": "display LEO max 1974 kms", "type": "nominal"},\r\n                     "color": {"value": "#AAB6C4"} }\r\n     },\r\n     { "transform": [\r\n         { "timeUnit": "year", "field": "Launch date", "as": "date"},\r\n         { "filter": "year(datum.date) == 1990"},\r\n         { "filter": {"field": "Orbit class", "equal": "Low Earth Orbit"}},\r\n         { "aggregate": [{"op": "min", "field": "Sin", "as": "min_sin_LEO"}] },\r\n         { "calculate": "datum.min_sin_LEO * 0", "as": "y-axis zero" },\r\n         { "calculate": "abs(round(datum.min_sin_LEO)) + \'km\'", "as": "display LEO max 1990 kms"}, \r\n         { "calculate": "abs(datum.min_sin_LEO)", "as": "x-axis LEO max 1990 distance" } ],\r\n       "mark": {"type": "text", "style": "label", "angle": 0, "baseline": "bottom", "align": "center", "fontSize": 16 },\r\n       "encoding": { "y": {"field": "y-axis zero", "type":  "quantitative"},\r\n                     "x": {"field": "x-axis LEO max 1990 distance", "type": "quantitative"},\r\n                     "text": {"field": "display LEO max 1990 kms", "type": "nominal"},\r\n                     "color": {"value": "#AAB6C4"} }\r\n     },\r\n     { "transform": [\r\n         { "timeUnit": "year", "field": "Launch date", "as": "date"},\r\n         { "filter": "year(datum.date) == 1990"},\r\n         { "filter": {"field": "Orbit class", "equal": "Geosynchronous Equatorial Orbit"}},\r\n         { "aggregate": [{"op": "min", "field": "Sin", "as": "min_sin_GEO"}] },\r\n         { "calculate": "datum.min_sin_GEO * 0", "as": "y-axis zero" },\r\n         { "calculate": "abs(round(datum.min_sin_GEO)) + \'km\'", "as": "display GEO max 1990 kms"}, \r\n         { "calculate": "abs(datum.min_sin_GEO)", "as": "x-axis GEO max 1990 distance" } ],\r\n       "mark": {"type": "text", "style": "label", "angle": 0, "baseline": "bottom", "align": "center", "fontSize": 16 },\r\n       "encoding": { "y": {"field": "y-axis zero", "type":  "quantitative"},\r\n                     "x": {"field": "x-axis GEO max 1990 distance", "type": "quantitative"},\r\n                     "text": {"field": "display GEO max 1990 kms", "type": "nominal"},\r\n                     "color": {"value": "#AAB6C4"} }\r\n     },\r\n     { "transform": [\r\n         { "timeUnit": "year", "field": "Launch date", "as": "date"},\r\n         { "filter": "year(datum.date) == 2006"},\r\n         { "filter": {"field": "Orbit class", "equal": "Low Earth Orbit"}},\r\n         { "aggregate": [{"op": "max", "field": "Cos", "as": "max_sin_LEO"}] },\r\n         { "calculate": "datum.max_sin_LEO * 0", "as": "x-axis zero" },\r\n         { "calculate": "abs(round(datum.max_sin_LEO)) + \'km\'", "as": "display LEO min 2006 kms"}, \r\n         { "calculate": "abs(datum.max_sin_LEO) * -1", "as": "y-axis LEO min 2006 distance" } ],\r\n       "mark": { "type": "text", "style": "label", "angle": 0, "baseline": "bottom", "align": "left", "fontSize": 16 },\r\n       "encoding": { "y": {"field": "y-axis LEO min 2006 distance", "type":  "quantitative"},\r\n                     "x": {"field": "x-axis zero", "type":  "quantitative"},\r\n                     "text": {"field": "display LEO min 2006 kms", "type": "nominal"},\r\n                     "color": {"value": "#AAB6C4"} }\r\n     },\r\n     { "transform": [\r\n         { "timeUnit": "year", "field": "Launch date", "as": "date"},\r\n         { "filter": "year(datum.date) == 2006"},\r\n         { "filter": {"field": "Orbit class", "equal": "Low Earth Orbit"}},\r\n         { "aggregate": [{"op": "median", "field": "Cos", "as": "med_sin_LEO"}] },\r\n         { "calculate": "datum.med_sin_LEO * 0", "as": "x-axis zero" },\r\n         { "calculate": "abs(round(datum.med_sin_LEO)) + \'km\'", "as": "display LEO med 2006 kms"}, \r\n         { "calculate": "abs(datum.med_sin_LEO) * -1", "as": "y-axis LEO med 2006 distance" } ],\r\n       "mark": { "type": "text", "style": "label", "angle": 0, "baseline": "bottom", "align": "left", "fontSize": 16 },\r\n       "encoding": { "y": {"field": "y-axis LEO med 2006 distance", "type":  "quantitative"},\r\n                     "x": {"field": "x-axis zero", "type":  "quantitative"},\r\n                     "text": {"field": "display LEO med 2006 kms", "type": "nominal"},\r\n                     "color": {"value": "#AAB6C4"} }\r\n     },\r\n     { "transform": [\r\n         { "timeUnit": "year", "field": "Launch date", "as": "date"},\r\n         { "filter": "year(datum.date) == 2006"},\r\n         { "filter": {"field": "Orbit class", "equal": "Low Earth Orbit"}},\r\n         { "aggregate": [{"op": "min", "field": "Cos", "as": "min_sin_LEO"}] },\r\n         { "calculate": "datum.min_sin_LEO * 0", "as": "x-axis zero" },\r\n         { "calculate": "abs(round(datum.min_sin_LEO)) + \'km\'", "as": "display LEO max 2006 kms"}, \r\n         { "calculate": "abs(datum.min_sin_LEO) * -1", "as": "y-axis LEO max 2006 distance" } ],\r\n       "mark": { "type": "text", "style": "label", "angle": 0, "baseline": "bottom", "align": "left", "fontSize": 16 },\r\n       "encoding": { "y": {"field": "y-axis LEO max 2006 distance", "type":  "quantitative"},\r\n                     "x": {"field": "x-axis zero", "type":  "quantitative"},\r\n                     "text": {"field": "display LEO max 2006 kms", "type": "nominal"},\r\n                     "color": {"value": "#AAB6C4"} }\r\n     },\r\n     { "transform": [\r\n         { "timeUnit": "year", "field": "Launch date", "as": "date"},\r\n         { "filter": "year(datum.date) == 2006"},\r\n         { "filter": {"field": "Orbit class", "equal": "Medium Earth Orbit"}},\r\n         { "aggregate": [{"op": "min", "field": "Cos", "as": "min_sin_MEO"}] },\r\n         { "calculate": "datum.min_sin_MEO * 0", "as": "x-axis zero" },\r\n         { "calculate": "abs(round(datum.min_sin_MEO)) + \'km\'", "as": "display MEO max 2006 kms"}, \r\n         { "calculate": "abs(datum.min_sin_MEO) * -1", "as": "y-axis MEO max 2006 distance" } ],\r\n       "mark": { "type": "text", "style": "label", "angle": 0, "baseline": "bottom", "align": "left", "fontSize": 16 },\r\n       "encoding": { "y": {"field": "y-axis MEO max 2006 distance", "type":  "quantitative"},\r\n                     "x": {"field": "x-axis zero", "type":  "quantitative"},\r\n                     "text": {"field": "display MEO max 2006 kms", "type": "nominal"},\r\n                     "color": {"value": "#AAB6C4"} }\r\n     },\r\n     { "transform": [\r\n         { "timeUnit": "year", "field": "Launch date", "as": "date"},\r\n         { "filter": "year(datum.date) == 2006"},\r\n         { "filter": {"field": "Orbit class", "equal": "Geosynchronous Equatorial Orbit"}},\r\n         { "aggregate": [{"op": "min", "field": "Cos", "as": "min_sin_GEO"}] },\r\n         { "calculate": "datum.min_sin_GEO * 0", "as": "x-axis zero" },\r\n         { "calculate": "abs(round(datum.min_sin_GEO)) + \'km\'", "as": "display GEO max 2006 kms"}, \r\n         { "calculate": "abs(datum.min_sin_GEO) * -1", "as": "y-axis GEO max 2006 distance" } ],\r\n       "mark": {"type": "text", "style": "label", "angle": 0, "baseline": "bottom", "align": "left", "fontSize": 16 },\r\n       "encoding": { "y": {"field": "y-axis GEO max 2006 distance", "type": "quantitative"},\r\n                     "x": {"field": "x-axis zero", "type":  "quantitative"},\r\n                     "text": {"field": "display GEO max 2006 kms", "type": "nominal"},\r\n                     "color": {"value": "#AAB6C4"} }\r\n     },\r\n     { "transform": [\r\n         { "timeUnit": "year", "field": "Launch date", "as": "date"},\r\n         { "filter": "year(datum.date) == 2021"},\r\n         { "filter": {"field": "Orbit class", "equal": "Low Earth Orbit"}},\r\n         { "aggregate": [{"op": "max", "field": "Sin", "as": "max_sin_LEO"}] },\r\n         { "calculate": "datum.max_sin_LEO * 0", "as": "y-axis zero" },\r\n         { "calculate": "abs(round(datum.max_sin_LEO)) + \'km\'", "as": "display LEO min 2021 kms"}, \r\n         { "calculate": "abs(datum.max_sin_LEO) * -1", "as": "x-axis LEO min 2021 distance" } ],\r\n       "mark": {"type": "text", "style": "label", "angle": 0, "baseline": "bottom", "align": "center", "fontSize": 16 },\r\n       "encoding": { "y": {"field": "y-axis zero", "type":  "quantitative"},\r\n                     "x": {"field": "x-axis LEO min 2021 distance", "type": "quantitative"},\r\n                     "text": {"field": "display LEO min 2021 kms", "type": "nominal"},\r\n                     "color": {"value": "#AAB6C4"} }\r\n     },\r\n     { "transform": [\r\n         { "timeUnit": "year", "field": "Launch date", "as": "date"},\r\n         { "filter": "year(datum.date) == 2021"},\r\n         { "filter": {"field": "Orbit class", "equal": "Low Earth Orbit"}},\r\n         { "aggregate": [{"op": "median", "field": "Sin", "as": "med_sin_LEO"}] },\r\n         { "calculate": "datum.med_sin_LEO * 0", "as": "y-axis zero" },\r\n         { "calculate": "abs(round(datum.med_sin_LEO)) + \'km\'", "as": "display LEO med 2021 kms"}, \r\n         { "calculate": "abs(datum.med_sin_LEO) * -1", "as": "x-axis LEO med 2021 distance" } ],\r\n       "mark": {"type": "text", "style": "label", "angle": 0, "baseline": "bottom", "align": "center", "fontSize": 16 },\r\n       "encoding": { "y": {"field": "y-axis zero", "type": "quantitative"},\r\n                     "x": {"field": "x-axis LEO med 2021 distance", "type": "quantitative"},\r\n                     "text": {"field": "display LEO med 2021 kms", "type": "nominal"},\r\n                     "color": {"value": "#AAB6C4"} }\r\n     },\r\n     { "transform": [\r\n         { "timeUnit": "year", "field": "Launch date", "as": "date"},\r\n         { "filter": "year(datum.date) == 2021"},\r\n         { "filter": {"field": "Orbit class", "equal": "Low Earth Orbit"}},\r\n         { "aggregate": [{"op": "min", "field": "Sin", "as": "min_sin_LEO"}] },\r\n         { "calculate": "datum.min_sin_LEO * 0", "as": "y-axis zero" },\r\n         { "calculate": "abs(round(datum.min_sin_LEO)) + \'km\'", "as": "display LEO max 2021 kms"}, \r\n         { "calculate": "abs(datum.min_sin_LEO) * -1", "as": "x-axis LEO max 2021 distance" } ],\r\n       "mark": {"type": "text", "style": "label", "angle": 0, "baseline": "bottom", "align": "center", "fontSize": 16 },\r\n       "encoding": { "y": {"field": "y-axis zero", "type": "quantitative"},\r\n                     "x": {"field": "x-axis LEO max 2021 distance", "type": "quantitative"},\r\n                     "text": {"field": "display LEO max 2021 kms", "type": "nominal"},\r\n                     "color": {"value": "#AAB6C4"} }\r\n     },\r\n     { "transform": [\r\n         { "timeUnit": "year", "field": "Launch date", "as": "date"},\r\n         { "filter": "year(datum.date) == 2021"},\r\n         { "filter": {"field": "Orbit class", "equal": "Medium Earth Orbit"}},\r\n         { "aggregate": [{"op": "min", "field": "Sin", "as": "min_sin_MEO"}] },\r\n         { "calculate": "datum.min_sin_MEO * 0", "as": "y-axis zero" },\r\n         { "calculate": "abs(round(datum.min_sin_MEO)) + \'km\'", "as": "display MEO max 2021 kms"}, \r\n         { "calculate": "abs(datum.min_sin_MEO) * -1", "as": "x-axis MEO max 2021 distance" } ],\r\n       "mark": {"type": "text", "style": "label", "angle": 0, "baseline": "bottom", "align": "center", "fontSize": 16 },\r\n       "encoding": { "y": {"field": "y-axis zero", "type":  "quantitative"},\r\n                     "x": {"field": "x-axis MEO max 2021 distance", "type": "quantitative"},\r\n                     "text": {"field": "display MEO max 2021 kms", "type": "nominal"},\r\n                     "color": {"value": "#AAB6C4"} }\r\n     },\r\n     { "transform": [\r\n         { "timeUnit": "year", "field": "Launch date", "as": "date"},\r\n         { "filter": "year(datum.date) == 2021"},\r\n         { "filter": {"field": "Orbit class", "equal": "Geosynchronous Equatorial Orbit"}},\r\n         { "aggregate": [{"op": "min", "field": "Sin", "as": "min_sin_GEO"}] },\r\n         { "calculate": "datum.min_sin_GEO * 0", "as": "y-axis zero" },\r\n         { "calculate": "abs(round(datum.min_sin_GEO)) + \'km\'", "as": "display GEO max 2021 kms"}, \r\n         { "calculate": "abs(datum.min_sin_GEO) * -1", "as": "x-axis GEO max 2021 distance" } ],\r\n       "mark": {"type": "text", "style": "label", "angle": 0, "baseline": "bottom", "align": "center", "fontSize": 16 },\r\n       "encoding": { "y": {"field": "y-axis zero", "type":  "quantitative"},\r\n                     "x": {"field": "x-axis GEO max 2021 distance", "type": "quantitative"},\r\n                     "text": {"field": "display GEO max 2021 kms", "type": "nominal"},\r\n                     "color": {"value": "#AAB6C4"} }\r\n     },\r\n     { "transform": [\r\n            { "aggregate": [{"op": "max", "field": "Sin", "as": "max_sin"}] },\r\n            { "calculate": "datum.max_sin * 0", "as": "x-axis zero for vertical rule" } ],\r\n       "mark": {"type": "rule",\r\n                "strokeDash": [1, 10],\r\n                "strokeDashOffset": 50000},\r\n       "encoding": { "x": { "field": "x-axis zero for vertical rule",\r\n                            "type": "quantitative"},\r\n                            "color": {"value": "#9DA4AC"},\r\n                            "size": {"value": 1} }\r\n     },\r\n     { "transform": [\r\n            { "aggregate": [{"op": "max", "field": "Sin", "as": "max_sin"}] },\r\n            { "calculate": "datum.max_sin * 0", "as": "y-axis zero for horizontal rule" } ],\r\n       "mark": {"type": "rule",\r\n                "strokeDash": [1, 10],\r\n                "strokeDashOffset": 50000},\r\n       "encoding": { "y": { "field": "y-axis zero for horizontal rule",\r\n                            "type": "quantitative"},\r\n                            "color": {"value": "#9DA4AC"},\r\n                            "size": {"value": 1} }\r\n     },\r\n     { "transform": [\r\n            { "aggregate": [{"op": "max", "field": "Sin", "as": "max_sin"}] },\r\n            { "calculate": "datum.max_sin * 0", "as": "origin zero" } ],\r\n       "mark": {"type": "image",\r\n                "width": 75,\r\n                "height": 75},\r\n       "encoding": { "x": { "aggregate": "median",\r\n                            "field": "origin zero",\r\n                            "type": "quantitative"},\r\n                     "y": { "aggregate": "median",\r\n                            "field": "origin zero",\r\n                            "type": "quantitative"},\r\n                     "url": { "datum": "data:image/png;base64,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",\r\n                              "type":  "nominal"}\r\n                   }\r\n     },\r\n     { "params": [{ "name": "grid",\r\n                    "select": "interval",\r\n                    "bind": "scales"},\r\n                  { "name": "legend",\r\n                    "select": {"type": "point", "fields": ["Orbit class"]},\r\n                    "bind": "legend"} ],\r\n       "mark": { "type": "point", \r\n                 "shape": "circle",\r\n                 "tooltip": true,\r\n                 "filled": true,\r\n                 "fillOpacity": 0.75},\r\n       "encoding": { "x": { "field": "Sin", \r\n                            "type": "quantitative",\r\n                            "title": false, \r\n                            "axis": {"disable": true,\r\n                                     "grid": false,\r\n                                     "labels": false},\r\n                                     "scale": {"domain": [-70400,70050]}},\r\n                      "y": { "field": "Cos", \r\n                             "type": "quantitative",\r\n                             "title": false, \r\n                             "axis": {"disable": true,\r\n                                      "grid": false,\r\n                                      "labels": false},\r\n                                      "scale": {"domain": [-69540,60370]}},\r\n                      "size": {"value": 50},\r\n                      "color": { "field": "Orbit class",\r\n                                 "type":  "nominal",\r\n                                 "sort": [ "Geostationary Transfer Orbit", "Geosynchronous Equatorial Orbit", "Medium Earth Orbit", "Low Earth Orbit"],\r\n                                 "scale": {"range": ["#C8F6FF", "#8DECFF", "#2DDBFF", "#11BBDE" ]} },\r\n                      "opacity": { "condition": {"param": "legend", "value": 1},\r\n                                   "value": 0.025 },\r\n                      "tooltip": [ {"field": "Satellite catalog number (NORAD)", "type": "nominal", "title": "NORAD"},\r\n                                   {"field": "International designator (COSPAR)", "title": "COSPAR"},\r\n                                   {"field": "Name", "title": "SATELLITE"},\r\n                                   {"field": "Country", "title": "COUNTRY"},\r\n                                   {"field": "Purpose", "title": "PURPOSE"},\r\n                                   {"field": "Users", "title": "USERS"},\r\n                                   {"field": "Launch date", "title": "LAUNCH DATE", "type": "temporal", "format": "%d-%m-%Y (DMY)"},\r\n                                   {"field": "Status", "title": "STATUS"},\r\n                                   {"field": "Orbit class", "title": "ORBIT CLASS"},\r\n                                   {"field": "Distance (km)", "title": "AVERAGE DISTANCE (km)", "type": "nominal", "format": ",.0f"},\r\n                                   {"field": "Apogee (km)", "title": "APOGEE (km)", "type": "nominal", "format": ",.0f"},\r\n                                   {"field": "Perigee (km)", "title": "PERIGEE (km)", "type": "nominal", "format": ",.0f"},\r\n                                   {"field": "Launch site", "title": "LAUNCH SITE"},\r\n                                   {"field": "Launch vehicle", "title": "LAUNCH VEHICLE"}] }\r\n     }\r\n    ],\r\n    "config": {\r\n      "legend": {"title": "null",\r\n                 "titleColor": "#ffffff",\r\n                 "labelColor": "#ffffff",\r\n                 "labelFontSize": 10,\r\n                 "labelFont": "Consolas",\r\n                 "orient": "top-left",\r\n                 "labelLimit": 260  }\r\n    }\r\n}\r\n```\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2022-08-27.md)** for this post\'s markdown file in GitHub.'},{id:1659189600,title:"How to webscrape recent Twitter posts by keyword using Python and Visual Studio Code",tag:"logo-python",date:"31 July 2022",content:"\r\nThis post shows how to webscrape recent Tweets based on a predefined keyword or hashtag, and other related data using the Twitter API, Visual Studio Code, and the Python-based Tweepy library.\r\n\r\n## Step 1. Create a free Twitter Developer account\r\n\r\nAs at the time of writing, you can get a Twitter Developer account off the back of an existing Twitter account or create a new one, but you will need to provide your mobile phone number for verification.\r\n\r\n* **[https://developer.twitter.com/](https://developer.twitter.com/)**\r\n\r\nFrom the Twitter Developer Portal, you can create your project and app, and then additionally request for Elevated access, which provides higher level Twitter API accesses and limits. Be mindful that your application may incur costs depending on the features you request, and in my case, I chose free Read-Only access.\r\n\r\n![Developer Portal: Project](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-07-31--01.png?raw=true)\r\n\r\nGenerate the API Key, Access Tokens, and Secrets that will be needed for your API connection, being careful that data is securely stored.\r\n\r\n![Developer Portal: Keys and tokens](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-07-31--02.png?raw=true)\r\n\r\n\r\n## Step 2. From Visual Studio Code install Python and set up your virtual environment\r\n\r\nInstall Python\r\n* **[https://www.python.org/downloads/windows/](https://www.python.org/downloads/windows/)**\r\n\r\nCreate a folder for your Python project and open it in Visual Studio Code. In this example, the folder name is *twitter-web-scraping*.\r\n\r\nInstall the Python Extension for Visual Studio Code.\r\n![Visual Studio Code](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--01.png?raw=true)\r\n\r\nCreate your virtual environment as a subfolder of your project.\r\n```\r\npython -m venv C:\\Project\\twitter-web-scraping\\venv\r\n```\r\n\r\nYou can find more detailed screenshot steps of the above from a previous blog post:\r\n* **[Add forecasts from Python using Visual Studio Code to Power BI](https://datamesse.github.io/#/post/1650117600)**\r\n\r\n## Step 3. Install relevant Python libraries\r\n\r\n```\r\npip install tweepy\r\npip install configparser\r\npip install pandas\r\n```\r\n\r\n## Step 4. Create a configuration file to store your Twittter API keys, tokens, and secrets\r\n\r\nCreate a config.ini file in your project folder, and add your API keys, tokens, and secrets in place of the ???? below.\r\n\r\n**config.ini**\r\n```\r\n[twitter]\r\n\r\napi_key = ????\r\napi_key_secret = ????\r\n\r\naccess_token = ????\r\naccess_token_secret = ????\r\n```\r\n\r\nBe very careful not to publish these details publically, such as via Github.\r\n\r\n![Visual Studio Code: config.ini](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-07-31--03.png?raw=true)\r\n\r\n\r\n## Step 5. Create a Python script to run the Tweet extraction\r\n\r\nThis Python script uses the keys, tokens, and secrets by referencing the config.ini from the previous step.\r\n\r\nIn the example below, the keyword search being used is \"Motorola\". Simply substitute that value for your preferred text string, or hashtag.\r\n\r\nAt the time of my testing, I can only retrieve a maximum of about 900 records every 15 minutes using my elevated API, and have coded that as part of the script's limit so it doesn't error out when it is executed.\r\n\r\n**Note:** tweet.text appears to be a truncated variation of the Tweet data, whereas tweet.full_text scrapes the full Tweet.\r\n\r\nBecause the data is saved to a CSV file, new lines and commas are replaced with space characters to keep the data to single row records.\r\n\r\n**python_script.py**\r\n```\r\nimport tweepy\r\nimport configparser\r\nimport pandas as pd\r\n\r\nconfig = configparser.ConfigParser()\r\nconfig.read('config.ini')\r\napi_key = config['twitter']['api_key']\r\napi_key_secret = config['twitter']['api_key_secret']\r\naccess_token = config['twitter']['access_token']\r\naccess_token_secret = config['twitter']['access_token_secret']\r\n\r\nauth = tweepy.OAuthHandler(api_key,api_key_secret)\r\nauth.set_access_token(access_token, access_token_secret)\r\napi = tweepy.API(auth)\r\n\r\nkeywords = ['Motorola']\r\n\r\nlimit = 901\r\ntweets = tweepy.Cursor(api.search_tweets, q=keywords, lang='en', result_type='recent', tweet_mode=\"extended\").items(limit)\r\n\r\ncolumns = ['TweetDateTime', \r\n           'UserID',\r\n           'UserName',\r\n           'UserScreenName',\r\n           'UserDescription',\r\n           'UserCreatedDateTime',\r\n           'UserVerified',\r\n           'UserLocation',\r\n           'UserFollowers',\r\n           'TweetContent',\r\n           'TweetFavourites',\r\n           'TweetRetweets']\r\ndata = []\r\nfor tweet in tweets:\r\n    data.append([tweet.created_at,\r\n                 tweet.user.id, \r\n                 tweet.user.name.replace('\\n', ' ').replace(',',' '),\r\n                 tweet.user.screen_name.replace('\\n', ' ').replace(',',' '),\r\n                 tweet.user.description.replace('\\n', ' ').replace(',',' '),\r\n                 tweet.user.created_at,\r\n                 tweet.user.verified,\r\n                 tweet.user.location.replace('\\n', ' ').replace(',',' '),\r\n                 tweet.user.followers_count,\r\n                 tweet.full_text.replace('\\n', ' ').replace(',',' '),\r\n                 tweet.favorite_count,\r\n                 tweet.retweet_count])\r\ndf = pd.DataFrame(data, columns=columns)\r\ndf.to_csv('tweets.csv')\r\n```\r\n\r\n![Visual Studio Code: python_script.py](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-07-31--04.png?raw=true)\r\n\r\n\r\n### References\r\n\r\n* YouTube video **[\"How to get TWEETS by Python | Twitter API 2022\" by AI Spectrum](https://www.youtube.com/watch?v=Lu1nskBkPJU)**\r\n\r\n* YouTube video **[\"Get TWEETS by User and Hashtag with Python | Twitter API 2022\" by AI Spectrum](https://www.youtube.com/watch?v=FmbEhKSpR7M)**\r\n\r\n* YouTube video **[\"how to automate search of trending hashtag and number of tweets from those hashtags\" by iknowpython](https://www.youtube.com/watch?v=ywl--vO3oGs)**\r\n\r\n* Twitter API v1 Search Tweets Reference **[https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/api-reference/get-search-tweets](https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/api-reference/get-search-tweets)**\r\n\r\n* Twitter API v2 data dictionary **[https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet](https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet)**\r\n\r\n* Tweepy library Exteded Tweets **[https://docs.tweepy.org/en/stable/extended_tweets.html](https://docs.tweepy.org/en/stable/extended_tweets.html)**\r\n\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2022-07-31.md)** for this post's markdown file in GitHub."},{id:165582e4,title:"How to embed an Excel workbook into a React.js application",tag:"logo-react",date:"22 June 2022",content:'\r\nIn this scenario, we are publishing an Excel workbook that we want the general public to view and interact with, but not allow them to download a copy of. We will walk through creating your own free OneDrive account to host the Excel file, extract and modify the JavaScript embed code, then apply it to our React.js component.\r\n\r\n**Note:** There are limitaions on what the free version of OneDrive allows you to publish in your Excel workbook. In my case, I found that External Data Connections (e.g. using Power Query to Get Data from a GitHub repository), and using "Camera Tools" to create a dynamic link to inserted workbook images, are not supported. So bear this in mind when building Excel dashboards for your portfolio.\r\n\r\n## Step 1. Ensure your Excel workbook has a cell range defined for what the users can view.\r\n\r\n![Excel cell range named](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-06-22--00.png?raw=true)\r\n\r\n## Step 2.  Create (or log into) your free **[OneDrive.com](https://onedrive.com)** account.\r\n\r\nIf creating a new account, click "See plans and pricing" then "Sign up for free".\r\n\r\n![Microsoft 365 OneDrive Personal Cloud Storage](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-06-22--01.png?raw=true)\r\n\r\n![OneDrive Basic 5GB Free](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-06-22--02.png?raw=true)\r\n\r\n\r\n## Step 3. Add your Excel file to your OneDrive.\r\n\r\n## Step 4. Extract the JavaScript embed code.\r\n\r\nRight-click the Excel file and click Embed.\r\n\r\n![OneDrive right-click for Embed code 1](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-06-22--03.png?raw=true)\r\n\r\nClick "Customise how this embedded workbook will appear to others".\r\n\r\n![OneDrive right-click for Embed code 2](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-06-22--04.png?raw=true)\r\n\r\nConfigure your settings as required for the Excel workbook, and this will autopopulate the JavaScript code.\r\n\r\nCopy the contents of the JavaScript code.\r\n\r\n![Extract OneDrive JavaScript Embed code](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-06-22--05.png?raw=true)\r\n\r\n## Step 5. Modify the JavaScript embed code and add to React.js component\r\n\r\nThe code you receive, discounting the comments, appears as below and is structured for basic HTML pages, i.e. on its own you can\'t just copy and paste it into a React.js component.\r\n\r\nExample JavaScript embed code from OneDrive:\r\n\r\n```\r\n<div id="myExcelDiv" style="width: 700px; height: 900px"></div>\r\n<script type="text/javascript" src="https://onedrive.live.com/embed?resid=ABC12345ABC123A%12345&authkey=%12AB123abc1AbcDEF1&em=3&wdItem=%22\'Dashboard\'!A%1ABCD%22&wdDivId=%22myExcelDiv%22&wdActiveCell=%22\'Dashboard\'!A1%22&wdAllowInteractivity=0&wdAllowTyping=1"><\/script>\r\n```\r\n\r\nWhat we will keep from the code above is the div tag and its id\r\n\r\n```\r\n<div id="myExcelDiv"></div>\r\n```\r\n\r\nand the src code from the script tag to view the file in an iframe.\r\n\r\n```\r\n"https://onedrive.live.com/embed?resid=ABC12345ABC123A%12345&authkey=%12AB123abc1AbcDEF1&em=3&wdItem=%22\'Dashboard\'!A%1ABCD%22&wdDivId=%22myExcelDiv%22&wdActiveCell=%22\'Dashboard\'!A1%22&wdAllowInteractivity=0&wdAllowTyping=1"\r\n```\r\n\r\nWe can then append the following code to the src code\r\n\r\n```\r\n&action=embedview&wdbipreview=true\r\n```\r\n\r\nSo it now appears as this.\r\n\r\n```\r\n"https://onedrive.live.com/embed?resid=ABC12345ABC123A%12345&authkey=%12AB123abc1AbcDEF1&em=3&wdItem=%22\'Dashboard\'!A%1ABCD%22&wdDivId=%22myExcelDiv%22&wdActiveCell=%22\'Dashboard\'!A1%22&wdAllowInteractivity=0&wdAllowTyping=1&action=embedview&wdbipreview=true"\r\n```\r\n\r\nFrom your React.js component, add the src code into your function before the return clause.\r\n\r\n```\r\nconst script = document.createElement("script");\r\n\r\nscript.src = "https://onedrive.live.com/embed?resid=ABC12345ABC123A%12345&authkey=%12AB123abc1AbcDEF1&em=3&wdItem=%22\'Dashboard\'!A%1ABCD%22&wdDivId=%22myExcelDiv%22&wdActiveCell=%22\'Dashboard\'!A1%22&wdAllowInteractivity=0&wdAllowTyping=1&action=embedview&wdbipreview=true";\r\n\r\nscript.async = true;\r\n\r\ndocument.body.appendChild(script);\r\n```\r\n\r\nAdd the myExcelDiv div inside the return clause.\r\n\r\n```\r\n<div id="myExcelDiv" className=\'embeddedExcel\'></div>\r\n```\r\n\r\nThen use CSS to style your myExcelDiv. In this example, we have tried to make this full width, minus 20 pixels for the scrollbar\'s width.\r\n\r\n```\r\n.embeddedExcel {\r\n    width: (100vw) - 20px;\r\n    height: 950px;\r\n}\r\n```\r\n\r\nSo the completed code for the React.js component would appear similar to below:\r\n\r\n```\r\nimport React from \'react\';\r\nimport \'../App.css\';\r\n\r\nexport default function ExcelCustomerSupportAgentPerformance() {\r\n\r\n    const script = document.createElement("script");\r\n\r\n    script.src = "https://onedrive.live.com/embed?resid=ABC12345ABC123A%12345&authkey=%12AB123abc1AbcDEF1&em=3&wdItem=%22\'Dashboard\'!A%1ABCD%22&wdDivId=%22myExcelDiv%22&wdActiveCell=%22\'Dashboard\'!A1%22&wdAllowInteractivity=0&wdAllowTyping=1&action=embedview&wdbipreview=true";\r\n    script.async = true;\r\n\r\n    document.body.appendChild(script);\r\n\r\n    return (\r\n        <>\r\n            <div id="myExcelDiv" className=\'embeddedExcel\'></div>\r\n\r\n        </>\r\n    )\r\n\r\n}\r\n```\r\n\r\n![React.js code](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-06-22--06.png?raw=true)\r\n\r\nReferences\r\n\r\n* Stack Overflow Question **[Adding a script tag to React.js](https://stackoverflow.com/questions/34424845/adding-script-tag-to-react-jsx)**\r\n\r\n* YouTube video **[STOP Emailing Excel Files - Securely Embed them in Web Pages Instead!](https://www.youtube.com/watch?v=uvA-U9FKgPw&t=385s)**\r\n\r\n\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2022-06-22.md)** for this post\'s markdown file in GitHub.'},{id:1650981600,title:"Creating custom visuals using Deneb and Vega-lite in Power BI",tag:"logo-powerbi",date:"27 April 2022",content:'\r\nThis is my first attempt at using Deneb to create two custom visuals in Power BI, based on 2 visual design elements from existing Tableau Public reports. The dataset used is from my International Marketplace SSIS package project, found in my previous blog post: **[https://datamesse.github.io/#/post/1641906000](https://datamesse.github.io/#/post/1641906000)**\r\n\r\nDeneb is a highly flexible custom visual developed by **[Daniel Marsh-Patrick](https://twitter.com/the_d_mp)** that can be used to create unique visualisations for Power BI reports with JSON-based syntaxes; Vega or Vega-lite.\r\n\r\nDeneb can be downloaded from the Microsoft App Store:\r\n\r\n* **[https://appsource.microsoft.com/en-US/product/power-bi-visuals/coacervolimited1596856650797.deneb?tab=Overview](https://appsource.microsoft.com/en-US/product/power-bi-visuals/coacervolimited1596856650797.deneb?tab=Overview)**\r\n\r\nThe official website for Deneb:\r\n\r\n* **[https://deneb-viz.github.io/](https://deneb-viz.github.io/)**\r\n\r\n\r\nThe Power BI report I used to incorporate these visuals:\r\n* **[https://community.powerbi.com/t5/Data-Stories-Gallery/International-Marketplace-profit-report-using-Python-and-Deneb/m-p/2480550#M7154](https://community.powerbi.com/t5/Data-Stories-Gallery/International-Marketplace-profit-report-using-Python-and-Deneb/m-p/2480550#M7154)**\r\n\r\n\r\n### Deneb visual 1: Time series circle chart with bar chart totals\r\n\r\nThe first custom visual was a design element mimicked from a Tableau Public community gallery visualisation by [Pradeep Kumar G](https://twitter.com/pradeep_zen) named **[Sample Superstore - Sales Performance | VOTD](https://public.tableau.com/app/profile/pradeepkumar.g/viz/SampleSuperstore-SalesPerformance/viz_)**.\r\n\r\n**[https://public.tableau.com/app/profile/pradeepkumar.g/viz/SampleSuperstore-SalesPerformance/viz_](https://public.tableau.com/app/profile/pradeepkumar.g/viz/SampleSuperstore-SalesPerformance/viz_)**\r\n\r\nThe original dashboard appears as below:\r\n\r\n![Tableau sales dashboard by Pradeep Kumar](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-27--01.png?raw=true)\r\n\r\nIf you scroll further down, you will find the design element I attempted to replicate using Deneb:\r\n\r\n![Tableau sales dashboard element by Pradeep Kumar](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-27--02.png?raw=true)\r\n\r\nPower BI native visuals do not have the ability to recreate the circle chart with gradient colours (that I can find as yet), so Deneb is useful here to create that component. \r\n\r\nOpening this dashboard in Tableau Desktop, the composition of the circle chart and the bar chart are simply two separate visuals placed side-by-side.\r\n\r\n![Tableau sales dashboard element by Pradeep Kumar in Tableau Desktop](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-27--03.png?raw=true)\r\n\r\nAnother major advantage of Deneb is that you can create multiple visuals as a single chart, so rather than having to create a separate bar chart, it can be created as an additional "view" alongside the circle chart.\r\n\r\nDeneb has a few ways to **[concatenate views](https://vega.github.io/vega-lite/docs/concat.html)**, which involves how you want to set the visual elements bedside each other. In this case I used hconcat for horizontal layout of the circle chart, the bar chart, and a third chart to act as a numerical marker for the bar chart.\r\n\r\n![Deneb hconcat Vega-lite outline](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-27--04.png?raw=true)\r\n\r\nVega-lite code structure:\r\n\r\n```\r\n "hconcat": [\r\n     {"mark": { "type": "circle"}\r\n       ...\r\n     },\r\n     {"mark": { "type": "bar"}\r\n       ...\r\n     },\r\n     {"mark": { "type": "text"}\r\n       ...\r\n     },\r\n\r\n```\r\n\r\nThe main challenge I had with the circle chart is that I have data for years 2022 and 2023, but I wasn\'t able to find a way to display these along the x-axis as separate levels of labels (i.e. months on the top row and year in the second row).\r\n\r\nIn hindsight, I could have created a custom column that concatenated year and month number, but during development I found severe slowness of the Power BI report as-is (likely due to the Python predictive analytics integration), so I opted to just display 2023 by filtering the visual (though this can also be done from within the Deneb code), and display the initial for each month.\r\n\r\n![Deneb initials for months](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-27--05.png?raw=true)\r\n\r\nVega-lite code structure:\r\n\r\n```\r\n"encoding": {\r\n              ...\r\n             "x": {"timeUnit": "month",\r\n                   "field": "OrderDate",\r\n                   "axis": { "labelAlign": "center",\r\n                             "labelExpr": "datum.label[0]" }\r\n```\r\n\r\nI haven\'t replicated the original for every detail, but have the general outlay in Deneb, and the rest would be minor finessing.\r\n\r\nThis is the finalised code:\r\n\r\n```\r\n{\r\n  "data": {"name": "dataset"},\r\n  "bounds": "flush",\r\n  "spacing": 6,\r\n  "params": [{"name": "pts",\r\n              "select": "interval"\r\n            }],\r\n  "hconcat": [\r\n     {"mark": { "type": "circle",\r\n                "size": 50,\r\n                "tooltip": true\r\n     },\r\n      "width": 150,\r\n      "height": 530,\r\n      "encoding": {\r\n                   "y": {"field": "Subcategory",\r\n                         "type": "nominal",\r\n                         "title": null,\r\n                         "axis": {"labelFontSize": 9, "grid": true, "tickBand": "extent", "gridColor": "#f6f6f6", "gridWidth": 1},\r\n                         "sort": {"op": "sum", "field": "Total quantity", "order": "descending"}\r\n                        },\r\n                   "x": {"timeUnit": "month",\r\n                         "field": "OrderDate",\r\n                         "title": null,\r\n                         "axis": { "labelAlign": "center",\r\n                                   "labelExpr": "datum.label[0]" }\r\n                        },\r\n                   "color": {\r\n                             "aggregate": "sum",\r\n                             "field": "Total quantity",\r\n                             "scale":  {\r\n                               "domainMin": 0, "domainMax": 100000,\r\n                               "range": ["#cbf2f2", "#82e0e0", "#90dfdf", "#32c9c9", "#19abab", "#048e8e"] }\r\n                            },\r\n                   "tooltip": [{"timeUnit": "year", "field": "OrderDate", "title": "YEAR"},\r\n                               {"timeUnit": "month", "field": "OrderDate", "title": "MONTH"},\r\n                               {"field": "Category", "type": "nominal", "title": "CATEGORY"},\r\n                               {"field": "Subcategory", "type": "nominal", "title": "SUBCATEGORY"},\r\n                               {"aggregate": "sum", "field": "Total quantity", "title": "QUANTITY"}]\r\n                  }\r\n     },\r\n     {"mark": { "type": "bar", "tooltip": true},\r\n      "width": 100,\r\n      "height": 530,\r\n      "encoding": {\r\n                   "y": {"field": "Subcategory",\r\n                         "type": "nominal",\r\n                         "title": null,\r\n                         "sort": {"op": "sum", "field": "Total quantity", "order": "descending"},\r\n                         "axis": {"labelFontSize": 9, "labels": false, "grid": true, "gridColor": "#f6f6f6", "gridWidth": 1}\r\n                        },\r\n                   "x": {"field": "Total quantity",\r\n                         "type": "quantitative",\r\n                         "aggregate": "sum",\r\n                         "title": null,\r\n                         "axis": {"labels": false,\r\n                                  "gridColor": "white"\r\n                         }\r\n                        },\r\n                   "color": {"aggregate": "sum",\r\n                             "field": "Total quantity",\r\n                             "scale":  {"range": ["#cbf2f2", "#82e0e0", "#90dfdf", "#32c9c9", "#19abab", "#048e8e"] }\r\n                            },\r\n                   "tooltip": [{"timeUnit": "year", "field": "OrderDate", "title": "YEAR"},\r\n                               {"field": "Category", "type": "nominal", "title": "CATEGORY"},\r\n                               {"field": "Subcategory", "type": "nominal", "title": "SUBCATEGORY"},\r\n                               {"aggregate": "sum", "field": "Total quantity", "title": "QUANTITY"}]\r\n                   }\r\n     },\r\n     {"mark": {"type": "text", "align": "left", "x": 0, "fontSize": 9},\r\n      "width": 50,\r\n      "height": 530,\r\n      "encoding": {"text": {"aggregate": "sum",\r\n                            "field": "Total quantity"\r\n                           },\r\n                   "y": {"field": "Subcategory",\r\n                         "type": "nominal",\r\n                         "title": null,\r\n                         "sort": {"op": "sum", "field": "Total quantity", "order": "descending"},\r\n                         "axis": {"labels": false, "labelFontSize": 9}\r\n                        }\r\n                  }\r\n     }\r\n  ],\r\n  "config": {"legend": {"disable": true} }\r\n}\r\n```\r\n\r\n\r\n\r\n### Deneb visual 2: Zoomable scatterplot\r\n\r\nThe second custom visual is yet another design element mimicked from a Tableau Public visualisation by [Pradeep Kumar G](https://twitter.com/pradeep_zen), this time named **[Profit Analysis - Sample Superstore](https://public.tableau.com/app/profile/pradeepkumar.g/viz/ProfitAnalysis-SampleSuperstore/viz)**.\r\n\r\n* **[https://public.tableau.com/app/profile/pradeepkumar.g/viz/ProfitAnalysis-SampleSuperstore/viz](https://public.tableau.com/app/profile/pradeepkumar.g/viz/ProfitAnalysis-SampleSuperstore/viz)**\r\n\r\nThe original dashboard appears as below:\r\n\r\n![Tableau profit dashboard by Pradeep Kumar](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-27--06.png?raw=true)\r\n\r\nOnce again, you need to scroll down a bit further to find the design element I tried to replicate with Deneb:\r\n\r\n![Tableau profit dashboard element by Pradeep Kumar](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-27--07.png?raw=true)\r\n\r\nThis was much easier to create, as I simply modified an existing official Vega-lite scatterplot example found here:\r\n\r\n* **[https://vega.github.io/vega-lite/examples/selection_translate_scatterplot_drag.html](https://vega.github.io/vega-lite/examples/selection_translate_scatterplot_drag.html)**\r\n\r\nThe main advantage of using Deneb here is the scatterplot has the ability to easily zoom in and out using the mouse scrollwheel, and pan using mouse-click-hold-and-drag action, whereas Power BI out of-the-box (as of at the time of writing), requires a lot of clicking around for the user to use the zoom slice for each axis.\r\n\r\n![Power BI zoom slicer on scaterplot](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-27--08.png?raw=true)\r\n\r\nThe original design element uses a hexagon with a centred dot for each point, and as Kerry Kolosko\'s blog shows, it is possible to place unique shapes and pictures as marks using Deneb:\r\n\r\n* **[https://kerrykolosko.com/images-and-custom-shapes-in-deneb/](https://kerrykolosko.com/images-and-custom-shapes-in-deneb/)**\r\n\r\nThough there may be a limitation to the level of detail in creating these marks as SVG paths. I created the hexagon using https://vecta.io/, but was unable to add the centre dot using Vega-lite, either because multi-path support doesn\'t exist as yet for marks, or I have simply struggled to figure out how to make it work.\r\n\r\n![Hexagon shape created using vecta.io](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-27--09.png?raw=true)\r\n\r\nVega-lite code structure:\r\n\r\n```\r\n  ...\r\n    "mark": {\r\n             "type": "point", \r\n             "shape": "M-3.671-1.9184L-.5855-3.875l3.0856 1.9566v2.837L-.5855 2.875-3.671.9186z"\r\n```\r\n\r\nThe main challenge of this visual was to add profit margin on the x-axis, and as a value in general, per customer. The reason for this is because I originally had a DAX measure that involved variables to help perform the calculation for profit margin, but it seemed to produce incorrect results when added to the Deneb visual. \r\n\r\nSo instead, I added both profit and sales measures to the Deneb visual (which were independently correct), then within the visual performed the profit margin calculation per customer.\r\n\r\nVega-lite code structure:\r\n\r\n```\r\n  ...\r\n      "transform": [\r\n        {\r\n          "aggregate": [\r\n            {"op": "sum", "field": "Total profit", "as": "sum_profit"},\r\n            {"op": "sum", "field": "Total sales", "as": "sum_sales"} ],\r\n            "groupby": ["CustomerID"]\r\n        },\r\n        {\r\n          "calculate": "datum.sum_profit", "as": "Profit"\r\n        },\r\n        {\r\n          "calculate": "datum.sum_sales", "as": "Sales"\r\n        },\r\n        {\r\n          "calculate": "datum.sum_profit/datum.sum_sales * 100", "as": "Profit margin"\r\n        },\r\n```\r\n\r\nThen I added condition colouring for the points, similar to the Tableau design reference, whereby any values with a profit margin less than 0% would be turned red.\r\n\r\n```\r\n  ...\r\n  "color": {\r\n    "condition": {\r\n      "test": "datum[\'Profit\'] < 0 || datum[\'Profit margin\'] < 0",\r\n      "value": "#ff0000"}},\r\n```\r\n\r\nThe last bells-and-whistles to add were to include an overlay of median values of the x-axis for profit margin, and the y-axis for profit, across all customers. Unlike the concatenation method used previously, since thi is just overlaying "rules" i.e. lines for the medians, views are simply added within the same layer, as opposed to beside each other.\r\n\r\n```\r\n{\r\n  ...\r\n  "layer": [\r\n    {\r\n      ...\r\n      "mark": {\r\n        "type": "point", \r\n        ...\r\n    },\r\n    {\r\n      ...\r\n      "mark": {\r\n        "type": "rule",\r\n        "strokeDash": [7, 6]},\r\n      "encoding": {\r\n        "x": {\r\n          "aggregate": "median",\r\n          "field": "Profit margin",\r\n           ...\r\n      }\r\n    },\r\n    {\r\n      ...\r\n      "mark": {\r\n        "type": "rule",\r\n        "strokeDash": [7, 6]},\r\n      "encoding": {\r\n        "y": {\r\n          "aggregate": "median",\r\n          "field": "Profit",\r\n          ...\r\n      }\r\n    }\r\n  ]\r\n```\r\n\r\nThe final product looks like this:\r\n\r\n![Deneb scatterplot in Power BI](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-27--10.png?raw=true)\r\n\r\nVega-lite code structure:\r\n\r\n```\r\n{\r\n  "data": {"name": "dataset"},\r\n  "layer": [\r\n    {\r\n      "params": [{\r\n        "name": "grid",\r\n        "select": "interval",\r\n        "bind": "scales"}],\r\n      "transform": [\r\n        {\r\n          "aggregate": [\r\n            {"op": "sum", "field": "Total profit", "as": "sum_profit"},\r\n            {"op": "sum", "field": "Total sales", "as": "sum_sales"} ],\r\n            "groupby": ["CustomerID"]\r\n        },\r\n        {\r\n          "calculate": "datum.sum_profit", "as": "Profit"\r\n        },\r\n        {\r\n          "calculate": "datum.sum_sales", "as": "Sales"\r\n        },\r\n        {\r\n          "calculate": "datum.sum_profit/datum.sum_sales * 100", "as": "Profit margin"\r\n        },\r\n        {\r\n          "calculate": "(datum.sum_profit/datum.sum_sales) * (datum.sum_profit/datum.sum_sales)", "as": "Point size"\r\n        }],\r\n      "mark": {\r\n        "type": "point", \r\n        "shape": "M-3.671-1.9184L-.5855-3.875l3.0856 1.9566v2.837L-.5855 2.875-3.671.9186z",\r\n        "tooltip": true,\r\n        "color":"#33cbcb",\r\n        "filled": false},\r\n      "encoding": {\r\n        "x": {\r\n          "field": "Profit margin",\r\n          "type": "quantitative",\r\n          "scale": {\r\n            "domain": [-25, 25],\r\n            "domainMax": 100,\r\n            "domainMin": -150},\r\n          "axis": {"format": ".0f", "titleFontSize": 11},\r\n          "title": "PROFIT MARGIN  (%)"},\r\n        "y": {\r\n          "field": "Profit",\r\n          "type": "quantitative",\r\n          "axis": {\r\n            "titleFontSize": 11,\r\n            "titleAngle": 360,\r\n            "titlePadding": 30},\r\n          "title": "PROFIT  ($)",\r\n          "scale": {\r\n            "domain": [-100, 100],\r\n            "domainMax": 12000,\r\n            "domainMin": -850 }},\r\n        "color": {\r\n          "condition": {\r\n            "test": "datum[\'Profit\'] < 0 || datum[\'Profit margin\'] < 0",\r\n            "value": "#ff0000"}},\r\n        "size": {\r\n          "field": "Point size",\r\n          "type": "quantitative"},\r\n        "tooltip": [\r\n          {"field": "CustomerID", "type": "nominal", "title": "CUSTOMER ID"},\r\n          {"field": "Profit", "title": "PROFIT ($)"},\r\n          {"field": "Sales", "title": "SALES ($)"},\r\n          {"field": "Profit margin", "format": ".1f", "title": "PROFIT MARGIN (%)"}]}\r\n    },\r\n    {\r\n      "transform": [\r\n        {\r\n          "aggregate": [\r\n            {"op": "sum", "field": "Total profit", "as": "sum_profit"},\r\n            {"op": "sum", "field": "Total sales", "as": "sum_sales"} ],\r\n          "groupby": ["CustomerID"]\r\n        },\r\n        {\r\n          "calculate": "datum.sum_profit", "as": "Profit"\r\n        },\r\n        {\r\n          "calculate": "datum.sum_sales", "as": "Sales"\r\n        },\r\n        {\r\n          "calculate": "datum.sum_profit/datum.sum_sales * 100", "as": "Profit margin"\r\n        }],\r\n      "mark": {\r\n        "type": "rule",\r\n        "strokeDash": [7, 6]},\r\n      "encoding": {\r\n        "x": {\r\n          "aggregate": "median",\r\n          "field": "Profit margin",\r\n          "type": "quantitative"},\r\n        "color": {"value": "#b3b3b3"},\r\n        "size": {"value": 2}\r\n      }\r\n    },\r\n    {\r\n      "transform": [\r\n        {\r\n          "aggregate": [\r\n            {"op": "sum", "field": "Total profit", "as": "sum_profit"}],\r\n          "groupby": ["CustomerID"]\r\n        },\r\n        {\r\n          "calculate": "datum.sum_profit", "as": "Profit"\r\n        }],\r\n      "mark": {\r\n        "type": "rule",\r\n        "strokeDash": [7, 6]},\r\n      "encoding": {\r\n        "y": {\r\n          "aggregate": "median",\r\n          "field": "Profit",\r\n          "type": "quantitative"},\r\n        "color": {"value": "#b3b3b3"},\r\n        "size": {"value": 2}\r\n      }\r\n    }\r\n  ],\r\n  "config": {"legend": {"disable": true} }\r\n}\r\n```\r\n\r\nThat\'s it! A couple of wonderful custom visualisations (with tooltips too). Hopefully on my next attempt they will be ones of my own design.\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2022-04-27.md)** for this post\'s markdown file in GitHub.'},{id:1650117600,title:"Add forecasts from Python using Visual Studio Code to Power BI",tag:"logo-python",date:"17 April 2022",content:"\r\nThis is a record of my first attempt at utilising Python for predictive analytics and embedding it into Power BI, using the International Marketplace dataset created using SQL Server Integration Services from my [previous blog post](https://datamesse.github.io/#/post/1641906000).\r\n\r\nThe normalised version of the output is used, which you can download from here:\r\n\r\n**[https://github.com/datamesse/data-visualisation-datasets/blob/main/International%20Marketplace%20sales/International%20Marketplace%20Normalised%20for%20Power%20BI.xlsx?raw=true](https://github.com/datamesse/data-visualisation-datasets/blob/main/International%20Marketplace%20sales/International%20Marketplace%20Normalised%20for%20Power%20BI.xlsx?raw=true)**\r\n\r\n\r\n\r\n## Step 1. Setup Visual Studio Code to run Python\r\n\r\n### Install the Python extension for Visual Studio Code\r\n\r\nInstall the Microsoft verified Python extension.\r\n\r\n![Visual Studio Code Python extension](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--01.png?raw=true)\r\n\r\n\r\n\r\n### Create a Virtual Environment to allow import of Python modules\r\n\r\nThe set up of a virtual environment is so that you can install as many libraries for data analysis and visualisation using Python (i.e. model training and testing), which may be more than the actual libraries needed for the code you ultimately embed in your Power BI report.\r\n\r\nIf you want to link your project to GitHub, follow the standard setup steps, i.e.:\r\n* Create a public repository in GitHub.com\r\n* Create a local repository for the project (Code > Open with GitHub Desktop)\r\n\r\nFrom Visual Studio Code, open a cmd terminal\r\n\r\n![Open a command terminal, part 1](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--02.png?raw=true)\r\n\r\nVisual Studio opens a PowerShell terminal by default, so switch it to Command Prompt.\r\n\r\n![Open a command terminal, part 2](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--03.png?raw=true)\r\n\r\nThe reason for this is PowerShell will by default block running potentially harmful scripts.\r\nSo you should be using cmd terminal from within Visual Studio Code, rather than the PowerShell terminal.\r\n\r\n![Open a command terminal, part 3](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--04.png?raw=true)\r\n\r\nFrom the Visual Studio Code cmd terminal, use the following command structure:\r\n\r\n```\r\npython -m venv yourpythonprojectfolder\\yourvenvname\r\n```\r\n\r\nFor example:\r\n```\r\npython -m venv C:\\Project\\python-for-power-bi\\venv\r\n```\r\n\r\n![Create virtual environment, part 1](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--05.png?raw=true)\r\n\r\nThe venv virtual environment will appear as a folder under VS Code's Explorer pane.\r\n\r\n![Create virtual environment, part 2](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--06.png?raw=true)\r\n\r\n\r\n\r\n### Activating the Virtual Environment\r\n\r\nCommand structure:\r\n```\r\nyourvenvname\\Scripts\\activate\r\n```\r\n\r\nFor example:\r\n```\r\nvenv\\Scripts\\activate\r\n```\r\n\r\n![Run virtual environment](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--07.png?raw=true)\r\n\r\n**Note:** When you close Visual Studio Code, the environment will be stopped. To activate it again, use the above command from a cmd terminal in Visual Studio Code.\r\n\r\n\r\n\r\n### Add the Virtual Environment to .gitignore\r\n\r\nLooking at the previous screenshot, you can see the Source Control icon has over 900 items pending change due to the venv environment. The Python modules that will be later installed for model training and testing, will also be added to that folder.\r\n\r\nSo to avoid pushing all those unnecesary files to GitHub (which _will_ stall it), create a new .gitignore file in your folder's directory and add venv (or whatever your virtual environment\u2019s folder name is), inside that .gitignore file.\r\n\r\nYou will notice the number of pending changes are cut down, and your virtual environment's folder should be greyed out.\r\n\r\n![Add virtual environment to .gitignore](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--08.png?raw=true)\r\n\r\n\r\n\r\n### Deactivate and delete Virtual Environment\r\n\r\nTo deactivate the environment (can be re-activated again later), use the deactivate command.\r\n\r\n```\r\ndeactivate\r\n```\r\n\r\nIf you need to delete the environment, use the following code via a PowerShell terminal if its folder was named venv, otherwise replace venv with the folder name you used.\r\n\r\n```\r\nrm -r venv\r\n```\r\n\r\n\r\n\r\n### Import Python libraries\r\n\r\nAfter performing the steps above (minus deactivating then deleting the environment), you can now install Python libraries a.k.a. modules. Be sure the (venv) prefix is displayed before running, so you know it installs to the virtual environment.\r\n\r\nCommand structure:\r\n```\r\npip install pythonmodulename\r\n```\r\n\r\nFor example:\r\n```\r\npip install pandas\r\npip install numpy\r\npip install scikit-learn\r\npip install matplotlib\r\npip install seaborn\r\n```\r\n\r\n![Install Python libraries](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--09.png?raw=true)\r\n\r\nYou may encounter a pip message preventing installation of subsequent libraries due to version incompatibility. In my case, the message asked to run the following, which you may need to do for your own folder.\r\n\r\n```\r\nc:\\project\\python-for-power-bi\\venv\\scripts\\python.exe -m pip install --upgrade pip\r\n```\r\n\r\nYou can test the environment and Python libraries, by creating a Python script with .py file format extension in your main folder with the following code.\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\njusticeLeague = np.array([\r\n   [1, 'Superman', 1938],\r\n   [2, 'Batman', 1939],\r\n   [3, 'Wonderwoman', 1941],\r\n   [4, 'Flash', 1940,]\r\n])\r\n\r\ncolumnNames = [\r\n   'League ID',\r\n   'Codename',\r\n   'First Appearance'\r\n]\r\n\r\ndf = pd.DataFrame(data = justiceLeague, columns = columnNames)\r\nprint(df)\r\n```\r\n\r\nThen click Run to render the script. If the data displays, then Python is working from the virtual environment.\r\n\r\n![Test Python script displays](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--10.png?raw=true)\r\n\r\n\r\n\r\n### Clearing terminal\r\n\r\nVisual Studio Code has removed the hotkey shortcut for clearing console, which is going to be regularly used for trial-and-error coding and displaying results.\r\n\r\nTo create a shortcut for it, from Visual Studio Code navigate to File > Preferences > Keyboard Shortcuts, then search for the command *workbench.action.terminal.clear*, and apply your keybinding, for example Ctrl + K.\r\n\r\n![Set up shortcut to clear terminal](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--11.png?raw=true)\r\n\r\n\r\n\r\n### Allow plotting to display within Visual Studio Code\r\n\r\nTo allow Visual Studio Code to display plots i.e. visualisations, place this tag at the top of your Python script: #%%\r\n\r\nIt will automatically add a paired tag to the end of the script.\r\n\r\nFor example, replace your test Python file's contents with this:\r\n\r\n```\r\n#%%\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nmonthlySales = np.array([\r\n   [1, 21],\r\n   [2, 17],\r\n   [3, 25],\r\n   [4, 29],\r\n   [5, 15],\r\n   [6, 11],\r\n   [7, 18],\r\n   [8, 13],\r\n   [9, 7],\r\n   [10, 3],\r\n   [11, 14],\r\n   [12, 32]\r\n])\r\n\r\ncolumnNames = [\r\n   'Month',\r\n   'Sales'\r\n]\r\n\r\ndf = pd.DataFrame(data = monthlySales, columns = columnNames)\r\nmonth = df.loc[:, 'Month'].values\r\nsales = df.loc[:, 'Sales'].values\r\nplt.plot(month, sales)\r\n```\r\n\r\nThen click the \"Run Cell\" hyperlink at the top of the script. If it isn't already installed, ipykernel package will prompt to be installed in your virtual environment, which you will need.\r\n\r\n![Install ipykernel](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--12.png?raw=true)\r\n\r\nOnce complete, the plot should display.\r\n\r\n![Data visual in Visual Studio Code](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--13.png?raw=true)\r\n\r\n\r\n\r\n## Step 2. Import data and do preliminary visualisation\r\n\r\nFor the moment, I am choosing to just look at the OrderDate and Profit columns i.e. ignoring other independent variables.\r\nAlthough the following can easily be done in an Excel or Power BI linechart, using Python I want to see the monthly profits.\r\n\r\nFirst by listing out the results:\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndata = pd.read_excel (r'https://github.com/datamesse/data-visualisation-datasets/blob/main/International%20Marketplace%20sales/International%20Marketplace%20Normalised%20for%20Power%20BI.xlsx', sheet_name='FactSales', parse_dates = True)\r\ndf = pd.DataFrame(data, columns= ['OrderDate','Profit'])\r\ndf.set_index('OrderDate')\r\n\r\n# create YearMonth column, and total column by YearMonth\r\ndf['YearMonth'] = df['OrderDate'].dt.to_period(\"M\")\r\ngroupby = df.groupby(['YearMonth'])\r\ndf['MonthlyProfit'] = groupby['Profit'].transform(np.sum)\r\n# remove OrderDate and Profit column so duplicate rows (of YearMonth and MonthlyProfit) becomes evident\r\n# .duplicated() checks if entire row's values match a previous row, and returns True if the case except for the first occurrence\r\ndf = df.drop(columns=['OrderDate','Profit'])\r\ndf.drop_duplicates(keep='first', inplace = True)\r\ndf.set_index('YearMonth')\r\ndf.index.freq = 'MS'\r\n\r\nprint(df)\r\n```\r\n\r\n![List of monthly profit](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--14.png?raw=true)\r\n\r\nThen adding the following code after that to plot/visualise it:\r\n\r\n```\r\nimport matplotlib.pyplot as plt\r\n\r\ndf['YearMonth'] = pd.to_datetime(df['YearMonth'].astype(str) + '-01')\r\nyearMonth = df.loc[:, 'YearMonth'].values\r\nmonthlyProfit = df.loc[:, 'MonthlyProfit'].values\r\nplt.plot(yearMonth, monthlyProfit, c = 'b', marker = '.', markersize = 10)\r\nplt.xticks(fontsize = 8)\r\nplt.yticks(fontsize = 8)\r\nplt.xlabel('Year-Month', fontsize = 10)\r\nplt.ylabel('Millions $', fontsize = 10)\r\n```\r\n\r\n![Plot of monthly profit](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--15.png?raw=true)\r\n\r\n\r\n\r\n## Step 3. Training and testing prediction models\r\n\r\nThis is the point where you would train and test models, with the ultimate objective of generating a list of forecasted figures to load into Power BI as a separate data file. The reason for this is it's better than embeddding the Python model inside the Power BI report directly, especially as complexity and result sets increase.\r\n\r\nAs at the time of writing, I am still aways off from being any kind of statistician, so I'll defer to using the exponential smoothing method Absent Data took in his blog post:\r\n\r\n**[https://www.absentdata.com/power-bi/forecasting-with-python-and-power-bi/](https://www.absentdata.com/power-bi/forecasting-with-python-and-power-bi/)**\r\n\r\nLike Absent Data, I've started off with a daily dataset that I want aggregated to monthly values (as the Power BI report I'll be designing won't be allowing drill down past monthly level for the forecast).\r\n\r\nFrom Step 2, you can see that I've needed to do some tinkering to get the monthly totals so that I could visualise them. That code will need to be incorporated into what we add into Power BI later, as it'll be a needed prep work step, otherwise I end up with forecasted values for Day 1 for each month, rather than the sum of all days for each month.\r\n\r\nFrom Visual Studio Code, the results look like this when listed:\r\n\r\n```\r\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\r\nmodel = ExponentialSmoothing(df['MonthlyProfit'],trend='mul',seasonal='mul',seasonal_periods=12).fit()\r\nrange = pd.date_range('01-01-2024', periods=12, freq='MS')\r\npredictions = model.forecast(12)\r\npredictions_range = pd.DataFrame({'MonthlyProfit':predictions,'YearMonth':range})\r\n\r\nprint(predictions_range)\r\n```\r\n\r\n![List of forecasted monthly profit](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--16.png?raw=true)\r\n\r\nThen when visualised, show a similar pattern to the original dataset:\r\n\r\n```\r\nimport matplotlib.pyplot as plt\r\n\r\nyearMonthForecast = predictions_range.loc[:, 'YearMonth'].values\r\nmonthlyProfitForecast = predictions_range.loc[:, 'MonthlyProfit'].values\r\ndf['YearMonth'] = pd.to_datetime(df['YearMonth'].astype(str) + '-01')\r\nyearMonth = df.loc[:, 'YearMonth'].values\r\nmonthlyProfit = df.loc[:, 'MonthlyProfit'].values\r\nplt.plot(yearMonth, monthlyProfit, c = 'b', marker = '.', markersize = 10)\r\nplt.plot(yearMonthForecast, monthlyProfitForecast, c = 'orange', marker = '.', markersize = 10)\r\nplt.xticks(fontsize = 8)\r\nplt.yticks(fontsize = 8)\r\nplt.xlabel('Year-Month', fontsize = 10)\r\nplt.ylabel('Millions $', fontsize = 10)\r\n```\r\n\r\n![Plot of forecasted monthly profit](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--17.png?raw=true)\r\n\r\n\r\n\r\n## Step 4. Integrate Python model into Power BI\r\n\r\nIn this scenario, only 12 values are produced, so there is likely minimal impact from incorporating it into Power BI.\r\n\r\n**Note:** However, if your scenario involves a larger output, it may be better to export the data from Visual Studio Code to a flat file using either of these commands, then loading the file to Power BI:\r\n\r\n```\r\ndf.to_csv(path_or_buf='export/filename.csv', index=False)\r\ndf.to_excel(excel_writer='export/filename.xlsx', index=False)\r\n```\r\n\r\nContinuing with our example, from Power Query in Power BI, I reference the fact table and perform a Group By to sum the daily profits of the original dataset.\r\n\r\n![Power Query Group By](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--18.png?raw=true)\r\n\r\nThen convert the OrderDate to data type text, and split column on the forwardslash / delimiter at every occurrence.\r\n\r\n![Power Query split date column](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--19.png?raw=true)\r\n\r\nThe reason for this is to create a new _text datatype_ column using dash delimiter, to align with Python's date formatting.\r\n\r\n```\r\n[Year] & \"-\"  & [Month] & \"-\" & \r\n(if Text.Length([Day]) = 1\r\nthen \"0\"\r\nelse \"\") & [Day]\r\n```\r\n\r\n![Power Query add custom date column](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--20.png?raw=true)\r\n\r\nThen remove the unnecessary columns, so only the essential ones are included for the dataset that Python ingests.\r\n\r\n![Power Query remove unneeded columns](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--21.png?raw=true)\r\n\r\nFrom Power Query's Transform tab, click \"Run Python Script\".\r\n\r\n![Power Query Run Python Script](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--22.png?raw=true)\r\n\r\nWhilst most of the code from your Python script can be copy-and-pasted, note that the data source is the step in Query Settings prior to your Python script, and is named \"dataset\", so you need to adjust your script to accommodate for that.\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\ndf = pd.DataFrame(dataset, columns = ['OrderDate','Profit'])\r\n\r\ndf['OrderDate'] = pd.to_datetime(df['OrderDate'], errors = 'ignore')\r\n\r\ndf.set_index('OrderDate')\r\ndf['YearMonth'] = df['OrderDate'].dt.to_period(\"M\")\r\ngroupby = df.groupby(['YearMonth'])\r\ndf['MonthlyProfit'] = groupby['Profit'].transform(np.sum)\r\ndf = df.drop(columns=['OrderDate','Profit'])\r\ndf.drop_duplicates(keep='first', inplace = True)\r\ndf.set_index('YearMonth')\r\ndf.index.freq = 'MS'\r\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\r\nmodel = ExponentialSmoothing(df['MonthlyProfit'],trend='mul',seasonal='mul',seasonal_periods=12).fit()\r\nrange = pd.date_range('01-01-2024', periods=12, freq='MS')\r\npredictions = model.forecast(12)\r\npredictions_range = pd.DataFrame({'MonthlyProfit':predictions,'YearMonth':range})\r\n```\r\n\r\n![Power Query add Python script](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--23.png?raw=true)\r\n\r\n**Important!**: Upon clicking OK, you may encounter error messages preventing the script from running, asking for you to install Python libraries in order to proceed. The reason for this is that the libraries you installed in Visual Studio Code were for your virtual environment. However, Power BI does not access that virtual environment, and instead is accessing your native Python installation, which you may not have installed the libraries to.\r\n\r\nThen click to expand Table for the predictions_range variable.\r\n\r\n![Power Query expand Table](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--24.png?raw=true)\r\n\r\n![Power Query expanded Table with monthly forecast values](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--25.png?raw=true)\r\n\r\nConnect the output to your date or calendar table to help ensure the values plot in your Power BI time series visuals. **Note:** I am aware that best practice modelling typically advises to avoid 2 way relationships as much as possible, so that's something that needds to be looked into amending the data model here later on.\r\n\r\n![Power BI Model view connect to Calendar table](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--26.png?raw=true)\r\n\r\nHere are the two line charts of actuals and forecasted figures next to each other:\r\n\r\n![Power BI line chart comparing actuals and forecast](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--27.png?raw=true)\r\n\r\nThen following one of many tutorials you can find on YouTube to merge actuals and forecasted figures, you can create a single linechart like below:\r\n\r\n![Power BI line chart comparing actuals and forecast](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-04-17--28.png?raw=true)\r\n\r\n\r\nMy recommendations are:\r\n\r\n* YouTube video: **[Showing actuals and forecasts in the same chart with Power BI by SQLBI](https://www.youtube.com/watch?v=DKgF-5QHY68)**\r\n\r\n* YouTube video: **[Combining ACTUALS and FORECAST as one LINE in Power BI by How to Power BI](https://www.youtube.com/watch?v=_TAGpAJ9rTQ)**\r\n\r\nThe second takes the first video one step futher by showing the calculation used to sum the actuals and forecasts into a single measure.\r\n\r\n\r\n\r\n### References\r\n\r\n* Blog post **[Forecasting with Python and Power BI by Absent Data](https://www.absentdata.com/power-bi/forecasting-with-python-and-power-bi/)**\r\n\r\n* YouTube video **[How do I find and remove duplicate rows in pandas?](https://www.youtube.com/watch?v=ht5buXUMqkQ)**\r\n\r\n* YouTube video **[How to install Python Libraries in Visual Studio Code by Aditya Thakur](https://www.youtube.com/watch?v=ThU13tikHQw)**\r\n\r\n* Stack Overflow question **[How to show Python plots in Visual Studio Code](https://stackoverflow.com/questions/49992300/)**\r\n\r\n* Stack Overflow question **[How to clear Visual Studio Code terminal](https://stackoverflow.com/questions/49992300/)**\r\n\r\n* Stack Overflow question **[Python script in Power BI returns date as Microsoft.OleDb.Date](https://stackoverflow.com/questions/51929420/python-script-in-power-bi-returns-date-as-microsoft-oledb-date)**\r\n\r\n* Stack Overflow question **[Setting the x axis as time (years, months) for a set of values](https://stackoverflow.com/questions/28948898/setting-the-x-axis-as-time-years-months-for-a-set-of-values)**\r\n\r\n* LinkedIn Learning course **[Python: Working with Predictive Analytics by Isil Berkun](https://www.linkedin.com/learning/python-working-with-predictive-analytics/)**\r\n\r\n* LinkedIn Learning course **[Python for Data Visualization by Michael Galarnyk and Madecraft](https://www.linkedin.com/learning/python-for-data-visualization)**\r\n\r\n\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2022-04-17.md)** for this post's markdown file in GitHub."},{id:1645880400,title:"Find grouped records with a date that matches another date column using DAX",tag:"logo-powerbi",date:"27 February 2022",content:"\r\nHow to use DAX to group records by an identity column, and see if the most recent row has a date value that matches a date value in another column.\r\n\r\nIn previous posts I've tried to find __[window aggregate values using DAX](https://datamesse.github.io/#/post/1634994000)__, and as many Power BI developers highlight, there are many ways to get the same outcome. However the difference between that previous post and this one is that the prior one applied a value to a specific row of the group, whereas this one applies the value to all rows of each grouping, which will be needed for other calculated columns.\r\n\r\nWhilst working on a small data integrity report at work, I came across this method which is a slightly modified script from a Power BI Communities post, intended to pull the desired value per grouping and apply it to all rows of each group.\r\n\r\n**DAX structure**\r\n```\r\nCustom column 1 =\r\nVAR identifiercolumn = 'Table'[ID column]\r\nVAR findlatestrecord = CALCULATE( MAX( 'Table'[Date A] ), ALLEXCEPT( 'Table', 'Table'[ID column] ) )\r\nRETURN\r\n   CALCULATE ( \r\n              MAX( 'Table'[Date A] ),\r\n              FILTER( 'Table', 'Table'[Date A] = findlatestrecord ),\r\n                      'Table'[ID column] = identifiercolumn               \r\n   )\r\n```\r\nWhat should be noted is the aggregation under RETURN doesn't matter in this case i.e. it could be MIN or MAX, because the second declared variable targets the specific row to get its value from, rather than actually aggregating all values for the group.\r\n\r\nThe scenario I applied this to involves a series of flat files produced on separate days to represent historic and future orders. Each order is scheduled to be delivered on a specific date at a certain cost. The report is intended to highlight problematic data, such as Order IDs that were scheduled for a specific date in one file, but do not appear in the other files generated on that same date.\r\n\r\nIn this context, we want to know the most recent *Planned Delivery Date* applied to an *Order ID* based on the latest *File's date* that the *Order ID* appears in, then apply that value to all the rows for that *Order ID*\r\n\r\nIn the example below, there are 2 Order IDs, 8 and 9. *Order ID* 8 has its *Planned Delivery Date* moved earlier, and *Order ID* 9 has its own moved later, in subsequent files. The ones highlighted orange are the ones we want to display.\r\n\r\n![Sample dataset 1](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-02-27--01.png?raw=true)\r\n\r\nApplying the above DAX structure, we get the following:\r\n\r\n**DAX implementation**\r\n```\r\nMost recent Planned Delivery Date = \r\nVAR orderid = 'Data integrity audit'[Order ID]\r\nVAR highestfiledatebyorderid =\r\n    CALCULATE(\r\n        MAX( 'Data integrity audit'[File's date] ), ALLEXCEPT( 'Data integrity audit','Data integrity audit'[Order ID] )\r\n    )\r\nRETURN\r\n    CALCULATE(\r\n        MAX ( 'Data integrity audit'[Planned Delivery Date] ),\r\n        FILTER( 'Data integrity audit', 'Data integrity audit'[File's date] = highestfiledatebyorderid ),\r\n                'Data integrity audit'[Order ID] = orderid\r\n    )\r\n```\r\n\r\n![Applied DAX structure to find recent date value by Order ID](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-02-27--02.png?raw=true)\r\n\r\n![Successfully applied DAX column added to table 1](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-02-27--03.png?raw=true)\r\n\r\nNow that we have applied the most recent *Planned Delivery Date* for every row based on *Order ID* as an anchor, we just need to check to if the *Order ID* has a row reflecting that it appears in a file whose generation date is the same as the most recent *Planned Delivery Date*. In this context, it serves as affirmation that it was delivered on that day.\r\n\r\n**DAX structure**\r\n```\r\nCustom column 2 =\r\nIF (\r\n    ISBLANK(\r\n            CALCULATE(\r\n               FIRSTNONBLANK( 'Table'[ID column], 1 ),\r\n               FILTER( ALLEXCEPT( 'Table', 'Table'[ID column] ),\r\n                                  'Table', 'Table'[Custom column 1] = 'Table'[Date B]               \r\n               )\r\n            )\r\n    )\r\n)\r\n```\r\n\r\nLooking at *Order ID* examples 8 and 16, the former has a matchng record and the latter does not.\r\n\r\n![Sample dataset 2](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-02-27--04.png?raw=true)\r\n\r\n**DAX implementation**\r\n```\r\nDoes Order ID appear in File with date matching Delivery Date? = \r\nIF(\r\n    ISBLANK(\r\n        CALCULATE(\r\n            FIRSTNONBLANK('Data integrity audit'[Order ID], 1),\r\n            FILTER( ALLEXCEPT('Data integrity audit','Data integrity audit'[Order ID] ),\r\n                              'Data integrity audit'[Most recent Planned Delivery Date] = 'Data integrity audit'[File's date] )\r\n        )\r\n    ) = FALSE,\r\n    \"Yes\",\r\n    \"No\"\r\n)\r\n```\r\n\r\n![Applied DAX structure to match different date columns and apply to all records per group](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-02-27--05.png?raw=true)\r\n\r\n![Successfully applied DAX column added to table 2](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-02-27--06.png?raw=true)\r\n\r\nOther more complex columns can then be built on top of this one, in my case, creating a conditional column that categorises the different patterns to highlight areas where data integrity may require review.\r\n\r\n![Complex conditional DAX query built on top of the aforementioned queries](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-02-27--07.png?raw=true)\r\n\r\n![Example categorisation from previous query added to a matrix](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-02-27--08.png?raw=true)\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2022-02-27.md)** for this post's markdown file in GitHub.\r\n"},{id:1639746e3,title:"How to create a free Azure account to post reports to the Power BI Gallery",tag:"logo-azure",date:"18 December 2021",content:'\r\nIf your work or school account does not provide you access to Power BI (for the purposes of publishing to Web, specifically the Community Gallery), you can create your own. \r\n\r\nYou can check your existing account by navigating to **[https://powerbi.microsoft.com/](https://powerbi.microsoft.com/)**, then clicking "Have an account? Sign in".\r\n\r\n![Check Power BI sign in](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-12-18--01.png?raw=true)\r\n\r\nIf you receive the following message:\r\n\r\n*"Sorry, we can\'t sign you up as ...*\r\n\r\n*Your IT department has turned off signup for Microsoft Power BI. Contact them to complete signup."*\r\n\r\n![Power BI sign up disabled](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-12-18--02.png?raw=true)\r\n\r\n...and have no sway over your IT department to give you access, you can sign up for a new account instead by clicking "No account? Create one!", and set up your new email and password.\r\n\r\n![Azure Sign in create account](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-12-18--03.png?raw=true)\r\n\r\n![Azure create a new account](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-12-18--04.png?raw=true)\r\n\r\nYou can then start your Azure free trial. Fill out your details, which will require providing a mobile number for identity verification.\r\n\r\n![Azure portal](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-12-18--05.png?raw=true)\r\n\r\nWith your account set up, you can now begin creating your own "organisation" or tenant, which would have Power BI enabled. Begin by going to Azure Active Directory.\r\n\r\n![Azure Active Directory](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-12-18--06.png?raw=true)\r\n\r\nClick "Manage tenants", and create your new tenant.\r\n![Azure Active Directory Manage tenants](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-12-18--07.png?raw=true)\r\n\r\nThen create a new user under that tenant domain e.g. yourname@yourtennt.onmicrosoft.com, and ensure that new user is added to the Administrators group of the tenant.\r\n\r\n![Azure Active Directory Add User](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-12-18--08.png?raw=true)\r\n\r\n![Azure Active Directory Add User Assignment](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-12-18--08a.png?raw=true)\r\n\r\nFor that new user account, assign the roles Power BI Administrator and Power Platform admin, which you can do from **[https://portal.office.com](https://portal.office.com)**.\r\n\r\n![Microsoft 365 admin center](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-12-18--09.png?raw=true)\r\n\r\nThen you can configure "Publish to Web" from the Admin portal at **[https://app.powerbi.com](https://app.powerbi.com)**. There are plenty of other resources for setup recommendations. For example, this post from **[Radacad](https://radacad.com/power-bi-administrator-tenant-settings-configuration-you-dont-dare-to-miss)**.\r\n\r\n![Power BI Admin portal](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-12-18--10.png?raw=true)\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2021-12-18.md)** for this post\'s markdown file in GitHub.'},{id:1635598800,title:"Customer support ticket update generator and sample dataset",tag:"logo-excel",date:"31 October 2021",content:'\r\nThis is a sample dataset created using Excel randomisation, and you can create your own using the file generator.\r\n\r\nYou can download or connect to the sample dataset from **[here](https://github.com/datamesse/data-visualisation-datasets/blob/main/Support%20ticket%20updates/Support%20ticket%20updates.xlsx?raw=true)**.\r\n\r\nThe Github repository with the agent photos can be found **[here](https://github.com/datamesse/data-visualisation-datasets/tree/main/Support%20ticket%20updates/agents)**.\r\n\r\nAn Excel random person name and business generator was also used to create this dataset, downloadable **[here](https://github.com/datamesse/data-visualisation-datasets/blob/main/Support%20ticket%20updates/Random%20name%20and%20business%20generator.xlsx?raw=true)**.\r\n\r\nThe dataset contains:\r\n - 5000 support tickets\r\n - 27780 support ticket update records\r\n - 29 agents across 8 countries and 12 cities\r\n - 1233 end users across 27 countries and 65 cities\r\n - Date/timestamps are based on Sydney, Australia time (AEST GMT+10/AEDT GMT+11)\r\n\r\nThe first worksheet "Updates" has the back-and-forth update records for each ticket, indicating if the update is a public user message or agent reply, or an internal message by an Agent.\r\n\r\n![Support ticket updates](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-31--01.png?raw=true)\r\n\r\nThe second worksheet "Assignment" has the ticket created versus ticket assigned date/time data.\r\n\r\n![Support ticket assignments](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-31--02.png?raw=true)\r\n\r\nThe third worksheet "Agents" has photo IDs that correlate to the images in this **[Github folder](https://github.com/datamesse/data-visualisation-datasets/tree/main/Support%20ticket%20updates/agents)**.\r\n\r\n![Support ticket agents](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-31--03.png?raw=true)\r\n\r\n![Support ticket agent photos](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-31--04.png?raw=true)\r\n\r\n\r\n**Why this dataset was created?**\r\n\r\nWorking in a global support team providing follow-the-sun (FTS) customer service, I wanted to be able to visualise the teamwork across staff members.\r\n\r\nAs I needed a fake dataset to test ideas on, I created an Excel file that randomly generated support ticket update information (e.g. when a end-user opens a ticket, when an agent first replies to the end-user, and the back-and-forth until the ticket becomes solved). The columns used in the dataset are meant to be similar to those seen in CRM systems that provide reporting on such ticket updates (e.g. Zendesk).\r\n\r\nI noticed my random generator did not account for the ticket IDs being in a realistic sequence, which needs to be amended after extracting the data from the generator. Also, the generator is limited in providing "follow-the-sun" support update records, so I spent some time manually editing records to demonstrate this in the sample dataset.\r\n\r\n\r\n**Disclaimer**\r\n\r\nThis dataset is free to use and alter as you need, and no attribution is required, though would be appreciated.\r\n\r\nAll names in this dataset are fictional and not based on real-life people. The random name generator that was used to create them can be found **[here](https://github.com/datamesse/excel-support-ticket-update-generator/blob/main/Random_name_and_business_generator.xlsx?raw=true)**.\r\n\r\nPhotographs were taken from **[Pixabay.com](https://pixabay.com/service/license/)** and **[Pexels.com](https://www.pexels.com/license/)** for non-commercial use, edited to fit the appearance of an organisational profile photo, and direct URL attribution included in the dataset for each photo.\r\n\r\nThe GitHub repository for this is **[here](https://github.com/datamesse/data-visualisation-datasets/tree/main/Support%20ticket%20updates)**.\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2021-10-31.md)** for this post\'s markdown file in GitHub.'},{id:1634994e3,title:"Calculate aggregate for grouped rows based on column value, DAX version",tag:"logo-powerbi",date:"24 October 2021",content:'\r\nHow to use DAX to find the aggregate value for rows grouped by a column value in Power BI.\r\n\r\nIn a __[previous post](https://datamesse.github.io/#/post/1634389200/)__, I wrote on window aggregation equivalents in Power Query, going on the notion that categorical columns are generally better off done in Power Query using M code, as opposed to done in DAX.\r\n\r\nHowever, in a Power BI report I am working on, it seemed re-implementing it using DAX had faster report load times than Power Query, which I attributed to existing Power Query merges needed for the model, which slowed down processing. It also required having 2 columns in the model (one for the aggregation, and the other for the conditional result), unlike DAX which only required one.\r\n\r\nThat M code looked like this:\r\n\r\n**Power Query M structure**\r\n```\r\n    #"Added Custom 1" = Table.NestedJoin(#"Previous step",\r\n                                      {"Column A to join on"},\r\n                                      Table.Group(Table.SelectRows(#"Changed Type", each ([Column C] = "Agent")),\r\n                                                  {"Column A to join on"},\r\n                                                  {{"Result of aggregation",\r\n                                                  each List.Min([#"Column B to aggregate on"]), type nullable datetime}}),\r\n                                      {"Column A to join on"},\r\n                                      "Merged group by table",\r\n                                      JoinKind.Inner),\r\n    #"Expanded Merged group by table" = Table.ExpandTableColumn(#"Added Custom 1", "Merged group by table", {"Result of aggregation"}, {"Result of aggregation"}),\r\n    #"Added Custom 2" = Table.AddColumn(#"Expanded Merged group by table", "M column result", each if [#"Column B to aggregate on"] = [#"Result of aggregation"] then "Yes" else "No")\r\n```\r\n\r\nIn this example we\'re still using randomised support ticket update data, and trying to find which agent reply to the user is the first for each Ticket ID.\r\n\r\n![Sample dataset with only 15 rows of data](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--01.png?raw=true)\r\n\r\n\r\n**Power Query implementation**\r\n```\r\n    #"Added Custom" = Table.NestedJoin(#"Changed Type",\r\n                                      {"Ticket ID"},\r\n                                      Table.Group(Table.SelectRows(#"Changed Type", each ([Updater role] = "Agent")),\r\n                                                  {"Ticket ID"},\r\n                                                  {{"Earliest date/time by Ticket ID",\r\n                                                  each List.Min([#"Update - Timestamp"]), type nullable datetime}}),\r\n                                      {"Ticket ID"},\r\n                                      "Merged group by table",\r\n                                      JoinKind.Inner),\r\n    #"Expanded Merged group by table" = Table.ExpandTableColumn(#"Added Custom", "Merged group by table", {"Earliest date/time by Ticket ID"}, {"Earliest date/time by Ticket ID"}),\r\n    #"Added Custom1" = Table.AddColumn(#"Expanded Merged group by table", "1st reply?", each if [#"Update - Timestamp"] = [#"Earliest date/time by Ticket ID"] then "Yes" else "No")\r\nin\r\n    #"Added Custom1"\r\n```\r\n\r\n![Sample dataset with only 15 rows of data Power Query M code](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--02.png?raw=true)\r\n\r\n\r\n![Sample dataset with only 15 rows of data Power Query performance](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--03.png?raw=true)\r\n\r\nNow let\'s look at the DAX alternative.\r\n\r\nIn DAX, we\'ve coded it as a new column, where an initial if condition is used to pre-determine the result for invalid rows (such as messages from client), and the nested false condition checks the remaining canidate rows (messages from agents) to see if the aggregate value (i.e. the earliest *Update - Timestamp*) matches for the existing row for each Ticket ID group.\r\n\r\n![Sample dataset with only 15 rows of data DAX Add New Column](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--04.png?raw=true)\r\n\r\n**DAX structure**\r\n```\r\nDAX column = IF([Column C] = "Column value of rows that should be excluded from aggregation",\r\n                "No",\r\n                IF(Tablename[Column B to aggregate on] = \r\n                    CALCULATE (\r\n                               MIN ( Tablename[Column B to aggregate on] ),\r\n                               FILTER(ALLEXCEPT (Tablename, Tablename[Column A to join on]), Tablename[Column C] = "Column value of rows to aggregate on" ))\r\n                              ,"Yes"\r\n                              ,"No"\r\n                )\r\n             )\r\n```\r\n\r\n**DAX implementation**\r\n```\r\n1st reply? = IF([Updater role] = "Client",\r\n                "No",\r\n                IF(Updates1[Update - Timestamp] = \r\n                    CALCULATE (\r\n                               MIN ( Updates1[Update - Timestamp] ),\r\n                               FILTER(ALLEXCEPT (Updates1, Updates1[Ticket ID]), Updates1[Updater role] = "Agent" ))\r\n                              ,"Yes"\r\n                              ,"No"\r\n                )\r\n             )\r\n```\r\n\r\n![Sample dataset with only 15 rows of data DAX code](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--05.png?raw=true)\r\n\r\n\r\n![Sample dataset with only 15 rows of data DAX performance](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--06.png?raw=true)\r\n\r\n\r\nComparing the initial table refreshes from the screenshots above seems to indicate Power Query in this scenario is more performant than DAX.\r\n* DAX: 224ms\r\n* Power Query: 94ms\r\n\r\nHowever, this is a flat dataset with only 15 rows. So I set out to test it with a larger flat dataset (27780 rows), and if the results still show Power Query is more performant than DAX for categorical window aggregation, I would test a second time with that larger dataset, but using pre-existing merges to more closer reflect my model.\r\n\r\n\r\n**COMPARING DAX AND M FOR CATEGORICAL WINDOW AGGREGATION**\r\n\r\nThis blog post compares DAX vs Power Query (M) implementation of this scenario against:\r\n1. flat dataset using M for the window aggregation column\r\n2. flat dataset using DAX for the window aggregation column\r\n3. dataset with existing merge using M for the window aggregation column\r\n4. dataset with existing merge using DAX for the window aggregation column\r\n\r\n**Test 1: Flat dataset using M for the aggregation column**\r\n\r\n![Large flat dataset using M window aggregation M code](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--07.png?raw=true)\r\n\r\nNote: Refresh 1 is when the column is first added to the visual.\r\n\r\n![Large flat dataset using M performance](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--09.png?raw=true)\r\n\r\n\r\n**Test 2: Flat dataset using DAX for the aggregation column**\r\n\r\n![Large flat dataset using M window aggregation DAX code](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--08.png?raw=true)\r\n\r\n\r\n![Large flat dataset using DAX](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--10.png?raw=true)\r\n\r\nNow we\'ve applied a simple merge to the dataset to display additional columns.\r\n![Large dataset with existing merge](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--11.png?raw=true)\r\n\r\n\r\n**Test 3: Merged dataset using M for the aggregation column**\r\n\r\n![Large dataset with existing merge using M performance](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--12.png?raw=true)\r\n\r\n\r\n![Large dataset with existing merge using DAX performance](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--15.png?raw=true)\r\n\r\n\r\n**Test 4: Merged dataset using DAX  for the aggregation column**\r\n\r\n![Large dataset with existing merge using DAX](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--13.png?raw=true)\r\n\r\n\r\n![Large dataset with existing merge using DAX performance](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--14.png?raw=true)\r\n\r\n\r\n![Large dataset with existing merge using DAX performance](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--16.png?raw=true)\r\n\r\nReviewing these table refresh times, it appears that for data sources involving a merge, using DAX for window aggregations is more performant than using Power Query, whereas it seems to be the reverse for data sources not involving merges.\r\n\r\n![Comparing table refresh times](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-24--17.png?raw=true)\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2021-10-24.md)** for this post\'s markdown file in GitHub.'},{id:1634389200,title:"Find aggregate value for grouped rows based on column value",tag:"logo-powerbi",date:"17 October 2021",content:'\r\nHow to use Power Query to find the aggregate value for rows grouped by a column value in Power BI.\r\n\r\nT-SQL uses the concept of window functions to perform aggregations across groupings of rows, which are based on a specific column\'s values. More info on this can be found on [Pinal Dave\'s blog](https://blog.sqlauthority.com/2015/05/28/sql-server-what-are-t-sql-window-functions-notes-from-the-field-082/).\r\n\r\n```\r\nTable.NestedJoin(#"Previous step in your Power Query code", \r\n                 {"Column(s) for left side of the join"}, \r\n                Table.Group(#"Previous step in your Power Query code", \r\n                             {"Column(s) to group by"}, \r\n                             {{"New column name for the aggregation result", \r\n                             each List.Min([#"Column to apply aggregation on"]), \r\n                             type nullable datatypeofyouraggregation}}),\r\n                 {"Column(s) for right side of the join"}, \r\n                 "New merged group by table name",\r\n                 JoinKind.Inner)\r\n```\r\n\r\nAs a basic example, say your dataset has rows representing quarterly sales and you need to calculate the proportion of sales which quarter represents for the whole year i.e. Quarterly Sales \xf7 Annual Sales.\r\n\r\nYou can do this by using a preliminary window function to first calculate annual sales by summing the rows based on shared year column values, providing that same value for each row of the group. Then for each row, calculate the proportion from there.\r\n\r\n![Example concept of Window function](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-17--01.png?raw=true)\r\n\r\nThe equivalent of Window functions in Power BI would typically be used for quantitative measures, and are thus more likely to be implemented using DAX calculations.\r\n\r\nHowever, there can be scenarios where window functions need to employed in a more categorical nature. Going by the general principle that custom measures (i.e. quantitative calculations) should be done in DAX, and that custom columns (typically categorical) should be done in Power Query, the latter is what we will employ here.\r\n\r\nIn this example scenario\'s dataset, we have 5 unique support ticket numbers, with each row representing an instance where a support agent has sent a response to the end-user for a ticket, as indicated by the date/timestamp.\r\n\r\n![Example categorical scenario of support ticket response date/timestamps](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-17--02.png?raw=true)\r\n\r\nOur objective is to create a column that identifies which rows of data represent the "first response" to the end-user for each ticket, with that intended 1st response flag being a rudimentary marker to help calculate other support agent statistics we may want (e.g. proportion of support tickets where the 1st responder is the assignee).\r\n\r\nThis is the equivalent of the previous example\'s proportion of annual sales that quarterly sales represents, in that the creation of a preliminary window function is needed. In our scenario, instead of doing a sum of sales based on rows sharing the same year, we will get the minimum (and hence the first) date/timestamps based on rows sharing the same ticket number.\r\n\r\nThe approach taken here is to create a new column whose definition is the combination of two common queries:\r\n1. The Table.NestedJoin function that\'s commonly seen in *Merge Queries* to combine your existing dataset to the conceptual grouped dataset.\r\n2. The Table.Group function creates a conceptual grouped by table of the existing dataset with the desired aggregate result, and that conceptual table being passed as the right-joined table parameter into the Table.NestedJoin.\r\n3. Then expand the merged table to display the aggregate result for each row of the original dataset.\r\n\r\n**Code structure**\r\n\r\n```\r\nTable.NestedJoin(#"Previous step in your Power Query code", \r\n                 {"Column(s) for left side of the join"}, \r\n                Table.Group(#"Previous step in your Power Query code", \r\n                             {"Column(s) to group by"}, \r\n                             {{"New column name for the aggregation result", \r\n                             each List.Min([#"Column to apply aggregation on"]), \r\n                             type nullable datatypeofyouraggregation}}),\r\n                 {"Column(s) for right side of the join"}, \r\n                 "New merged group by table name",\r\n                 JoinKind.Inner)\r\n```\r\n\r\n**Example**\r\n\r\nApplying the code structure above, the M code would be as below:\r\n\r\n![M code with merged](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-17--03.png?raw=true)\r\n\r\n```\r\n    #"Added Custom" = Table.NestedJoin(#"Changed Type", \r\n                                       {"Ticket ID"},\r\n                                       Table.Group(#"Changed Type",\r\n                                                   {"Ticket ID"},\r\n                                                   {{"1st response",\r\n                                                   each List.Min([#"Update - Timestamp"]), type nullable datetime}}),\r\n                                       {"Ticket ID"},\r\n                                       "Merged group by table",\r\n                                       JoinKind.Inner)\r\n```\r\n\r\nThe result of the new column addition (which is a merged table), will appear as below.\r\n\r\n![Power Query with new column for merged Group By table](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-17--04.png?raw=true)\r\n\r\nThen you simply need to expand out the aggregate column from the merge.\r\n\r\n![Power Query expand merged Group By table to display the aggregate column 1](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-17--05.png?raw=true)\r\n\r\n![Power Query expand merged Group By table to display the aggregate column 2](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-17--06.png?raw=true)\r\n\r\nNow that the preliminary window function is complete, we can address the example objective of creating a custom column to indicate which data row per Ticket ID represents the first support agent response to an end-user, which is a simple if condition to compare the "Update - Timestamp" and "1st response" columns.\r\n\r\n![Power Query if condition for example custom column](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-17--07.png?raw=true)\r\n\r\n![Power Query final example custom column](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-17--08.png?raw=true)\r\n\r\nIn this way, regardless of how the data is sorted in Power Query, the records representing the 1st responses will remain.\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2021-10-17.md)** for this post\'s markdown file in GitHub.'},{id:1633784400,title:"Dynamically apply time zone and daylight savings on date/times in Power BI",tag:"logo-powerbi",date:"10 October 2021",content:'\r\nHow to use Power Query to apply time zone offsets based on daylight savings "anchors" on date/times using a parameter, and without needing a separate calendar table in Power BI.\r\n\r\nThe final product is being able to use a Parameter to select a desired time zone, and apply it to your dataset\'s "Date/Time" column, and produce a "Date/Time/Zone" value.\r\n![Power BI Tokyo example](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--01.png?raw=true)\r\n\r\nWhat makes this different from the DateTime.Zone function alone, is that this method respects when the UTC offset changes based on time zone by creating custom functions to do this.\r\n\r\nThis post incorporates my previous posts on [how to import time zone and daylight saving observations from Wikipedia](https://datamesse.github.io/#/post/1633183200), which indicate when different offsets are applied (e.g. first Sunday of October), and my post on [how to find the nth day of a month](https://datamesse.github.io/#/post/1632578400) to convert those into usable "anchors" to determine the offset value for the date/times.\r\n\r\n**Note:** This is not an appropriate solution to time zone application in terms of data accuracy, processing efficiency, and coding involved. Ideally a predefined dataset or an API with actual date/time values for observation period start/ends would be best.\r\n\r\nA good example of this can be found in [a blog post by John White](https://whitepages.unlimitedviz.com/2020/10/dynamic-time-zone-conversion-using-power-bi/)\r\n\r\nThis post shows how date/time anchor values can be used as an alternative way to solve this problem, which does not use calendar tables nor APIs.\r\n\r\nThis is the **[sample date/time dataset](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/Date_times_to_convert.xlsx?raw=true)** we will dynamically apply time zone offsets to via parameter selection.\r\n\r\nNote there is no time zone in the data itself, so assumptions made by any application (e.g. user\'s machine time zone) may be incorrect.\r\n![Sample date time dataset in Excel](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--02.png?raw=true)\r\n\r\n![Sample date time dataset imported into Power Query](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--03.png?raw=true)\r\n\r\nAfter importing the sample date/time dataset, we next import the combined time zone offset and daylight savings observation **[dataset](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/Time_zone_offsets_and_DST_observations.xlsx?raw=true)** created in this [post](https://datamesse.github.io/#/post/1633183200).\r\n\r\nAt this point, if you only need certain time zones to select from, you can filter for them here before proceeding.\r\n\r\n![Offset and observation dataset imported into Power Query](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--04.png?raw=true)\r\n\r\nNext, we will create a list from the Timezone column, to be used as the available selections of the parameter.\r\n\r\n![Power Query Convert to List Part A](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--05.png?raw=true)\r\n\r\n![Power Query Convert to List Part B](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--06.png?raw=true)\r\n\r\nThen set up the parameter itself to pull from that list.\r\n\r\n![Power Query Create new parameter](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--07.png?raw=true)\r\n\r\n![Power Query Manage Parameters Part A](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--08.png?raw=true)\r\n\r\n![Power Query Manage Parameters Part B](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--09.png?raw=true)\r\n\r\nNext, we create a new column in the sample dataset whose value is the parameter selection.\r\n\r\n![Power Query custom column for Parameter value Part A](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--10.png?raw=true)\r\n\r\n![Power Query custom column for Parameter value Part B](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--11.png?raw=true)\r\n\r\nThen merge the two datasets using that new custom column.\r\n![Power Query Merge Queries](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--12.png?raw=true)\r\n\r\n![Power Query Merge Queries](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--13.png?raw=true)\r\n\r\nWhilst expanding the merged table, we can prefix the column names, which may be useful if intending to merge multiple times, e.g. parameter for data source\'s actual time zone vs parameter for desired time zone. We will only do the merge once, in this example.\r\n\r\n![Power Query expand merge queries Part A](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--14.png?raw=true)\r\n\r\n![Power Query expand merge queries Part B](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--15.png?raw=true)\r\n\r\nOur next step is to create a custom column to convert our Date/Time value to match the time zone of the parameter selection. This is not as simple as appending an offset to our Date/Time value, because of these considerations\r\n\r\na. The need to account for different Date/Time offsets based on standard vs daylight savings.\r\n\r\nb. The anchor dataset (which determines whether or not daylight savings is applied) has a mix of data structures e.g. anchor date/times can be either UTC or local time-based, and can either have a specific date of the month, or relative day position of the month.\r\n\r\nc. The datasets\' standard and daylight saving offset values are in a text based structure e.g. "+10:00", rather than straight numbers, which are more easily consumed by Power Query functions (e.g. *DateTime.AddZone()*).\r\n\r\nBefore we create the custom column, we will need 3 custom functions.\r\n1. A simple suffix of the standard or daylight daylight offset to the Date/Time value to make it a DateTimeZone value, which we\'ll name **DatetimeToDatetimezone**.\r\n2. A slightly more complex function that pulls in all date anchor values to convert to an actual date. But it only lets single parameters to pass for the time anchor (which could be UTC or local time) and offset (standard or daylight saving), which we\'ll name **AnchorToDatetimezone**.\r\n3. The complex function that applies the time zone to the Date/Time value, factoring in daylight saving and standard time observation by using the previous two functions, which we\'ll name **DatetimeAppendZone**.\r\n\r\nBeginning with the simple **DatetimeToDatetimezone** custom function, which is meant to resolve consideration *c)*.\r\n\r\n![Power Query create custom function Part A](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--16.png?raw=true)\r\n\r\n![Power Query create custom function Part B](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--17.png?raw=true)\r\n\r\n```\r\nlet\r\n  DatetimeToDatetimezone = (DateTimestamp as datetime, Offset as nullable text) => \r\n    DateTimeZone.FromText(DateTime.ToText(DateTimestamp) & " " & Offset)\r\nin\r\n  DatetimeToDatetimezone\r\n```\r\n\r\n![Power Query create 1st custom function](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--18.png?raw=true)\r\n\r\nNext we create the second custom function **AnchorToDatetimezone**, which takes in the dependent date/time value to get the year, and nullable parameters for time, offset, and the anchors for month, day, and *n*th occurrence of the day within the month. It checks the data structure to create the anchor\'s date, and appends the time and offset values passed into it.\r\n\r\n* The first if condition checks that there are no incorrect nor incomplete data structures.\r\n* The first else if condition creates the date time zone if there is a date anchor e.g. "21st" of given month.\r\n* The second and third else if conditions convert to date time zones based on 1st, 2nd, 3rd, or 4th occurrence of specified day anchors, and are based on this previous [post](https://datamesse.github.io/#/post/1632578400).\r\n* The fourth and fifth if else conditions convert based on the last occurrence of the specified day anchor, based on the last section of that same [post](https://datamesse.github.io/#/post/1632578400).\r\n\r\n```\r\nlet\r\n  AnchorToDatetimezone = (DateTimestamp as datetime, MonthAnchor as nullable number, DayAnchor as nullable number, PositionAnchor as nullable number, DateAnchor as nullable number, Time as nullable time, Offset as nullable text) => \r\n  /* Error-handling based on insufficient data or incorrect value combination */\r\n  if (MonthAnchor = null) \r\n    or (DateAnchor = null and DayAnchor = null)\r\n    or (DateAnchor <> null and DayAnchor <> null)\r\n    or (DayAnchor <> null and PositionAnchor = null)\r\n    or (DayAnchor = null and PositionAnchor <> null)\r\n    or (Time = null)\r\n    or (Offset = null)\r\n  then "Incomplete data"\r\n  /* Applying time zone to DateTimestamp, with separate conditions for position anchor = 9 i.e. "Last" */\r\n  else if DateAnchor <> null\r\n    then Text.From(Date.Year(DateTimestamp)) & "/" & Text.From(MonthAnchor)  & "/" & Text.From(DateAnchor) & " " & Text.From(Time) & Offset\r\n  else if DayAnchor < 6 and PositionAnchor > 0 and PositionAnchor < 5\r\n  /* Optional parameter in Date.DayOfWeek 1 = Day.Monday will get Sunday, hence DayAnchor (Sunday = 0) + 1 */\r\n    then Text.From(Date.Year(DateTimestamp)) & "/" & Text.From(MonthAnchor) & "/" & Text.From( (7 - Date.DayOfWeek(Date.FromText(Text.From(Date.Year(DateTimestamp)) & "/" & Text.From(MonthAnchor) & "/1"), DayAnchor + 1)) + (-7 + (7 * PositionAnchor)) ) & " " & Text.From(Time) & Offset\r\n  /* Need to pass Day.Sunday to get Saturday */\r\n  else if DayAnchor = 6 and PositionAnchor > 0 and PositionAnchor < 5\r\n    then Text.From(Date.Year(DateTimestamp)) & "/" & Text.From(MonthAnchor) & "/" & Text.From( (7 - Date.DayOfWeek(Date.FromText(Text.From(Date.Year(DateTimestamp)) & "/" & Text.From(MonthAnchor) & "/1"), Day.Sunday )) + (-7 + (7 * PositionAnchor)) ) & " " & Text.From(Time) & Offset\r\n  /* handling for last specific day of month */\r\n  else if DayAnchor = 0 and PositionAnchor = 9\r\n    then Text.From(Date.AddDays(Date.EndOfMonth(Date.From(Text.From(Date.Year(DateTimestamp)) & "/" & Text.From(MonthAnchor) & "/1")),(-1 * Number.From(Date.DayOfWeek(Date.EndOfMonth(Date.From(Text.From(Date.Year(DateTimestamp)) & "/" & Text.From(MonthAnchor) & "/1")), Day.Sunday))))) & " " & Text.From(Time) & Offset\r\n  else if DayAnchor > 0 and PositionAnchor = 9\r\n    then Text.From(Date.AddDays(Date.EndOfMonth(Date.From(Text.From(Date.Year(DateTimestamp)) & "/" & Text.From(MonthAnchor) & "/1")),(-1 * (Number.From(Date.DayOfWeek(Date.EndOfMonth(Date.From(Text.From(Date.Year(DateTimestamp)) & "/" & Text.From(MonthAnchor) & "/1")), Day.Sunday)) + ( 7 - DayAnchor ))))) & " " & Text.From(Time) & Offset\r\n  else null\r\nin\r\n  AnchorToDatetimezone\r\n```\r\n\r\n![Power Query create 2nd custom function](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--19.png?raw=true)\r\n\r\nThe third custom function **DatetimeAppendZone** uses the **AnchorToDatetimezone** custom function to create date/time anchors for the start and end of daylight savings, then compares them with the Date/Time value to determine if standard or daylight savings offsets should be suffixed to the Date/Time value, using the **DatetimeToDatetimezone** custom function.\r\n\r\nSome data sources may be incorrectly failing to account for daylight savings differences. In this third custom function, you can use the "Difference" parameter to add/subtract the missed hour(s) to compensate for those. The code below does not do that, but you can add it in, if required.\r\n\r\n```\r\nlet\r\n  DatetimeAppendZone = (DateTimestamp as datetime, Difference as number, StandardOffset as nullable text, DaylightOffset as nullable text, DSTstartAncDate as nullable number, DSTstartAncPosition as nullable number, DSTstartAncDay as nullable number, DSTstartAncMonth as nullable number, DSTstartAncUTC as nullable time, DSTstartAncLocal as nullable time, DSTendAncDate as nullable number, DSTendAncPosition as nullable number, DSTendAncDay as nullable number, DSTendAncMonth as nullable number, DSTendAncUTC as nullable time, DSTendAncLocal as nullable time) => \r\n  /* Validation to ensure same time anchor types for start and end are used */\r\n  if Difference <> 0 and ( (DSTstartAncLocal = null and DSTstartAncUTC = null) or (DSTendAncLocal = null and DSTendAncUTC = null) )\r\n    then "Incomplete data"\r\n\r\n  /* Where DST is not observed, just append Standard UTC offset */\r\n  else if Difference = 0\r\n    then DatetimeToDatetimezone(DateTimestamp, StandardOffset)\r\n\r\n  /* From this point on, if using a data source that doesn\'t properly account for Daylight Savings offsets, you can factor those into the calculations */\r\n\r\n  /* Where DST is observed with Standard time result */\r\n  else if Difference <> 0\r\n    and (\r\n          (\r\n           /* Where local offset is used, 1 DST period in same year, datetimestamp is outside daylight savings */\r\n           DSTstartAncLocal <> null and DSTstartAncMonth < DSTendAncMonth\r\n           and ( DatetimeToDatetimezone(DateTimestamp,StandardOffset) < DateTimeZone.From(AnchorToDatetimezone(DateTimestamp, DSTstartAncMonth, DSTstartAncDay, DSTstartAncPosition, DSTstartAncDate, DSTstartAncLocal, StandardOffset))\r\n                or DatetimeToDatetimezone(DateTimestamp,StandardOffset) > DateTimeZone.From(AnchorToDatetimezone(DateTimestamp, DSTendAncMonth, DSTendAncDay, DSTendAncPosition, DSTendAncDate, DSTendAncLocal, StandardOffset)) )\r\n          )\r\n      or  (\r\n           /* Where local offset is used, 2 DST periods in same year, datetimestamp is outside both daylight savings periods */\r\n           DSTstartAncLocal <> null and DSTstartAncMonth > DSTendAncMonth \r\n           and Date.Month(DateTimestamp) >= DSTendAncMonth and Date.Month(DateTimestamp) <= DSTstartAncMonth\r\n           and DatetimeToDatetimezone(DateTimestamp,DaylightOffset) > DateTimeZone.From(AnchorToDatetimezone(DateTimestamp, DSTendAncMonth, DSTendAncDay, DSTendAncPosition, DSTendAncDate, DSTendAncLocal, DaylightOffset))\r\n           and DatetimeToDatetimezone(DateTimestamp,DaylightOffset) < DateTimeZone.From(AnchorToDatetimezone(Date.AddYears(DateTimestamp,1), DSTstartAncMonth, DSTstartAncDay, DSTstartAncPosition, DSTstartAncDate, DSTstartAncLocal, DaylightOffset))           \r\n          )\r\n      or  (\r\n           /* Where UTC offset is used, 1 DST period in same year, datetimestamp is inside daylight savings */\r\n           DSTstartAncUTC <> null and DSTstartAncMonth < DSTendAncMonth\r\n           and ( DateTimeZone.ToUtc(DatetimeToDatetimezone(DateTimestamp,StandardOffset)) <  DateTimeZone.FromText(AnchorToDatetimezone(DateTimestamp, DSTstartAncMonth, DSTstartAncDay, DSTstartAncPosition, DSTstartAncDate, DSTstartAncUTC, "+00:00"))\r\n                 or DateTimeZone.ToUtc(DatetimeToDatetimezone(DateTimestamp,StandardOffset)) > DateTimeZone.FromText(AnchorToDatetimezone(DateTimestamp, DSTendAncMonth, DSTendAncDay, DSTendAncPosition, DSTendAncDate, DSTendAncUTC, "+00:00")) )\r\n          )\r\n    )\r\n    then DatetimeToDatetimezone(DateTimestamp, StandardOffset)\r\n\r\n  /* Where DST is observed with Dayliht Saving time result */\r\n  else if Difference <> 0\r\n    and (\r\n          (\r\n           /* Where local offset is used, 1 DST period within same year, datetimestamp is inside daylight savings */    \r\n           DSTstartAncLocal <> null and DSTstartAncMonth < DSTendAncMonth\r\n           and ( DatetimeToDatetimezone(DateTimestamp,StandardOffset) >= DateTimeZone.From(AnchorToDatetimezone(DateTimestamp, DSTstartAncMonth, DSTstartAncDay, DSTstartAncPosition, DSTstartAncDate, DSTstartAncLocal, StandardOffset))\r\n                or DatetimeToDatetimezone(DateTimestamp,StandardOffset) <= DateTimeZone.From(AnchorToDatetimezone(DateTimestamp, DSTendAncMonth, DSTendAncDay, DSTendAncPosition, DSTendAncDate, DSTendAncLocal, StandardOffset)) )\r\n          )\r\n      or  (\r\n           /* Where local offset is used, 2 DST periods in same year, datetimestamp is inside 1st daylight savings period */\r\n           DSTstartAncLocal <> null and DSTstartAncMonth > DSTendAncMonth and Date.Month(DateTimestamp) <= DSTendAncMonth\r\n           and DatetimeToDatetimezone(DateTimestamp,DaylightOffset) <= DateTimeZone.From(AnchorToDatetimezone(DateTimestamp, DSTendAncMonth, DSTendAncDay, DSTendAncPosition, DSTendAncDate, DSTendAncLocal, DaylightOffset))\r\n          )\r\n      or  (\r\n           /* Where local offset is used, 2 DST periods in same year, datetimestamp is inside 2nd daylight savings period */\r\n           DSTstartAncLocal <> null and DSTstartAncMonth > DSTendAncMonth and Date.Month(DateTimestamp) >= DSTendAncMonth\r\n           and DatetimeToDatetimezone(DateTimestamp,DaylightOffset) >= DateTimeZone.From(AnchorToDatetimezone(DateTimestamp, DSTstartAncMonth, DSTstartAncDay, DSTstartAncPosition, DSTstartAncDate, DSTstartAncLocal, DaylightOffset))\r\n          )\r\n      or  (\r\n           /* Where UTC offset is used, 1 DST period in same year, datetimestamp is outside daylight savings */\r\n           DSTstartAncUTC <> null and DSTstartAncMonth < DSTendAncMonth\r\n           and DateTimeZone.ToUtc(DatetimeToDatetimezone(DateTimestamp,StandardOffset)) >=  DateTimeZone.From(AnchorToDatetimezone(DateTimestamp, DSTstartAncMonth, DSTstartAncDay, DSTstartAncPosition, DSTstartAncDate, DSTstartAncUTC, "+00:00"))\r\n           and DateTimeZone.ToUtc(DatetimeToDatetimezone(DateTimestamp,StandardOffset)) <= DateTimeZone.From(AnchorToDatetimezone(DateTimestamp, DSTendAncMonth, DSTendAncDay, DSTendAncPosition, DSTendAncDate, DSTendAncUTC, "+00:00")) \r\n          )\r\n    )\r\n    then DatetimeToDatetimezone(DateTimestamp, DaylightOffset)\r\n  else null\r\nin\r\n  DatetimeAppendZone\r\n```\r\n\r\n![Power Query create 3rd custom function](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--20.png?raw=true)\r\n\r\nNow that our functions are complete, we can use the third **DatetimeAppendZone** to create the custom column which converts Date/Time to a datetimezone value, by passing in all the respective anchors as parameters.\r\n\r\n```\r\nDatetimeAppendZone([#"Date/Time"],[#"Daylight offset - Standard offset"],[Standard UTC offset],[Daylight Saving UTC offset],[#"DST start (date anchor)"],[#"DST start (position anchor)"],[#"DST start (day anchor)"],[#"DST start (month anchor)"],[#"DST start (UTC time anchor)"],[#"DST start (local time anchor)"],[#"DST end (date anchor)"],[#"DST end (position anchor)"],[#"DST end (day anchor)"],[#"DST end (month anchor)"],[#"DST end (UTC time anchor)"],[#"DST end (local time anchor)"])\r\n```\r\n\r\n![Power BI Create report](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--21.png?raw=true)\r\n\r\nIf we now create a table in the Power BI\'s report designer, we can see the appropriate suffixing of standard vs daylight saving offsets to our Date/Time values as we change the parameter. Then we can test to see if, regardless of the data structure used for the daylight savings anchoring, that the appropriate offset is applied to our Date/Time dataset.\r\n\r\nAustralia, Sydney is a time zone that uses local time and non-"last position" for its daylight savings start and end anchors.\r\n![Power BI Sydney example Part A](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--22a.png?raw=true)\r\n\r\n![Power BI Sydney example Part B](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--22.png?raw=true)\r\n\r\nEurope, Dublin is a time zone that uses UTC time and "last position" (indicated by the 9), for its anchors.\r\n![Power BI Dublin example Part A](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--23a.png?raw=true)\r\n\r\n![Power BI Dublin example Part B](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--23.png?raw=true)\r\n\r\nAfrica, Casablanca is a time zone that uses a fixed date of the month for its anchors.\r\n![Power BI Casablanca example Part A](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--24a.png?raw=true)\r\n\r\n![Power BI Casablanca example Part B](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-10--24.png?raw=true)\r\n\r\nSo now we have created a parameterised way of defining our data source\'s time zone, with potential to compensate for skipped daylight savings conversions it may have, by using the concept of anchor values to construct relative date/times.\r\n\r\nUse cases for this include:\r\n1. Dynamically converting date/times to another time zone e.g. a report reader from one time zone wanting to know the date/times from the perspective of another time zone.\r\n2. The admittedly rare instance where the time zone of date/times may vary at access or export when creating a datset.\r\n\r\nAn example where I encountered use case #2, was manually exporting data from Explore, the reporting tool for the Zendesk customer service platform, where date/times are automatically converted to match the time zone of the extractor\'s Zendesk login.\r\n\r\nIn that scenario, if multiple people from different time zones are creating or maintaining dashboards not made in the native Explore tool (e.g. via Power BI), their extract date times can be inconsistent.\r\n\r\nThere are simple ways around this:\r\n* Creating a shared account fixed to a specific time zone for data extracts.\r\n* Having access to the data source\'s API.\r\n\r\nYou can take this further for other solutions, such as hard-coding the desired time zone, or make it based on the values of another column e.g. time zone is based on country or city values.\r\n\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2021-10-10.md)** for this post\'s markdown file in GitHub.'},{id:1633183200,title:"Import time zone offsets and observations from Wikipedia in Power BI",tag:"logo-powerbi",date:"3 October 2021",content:'\r\nHow to use Power BI to scrape Wikipedia pages and create a data source for UTC time zone offsets and daylight saving observation anchors (e.g. the first Sunday of October).\r\n\r\nHere we will be importing Wikipedia table data from 2 different pages. The first example contains structured data values requiring minimal data cleaning. The second contains data which requires disaggregation of qualitative information to make it more quantitative.\r\n\r\nTime zone offset hours\r\n[https://en.wikipedia.org/wiki/List_of_tz_database_time_zones](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones)\r\n\r\n![Wikipedia List of tz database time zones](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--01.png?raw=true)\r\n\r\nDaylight Saving observation period anchors\r\n[https://en.wikipedia.org/wiki/Daylight_saving_time_by_country](https://en.wikipedia.org/wiki/Daylight_saving_time_by_country)\r\n\r\n![Wikipedia Daylight saving time by country](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--02.png?raw=true)\r\n\r\n**Exercise 1:**\r\n\r\nBeginning with the time zone offset hours, we Get Data from Web and provide the URL.\r\n\r\n![Power BI Import data from a web page](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--03.png?raw=true)\r\n\r\nSelect the Basic option. The intention is to export the results to Excel, as opposed to a live ongoing connection. This is to mitigate problems regarding source page changes and connection delays.\r\n\r\n![Power BI Import data Basic setting and set URL](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--04.png?raw=true)\r\n\r\nThe HTML table we are after is the list containing the offsets.\r\nTick it, then click Transform Data.\r\n\r\n![Power BI Import data web page table selection](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--05.png?raw=true)\r\n\r\nNext we Use First Row as Headers to assign the column names.\r\n\r\n![Power Query Use First Row as Headers](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--06.png?raw=true)\r\n\r\nThen we add new custom columns to substitute existing columns to clean the data.\r\n\r\nFirst we add a new column to substitute the TZ database name column, replacing the single forward slashes \u201c/\u201d with forward slashes surrounded by spaces \u201c / \u201c, and replace the underscores \u201c_\u201d with spaces \u201c \u201c, for readability.\r\n\r\n```\r\nText.Replace(Text.Replace([TZ database name],"/", ", "),"_"," ")\r\n```\r\n\r\n![Power Query Replace text to make more readable](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--07.png?raw=true)\r\n\r\nSecondly, the data source\u2019s offsets use a different dash character "\u2212" (slightly longer) from the mathematical operator "-" (shorter), so we need to create custom columns to substitute the longer dash with the shorter one.\r\n\r\nFor the Standard UTC offset:\r\n\r\n```\r\nText.Replace([#"UTC offset \xb1hh:mm"],"\u2212","-")\r\n```\r\n![Power Query Custom Column: Standard UTC offset](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--08.png?raw=true)\r\n\r\nFor the Daylight Saving UTC offset:\r\n\r\n```\r\nText.Replace([#"UTC DST offset \xb1hh:mm"],"\u2212","-")\r\n```\r\n![Power Query Custom Column: Daylight Saving UTC offset](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--09.png?raw=true)\r\n\r\nNow to add a column that shows the difference between the standard and daylight savings offsets.\r\nThe nature of the data means you cannot subtract them in a simple way.\r\nComments are included in the code to explain what is occurring.\r\n\r\n```\r\n/* If the offsets are identical, it may imply no Daylight Saving observed */\r\nif [Standard UTC offset] = [Daylight Saving UTC offset]\r\nthen 0\r\n\r\n/* If minutes are the same and aren\'t zero, just subtract hours */\r\nelse if (Text.End([Standard UTC offset],2) <> "00" or \r\nText.End([Daylight Saving UTC offset],2) <> "00") and Text.End([Standard UTC offset],2) = Text.End([Daylight Saving UTC offset],2)\r\nthen Number.FromText(Text.Range([Daylight Saving UTC offset],0,3)) - Number.FromText(Text.Range([Standard UTC offset],0,3))\r\n\r\n/* If minutes are different and either of them aren\'t zero, convert minutes to proper decimals, subtract, then convert minutes back */\r\nelse if (Text.End([Standard UTC offset],2) <> "00" or \r\nText.End([Daylight Saving UTC offset],2) <> "00") and Text.End([Standard UTC offset],2) <> Text.End([Daylight Saving UTC offset],2)\r\nthen (Number.FromText(Text.Range([Daylight Saving UTC offset],1,2)) + (Number.FromText(Text.End([Daylight Saving UTC offset],2)) / 60)) - (Number.FromText(Text.Range([Standard UTC offset],1,2)) + (Number.FromText(Text.End([Standard UTC offset],2)) / 60))\r\n\r\n/* Standard expectation that difference is only in the hour values */\r\nelse Number.FromText(Text.Range([Daylight Saving UTC offset],1,2)) - Number.FromText(Text.Range([Standard UTC offset],1,2))\r\n```\r\n\r\n![Power Query Custom Column: Offset difference](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--10.png?raw=true)\r\n\r\nNext we filter out the data rows not required, starting with only including Canonical status offsets\r\n\r\n![Power Query Filter for Canonical records](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--11.png?raw=true)\r\n\r\nThen we filter for time zones that have a country code.\r\n\r\n![Power Query Filter for country codes](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--12.png?raw=true)\r\n\r\nLastly, remove columns that won\u2019t be needed, depending on what you need for your data source.\r\n\r\n![Power Query Remove other columns](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--13.png?raw=true)\r\n\r\nIn my scenario, I want to retain this data separately in an Excel file, so I create a table in Power BI with all the columns, then Export.\r\n\r\n![Power BI Export table results](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--14.png?raw=true)\r\n\r\n\r\n\r\n**Exercise 2:**\r\n\r\nFor our second dataset, we need to retrieve the relative anchors for daylight saving periods using the second URL: [https://en.wikipedia.org/wiki/Daylight_saving_time_by_country](https://en.wikipedia.org/wiki/Daylight_saving_time_by_country)\r\n\r\n![Power BI Import data Basic setting and set URL](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--15.png?raw=true)\r\n\r\nAgain, click Transform Data and Use First Row as Headers.\r\n\r\n![Power BI Import data web page table selection](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--16.png?raw=true)\r\n\r\n![Power Query Use First Row as Headers](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--17.png?raw=true)\r\n\r\nNext we filter for records with a valid current DST start value.\r\n\r\n![Power Query Filter for valid DST start](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--18.png?raw=true)\r\n\r\nThe problem with this dataset is that the DST start and DST end columns are not quantified at a low enough level to be easily worked with. Revising these columns, we can see value commonalities that can be separated out into custom columns as declartive \u201canchors\u201d for each daylight saving period\u2019s start and end.\r\n\r\nThis includes:\r\n* Positions (i.e. first, second, third, fourth, last)\r\n* Weekday names\r\n* Month names\r\n* \u201cUTC\u201d prefixed with a specific UTC time (e.g. 01:00 UTC), or prefixed with a non-UTC time (e.g. 002:00 AST (UTC-4)\r\n* Phrases \u201clocal standard time\u201d and \u201clocal daylight saving time\u201d prefixed with a time\r\n\r\n![Power Query Exploring qualitative data values](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--19.png?raw=true)\r\n\r\nThe custom columns are base conditions that check for substrings and substitute them with numerical data where possible, so they can be referenced by other custom functions.\r\n\r\nWe\u2019ll begin with the position-related column.\r\n\r\nEvery day has at least four occurrences in each month, but the \u201clast\u201d position could either be the fourth or fifth occurrence of that day. I chose to use an arbitrary value of 9 for the output of last, given 5 could possibly be used for the fifth instance of the day. Note: With this particular dataset, neither Fourth nor Fifth occur, so they can be omitted here if you want.\r\n\r\n```\r\nif Text.Contains([DST start], "First")\r\nthen 1\r\nelse if Text.Contains([DST start], "Second")\r\nthen 2\r\nelse if Text.Contains([DST start], "Third")\r\nthen 3\r\nelse if Text.Contains([DST start], "Fourth")\r\nthen 4\r\nelse if Text.Contains([DST start], "Fifth")\r\nthen 5\r\nelse if Text.Contains([DST start], "Last")\r\nthen 9\r\nelse null\r\n```\r\n\r\n![Power Query Custom Column: DST start position anchor](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--20.png?raw=true)\r\n\r\nNext to create a custom column for the weekdays, using Power Query\u2019s Day functions, which translate as numbers from 0 for Sunday to 6 for Saturday.\r\n\r\n```\r\nif Text.Contains([DST start], "Sunday")\r\nthen Day.Sunday\r\nelse \r\nif Text.Contains([DST start], "Monday")\r\nthen Day.Monday\r\nelse \r\nif Text.Contains([DST start], "Tuesday")\r\nthen Day.Tuesday\r\nelse \r\nif Text.Contains([DST start], "Wednesday")\r\nthen Day.Wednesday\r\nelse \r\nif Text.Contains([DST start], "Thursday")\r\nthen Day.Thursday\r\nelse \r\nif Text.Contains([DST start], "Friday")\r\nthen Day.Friday\r\nelse \r\nif Text.Contains([DST start], "Saturday")\r\nthen Day.Saturday\r\nelse null\r\n```\r\n\r\n![Power Query Custom Column: DST start day anchor](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--21.png?raw=true)\r\n\r\nWe repeat with similar logic for the DST start field\u2019s month anchor. At time of writing, Power Query has a function that converts month numbers to month names, but not the other way around.\r\n\r\n```\r\nif Text.Contains([DST start], "January")\r\nthen 1\r\nelse if Text.Contains([DST start], "February")\r\nthen 2\r\nelse if Text.Contains([DST start], "March")\r\nthen 3\r\nelse if Text.Contains([DST start], "April")\r\nthen 4\r\nelse if Text.Contains([DST start], "May")\r\nthen 5\r\nelse if Text.Contains([DST start], "June")\r\nthen 6\r\nelse if Text.Contains([DST start], "July")\r\nthen 7\r\nelse if Text.Contains([DST start], "August")\r\nthen 8\r\nelse if Text.Contains([DST start], "September")\r\nthen 9\r\nelse if Text.Contains([DST start], "October")\r\nthen 10\r\nelse if Text.Contains([DST start], "November")\r\nthen 11\r\nelse if Text.Contains([DST start], "December")\r\nthen 12\r\nelse null\r\n```\r\n\r\n![Power Query Custom Column: DST start month anchor](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--22.png?raw=true)\r\n\r\nLastly, we will pull where a UTC time is specified. There are entries where a local time with its UTC offset value is provided, but since these entries are few and complex to manage, I will edit the export result afterward to account for these. It\u2019s a cost vs benefit juggle.\r\n\r\n```\r\nif Text.Contains([DST start], " UTC") then Text.Range([DST start], Text.PositionOf([DST start]," UTC") - 5, 5) else null\r\n```\r\n\r\n![Power Query Custom Column: DST start UTC time anchor](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--23.png?raw=true)\r\n\r\nRinse-and-repeat the creation of those anchor columns. An alternative is creating a custom function to make it easier to manage later on.\r\n\r\n![Power Query Custom Columns for DST end](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--24.png?raw=true)\r\n\r\nThen we can retain the columns we need, such as Country/Territory, Notes, and the custom columns we created.\r\n\r\n![Power Query Remove other columns](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--25.png?raw=true)\r\n\r\nLastly, as with the previous dataset, we will export this to Excel, and clean up the file from there, e.g. accounting for records that have a different data structure for their anchors, such as an exact date for day and month per year, and records that include local time, etc.\r\n\r\n![Power BI Export table results](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--26.png?raw=true)\r\n\r\nAt this point, we typically would merge these datasets, similar to left outer joins in SQL. Unfortunately, the first dataset uses an incoherent structure for its time zone name values, e.g. _country, city_ or _region, city_ or _region, country, city_ etc., as opposed to the second data set, which only lists country.\r\n\r\n![Power Query Merge Queries](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--27.png?raw=true)\r\n\r\nI tried fuzzy matching, but as at time of writing, it cannot connect a high enough number of the records, regardless of adjustments made to the accuracy.\r\n\r\n![Power Query Merge using fuzzy matching](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--28.png?raw=true)\r\n\r\nAn alternative solution would be to create a list based on the second dataset\u2019s county column, but this would neglect the _region, city_ joins from the first dataset. Another would be to find a third dataset to extend the other datasets and formulate a common column for the merge.\r\n\r\nIn my scenario, it would be more time efficient to do the mapping manually, as this dataset is small, and intended for a niche non-scaled need. \r\n\r\nFind a copy of the end product to download as an Excel file **[here](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/Time_zone_offsets_and_DST_observations.xlsx?raw=true)**.\r\n\r\nAs a reminder, this is strictly an exercise file, and its data is not comprehensive nor accurate, so please be mindful of that if using it.\r\n\r\n![Manually cleaned output](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-10-03--29.png?raw=true)\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2021-10-03.md)** for this post\'s markdown file in GitHub.'},{id:1632578400,title:"Find date for the nth day of a month in Power BI",tag:"logo-powerbi",date:"26 September 2021",content:'\r\nHow to use Power Query to find the date for the nth day of a month/year based on another date column (e.g. 3rd Tuesday of October 2021).\r\n\r\nIn Power BI this can be used for the conditional logic of other Custom Columns. For example, to create indicators that data rows occur on or fall between relative date ranges (e.g. Black Friday sales). The following involves adding a Custom Column in Power Query i.e. M code, not DAX.\r\n\r\nThis finds the first Monday of the month, where our dependent date column is OurDateField.\r\n\r\n```\r\nDate.FromText(Text.From(Date.Year([OurDateField])) & "/10/" & Text.From((7 - (Date.DayOfWeek(Date.FromText(Text.From(Date.Year([OurDateField])) & "/10/1"),Day.Monday)))))\r\n```\r\n\r\n![Power Query: 1st Sunday of month](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--01.png?raw=true)\r\n\r\n**How it works**\r\n\r\nTo find the first Sunday of a specific month/year relative to another date, first establish the start of the month e.g. 1/10 (1st October), passing in the date field you are using e.g. [OurDateField], to append its year.\r\n\r\n```\r\nDate.FromText(Text.From(Date.Year([OurDateField])) & "/10/1")\r\n```\r\n\r\nIn this example, we are hard-coding October regardless of OurDateField\u2019s month value, but if you need it to be relative to its month too, simply add an extra concatenation for month in the same way year is treated, i.e. using Date.Month().\r\n\r\nNow we need to identify what day of the week that this first day of the month is, using Date.DayOfWeek, and setting the optional parameter for what the start of the week is, as Day.Monday\r\n\r\n```\r\nDate.DayOfWeek(Date.FromText(Text.From(Date.Year([OurDateField])) & "/10/1"),Day.Monday)\r\n```\r\n\r\n![Power Query: Day of week number](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--02.png?raw=true)\r\n\r\n \r\nIn this example, 1st October 2021 is a Friday, and Friday\u2019s day number is 4 (with Monday being 0).\r\n\r\n![Calendar: Weekday of 1st day of month](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--03.png?raw=true)\r\n\r\nIf you do not provide the Day.Monday parameter, it will default to Day.Monday in the background. If another parameter is used e.g. Day.Sunday, then the assignment numbers will change.\r\n\r\nNow we subtract the weekday number 4 from 7, and get 3, which is the first Sunday\u2019s date.\r\n\r\n```\r\n7 - (Date.DayOfWeek(Date.FromText(Text.From(Date.Year([OurDateField])) & "/10/1"),Day.Monday))\r\n```\r\n\r\n![Power Query: Date of 1st Sunday](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--04.png?raw=true)\r\n\r\nThen concatenate this with the year month retrieved earlier\r\n\r\n```\r\nDate.FromText(Text.From(Date.Year([OurDateField])) & "/10/" & Text.From((7 - (Date.DayOfWeek(Date.FromText(Text.From(Date.Year([OurDateField])) & "/10/1"),Day.Monday)))))\r\n```\r\n\r\n![Power Query: Concatenate the month year to the date](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--05.png?raw=true)\r\n\r\nIf you need to change the weekday that Power Query needs to find, simply increment the Day.Monday parameter to the following day of the desired weekday.\r\n\r\nFor example, if you want to find the first Wednesday, change the parameter to Day.Thursday.\r\n\r\n```\r\nDate.FromText(Text.From(Date.Year([OurDateField])) & "/10/" & Text.From((7 - (Date.DayOfWeek(Date.FromText(Text.From(Date.Year([OurDateField])) & "/10/1"),Day.Thursday)))))\r\n```\r\n\r\n![Power Query: 1st Wednesday of month](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--06.png?raw=true)\r\n\r\n![Calendar: 1st Wednesday of the month](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--07.png?raw=true)\r\n\r\nIf you need to change the position from first, to second, third, or fourth Sunday, simply add 7 for the second, 14 for the third, and 21 for the fourth.\r\n\r\nFor example, we will retrieve the 3rd Sunday.\r\n\r\n```\r\nDate.FromText(Text.From(Date.Year([OurDateField])) & "/10/" & Text.From((7 - (Date.DayOfWeek(Date.FromText(Text.From(Date.Year([OurDateField])) & "/10/1"),Day.Monday)) + 14 )))\r\n```\r\n\r\n![Calendar: 3rd Sunday of the month](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--08.png?raw=true)\r\n\r\n\r\n\r\n**Edit: 9th October 2021**\r\n\r\nIf the last occurrence of a specific day in a month needs to be retrieved, it can possibly be the 4th or 5th instance of that day. Retrieving this may be required for conditional or other custom column dependencies. \r\n\r\nAs an example, this Power Query code finds the last Sunday of the month, where our dependent date column is OurDateField.\r\n\r\n```\r\nDate.AddDays(Date.EndOfMonth([OurDateField]),(-1 * Number.From(Date.DayOfWeek(Date.EndOfMonth([OurDateField]), Day.Sunday))))\r\n```\r\n![Power Query Day number of last of the month year Day.Sunday](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--12.png?raw=true)\r\n\r\n**How it works**\r\n\r\nWe will try to retrieve the last Sunday of a specific month/year, passing in our relative *OurDateField*.\r\n\r\n```\r\nDate.EndOfMonth([OurDateField])\r\n```\r\n![Power Query last day of the month year](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--09.png?raw=true)\r\n\r\nThen find out which day of the week it is.\r\n\r\n```\r\nDate.DayOfWeek(Date.EndOfMonth([OurDateField]), Day.Monday)\r\n```\r\n![Power Query Day number of last of the month year Day.Monday](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--10.png?raw=true)\r\n\r\nIn this example, 31st January 2021 is a Sunday, and Sunday\u2019s day number is 6. This is if the optional parameter for start of the week is Day.Monday (which is the default, if not provided).\r\n\r\n![Calendar using Day.Monday](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--11.png?raw=true)\r\n\r\nNow we will swap that Day.Monday parameter out with Day.Sunday, so that the value for Sunday becomes 0 instead of 6.\r\n\r\n```\r\nDate.DayOfWeek(Date.EndOfMonth([OurDateField]), Day.Sunday)\r\n```\r\n![Power Query Day number of last of the month year Day.Sunday](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--12.png?raw=true)\r\n\r\n![Calendar using Day.Sunday](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--13.png?raw=true)\r\n\r\nWe then minus this number from the last date of the month, to get the last Sunday of the month, whih ironically is the same day i.e. Sunday 31/01/2021 - 0 = 31/01/2021. We do this using the Date.AddDays function and multiplying the number with -1.\r\n\r\n```\r\nDate.AddDays(Date.EndOfMonth([OurDateField]),(-1 * Number.From(Date.DayOfWeek(Date.EndOfMonth([OurDateField]), Day.Sunday))))\r\n```\r\n![Power Query last date Sunday](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--14.png?raw=true)\r\n\r\nLastly, if you need to pick any other last specific day of the month, just increment by one for each day you want to go earlier in the week e.g. Saturday is +1, Friday is +2 etc.\r\n\r\nFor example, the last Friday of the month year.\r\n\r\n```\r\nDate.AddDays(Date.EndOfMonth([OurDateField]),(-1 * (Number.From(Date.DayOfWeek(Date.EndOfMonth([OurDateField]), Day.Sunday)) + 2)))\r\n```\r\n![Power Query last date Friday](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--15.png?raw=true)\r\n\r\n![Power Query last date Friday](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2021-09-26--16.png?raw=true)\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2021-09-26.md)** for this post\'s markdown file in GitHub.'},{id:1326286800,title:"International Marketplace dataset created with Integration Services",tag:"logo-sqlserver",date:"12 January 2022",content:"\r\nSQL Server Integration Services (SSIS) was used to create this fictional dataset, by merging Microsoft's *Wide World Importers* database and *Contoso* data warehouse, with Tableau's *Sample - APAC Superstore* dataset, with some data alterations.\r\n\r\n![International Marketplace SSIS package](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--01.png?raw=true)\r\n\r\n![International Marketplace in Power BI sales by city](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--00.png?raw=true)\r\n\r\n**Final output**\r\n\r\nYou can download Excel copies of the completed dataset:\r\n\r\n* Star schema (for Power BI data visualisation) **[download here](https://github.com/datamesse/data-visualisation-datasets/raw/main/International%20Marketplace%20sales/International%20Marketplace%20Normalised%20for%20Power%20BI.xlsx)**.\r\n\r\n![International Marketplace: Normalised star schema](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--02.png?raw=true)\r\n\r\n* Denormalised (for Tableau data visualisation) **[download here](https://github.com/datamesse/data-visualisation-datasets/raw/main/International%20Marketplace%20sales/International%20Marketplace%20Denormalised%20for%20Tableau.xlsx)**.\r\n\r\n![International Marketplace: Denormalised](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--03.png?raw=true)\r\n\r\nThis blog post outlines the planning and challenges in making the dataset, which was my first attempt at creating an SSIS package from scratch. For information on the data flows and SQL scripts used in the package, visit the **[GitHub repository](https://github.com/datamesse/data-visualisation-datasets/tree/main/International%20Marketplace%20sales)**.\r\n\r\n**REVIEWING THE ORIGINAL DATA SOURCES**\r\n\r\n**APAC Superstore**\r\n\r\nTableau's APAC Superstore dataset can be found and extracted from Tableau Desktop's *Saved Data Sources*.\r\n\r\n - Customers: 795 (by name), 5,220 (by name and city)\r\n - Products: 1,913\r\n - Countries: 23\r\n - Cities: 537\r\n - Sales records: 10,925\r\n - Sales years: 2018 to 2021\r\n\r\nThis is a succinct and denormalised dataset, where customers have no personal locations, as the same names are replicated across many countries. The names are also not very ethnically diverse, given the name of the dataset.\r\n\r\n![APAC Superstore order map](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--04.png?raw=true)\r\n\r\n![APAC Superstore data source](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--05.png?raw=true)\r\n\r\nAt the time of writing this blog post, the order dates covered are from 2018 to 2021. I'm highlighting that because I suspect that unlike the other datasets, Tableau's dataset has its dates automatically changed over time, and the SSIS package we create needs to accommodate for that.\r\n\r\nThe structure of the dataset merge will be based on APAC Superstore, since it has the least dimensions and facts compared to the Microsoft ones, which are proper databases.\r\n\r\n\r\n**Wide World Importers**\r\n\r\nMicrosoft's Wide World Importers database backup (.bak) file can be downloaded from here:\r\n**[https://github.com/Microsoft/sql-server-samples/releases/tag/wide-world-importers-v1.0](https://github.com/Microsoft/sql-server-samples/releases/tag/wide-world-importers-v1.0)**\r\n\r\n - Customers: 663\r\n - Products: 227\r\n - Countries: 1\r\n - Cities: 655\r\n - Sales records: 228,265\r\n - Sales years: 2013 to 2016\r\n\r\nThis is an extensive normalised dataset with customers based only in the United States. Each customer only exists in one city, but there are customer names for the same corporation in different cities e.g.\r\n - Tailspin Toys (Arietta, NY)\r\n - Tailspin Toys (Trentwood, WA)\r\nThese make up 60.6% of the customer records (402 out of 663).\r\n\r\n![Wide World Importers customer map](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--06.png?raw=true)\r\n\r\nWhilst 263 customer records have them based in a specific city, most have invoices for multiple other cities, presumably reflecting B2B sales.\r\n\r\n![Wide World Importers tables](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--07.png?raw=true)\r\n\r\n\r\n**Contoso BI Demo Dataset for Retail Industry**\r\n\r\nMicrosoft's Contoso BI Demo Dataset for Retail Industry data warehouse .bak file can be downloaded from here:\r\n**[https://www.microsoft.com/en-us/download/details.aspx?id=18279](https://www.microsoft.com/en-us/download/details.aspx?id=18279)**\r\n\r\n - Customers: 18,785 (by name) 18,868 (by name and city)\r\n - Products: 2,516\r\n - Countries: 29\r\n - Cities: 476\r\n - Sales records: 3,324,410 (online sales only)\r\n - Sales years: 2007 to 2009\r\n\r\n![Contoso customer map](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--08.png?raw=true)\r\n\r\nThis is the largest of the 3 datasets in terms of the number of sales records. Each sales row represents a single unit for a product, meaning aggregations need to be calculated where the same product is sold multiple times under the same order number.\r\n\r\nSo a lot of record removal is needed to bring down the dataset to a more manageable size, only using the latest order number for each customer (retaining multiple order line rows), which will leave a usable 19 thousand records.\r\n\r\n - Customers: 18,674 (by name) 18,753 (by name and city)\r\n - Products: 463\r\n - Countries: 29\r\n - Cities: 476\r\n - Sales records: 19,419\r\n - Sales years: 2007 to 2009\r\n\r\nContoso also has a lot of trailing spaces in values, which need to be trimmed as part of the ETL's data cleaning. Geographic corrections also need to be made, for example, the city Perth is incorrectly listed under South Australia.\r\n\r\n![Contoso tables](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--09.png?raw=true)\r\n\r\nThe table below shows a summary of how the dataset columns align as-is, and the T-SQL needed to populate respective columns with existing data.\r\n\r\n![Comparing columns across the 3 datasets](https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-blog/2022-01-12--10.png?raw=true)\r\n\r\n**CHALLENGES**\r\n\r\n**Region inconsistency**\r\n\r\nWide World Importers has a comprehensive Country table for Region and Subregion. We will remap the countries from APAC Superstore and Contoso to those regions for consistency.\r\n\r\n**Support for special characters**\r\n\r\nA large number of products use commas, and many names, states, cities and the like, use special characters, which I found to store accurately in Unicode tab-delimited text files, as opposed to some other file format types which cannot render the characters correctly. So both the APAC Superstore, and the remapping files use this file format.\r\n\r\n**Customer names are incoherent and do not reflect country-of-origin**\r\n\r\nThe APAC Superstore and Contoso datasets have customers outside the United States, but their names do not reflect those locations. Using my past experience with country-based name randomisation (plugging my previous blog post **[here](https://datamesse.github.io/#/post/1635598800)**), I created customer name re-mapping files **[here](https://github.com/datamesse/data-visualisation-datasets/tree/main/International%20Marketplace%20sales/text%20files%20for%20value%20remapping%20in%20SSIS)**.\r\n\r\nContoso also has company names with a missing space before the \"Company\" suffix. As these are just the city with \"Company\" suffixed, they are remapped with a person's name instead.\r\n\r\n**Mostly United States sales**\r\n\r\nWide World Importers is a United States sales dataset, so the U.S. sales data from Contoso will be remapped to other countries, to give the final dataset more global reach. The geographic remapping file for this is **[here](https://github.com/datamesse/data-visualisation-datasets/tree/main/International%20Marketplace%20sales/text%20files%20for%20value%20remapping%20in%20SSIS)**.\r\n\r\n**Product, Category/Subcategory, and Supplier remapping**\r\n\r\nThere are shared _Supplier_ names across Contoso and Wide World Importers, but the spelling needs to be amended so they align. World Wide Importers uses _Stock Groups_, where each product item can belong to multiple groups (1:M), whereas APAC Superstore and Contoso have 1:1 Category and Sub-Category for their products. So all of these have been remapped and re-classified as a whole. Remapping files are **[here](https://github.com/datamesse/data-visualisation-datasets/tree/main/International%20Marketplace%20sales/text%20files%20for%20value%20remapping%20in%20SSIS)**.\r\n\r\n**Customer segmentation**\r\n\r\nAPAC Superstore defines its segmentation by Consumer, Home Office, and Corporate. International Marketplace merges Home Office with Consumer, as Contoso uses Person and Company for its segmentations. So these are easily remapped:\r\n\r\n```\r\ncase\r\n   when c.CustomerType = 'Person' then N'Consumer'\r\n   when c.CustomerType = 'Company' then N'Corporate'\r\n   else null end as \"Segment\"\r\n```\r\n\r\nWorld Wide Importers made the remapping simple as company names are suffixed with the office's city in parenthesis:\r\n\r\n```\r\ncase\r\n   when c.CustomerName like '%(%' then N'Corporate'\r\n   else N'Consumer' end as \"Segment\"\r\n```\r\n\r\n**Ship Date missing**\r\n\r\nOnly APAC Superstore has this column, with no decent equivalents in Contoso and World Wide Importers. So we used T-SQL randomisation to populate this column.\r\n\r\nAs Contoso is a regional dataset, the _Ship Date_ is set to randomise up to 17 days after the _Order Date_.\r\n\r\n```\r\nconvert(varchar, (dateadd(day,(abs(checksum(newid())) % 18),s.Datekey)),103) \"Ship Date\"\r\n```\r\n\r\nTo give some variation toward shorter ship times, World Wide Importers randomises up to 7 days after the _Order Date_.\r\n\r\n```\r\nconvert(varchar,dateadd(day,(abs(checksum(newid())) % 8),o.OrderDate),103) \"Ship Date\",\r\n```\r\n\r\n**Ship Mode**\r\n\r\nLike _Ship Date_, only APAC Superstore has this column. This value is randomised in different phases for Contoso and Wide World Importers.\r\n\r\nThe first pass involves checking where the _Order Date_ and _Ship Date_ are the same value, and approximately 66.6% of them are randomised to be treated as \"Same Day\" shipment.\r\n\r\n```\r\nupdate zs2\r\n   set zs2.FromShipMode = 'Same Day'\r\n   from z_sales zs2\r\n   inner join\r\n      (select cast((ABS(CHECKSUM(NewId()))%3) as bigint) as \"random\", zs1.SalesStagingID\r\n       from  z_sales zs1\r\n       where zs1.FromOrderDate = zs1.FromShipDate and zs1.FromShipMode is null) as rzs\r\n      on zs2.SalesStagingID = rzs.SalesStagingID\r\n   where rzs.random < 2;\r\n```\r\n\r\nAs randomisation in SQL Server seems to result in near-equal value distribution, all remaining sales are allocated _Ship Mode_ based on a 0 to 9 randomisation with categories disproportionately assigned to integer ranges. \r\n\r\n```\r\nupdate zs2\r\n   set zs2.FromShipMode =\r\n      case\r\n         when rzs.random between 0 and 5 then N'Standard Class'           \r\n         when rzs.random between 6 and 8 then N'Second Class'\r\n         else N'First Class' end\r\n   from z_sales zs2\r\n   inner join\r\n      (select cast((ABS(CHECKSUM(NewId()))%10) as bigint) as \"random\", zs1.SalesStagingID\r\n       from  z_sales zs1\r\n       where zs1.FromShipMode is null) as rzs\r\n      on zs2.SalesStagingID = rzs.SalesStagingID;\r\n```\r\n\r\n**Different date ranges**\r\n\r\nAligning the order dates across the datasets to cover the same years is done with an initial randomised approximately 50:50 split of the sales data to be moved to 2022.\r\n\r\n```\r\nupdate zs2\r\n   set\r\n      zs2.toshipdate = dateadd(year,(2022-cast(convert(varchar(4),zs2.fromshipdate,112) as int)),zs2.fromshipdate),\r\n      zs2.toorderdate = dateadd(year,(2022-cast(convert(varchar(4),zs2.fromorderdate,112) as int)),zs2.fromorderdate)\r\n   from z_sales zs2\r\n   inner join\r\n      (select cast((ABS(CHECKSUM(NewId()))%2) as bigint) as \"random\", zs1.SalesStagingID\r\n       from  z_sales zs1 where zs1.toorderdate is null and zs1.toshipdate is null) as rzs\r\n       on zs2.SalesStagingID = rzs.SalesStagingID\r\n   where rzs.random < 1;\r\n```\r\n\r\nWith the remainder set to the following year. In hindsight, it would have been better to implement this as a package-scoped variable to allow the user to place any year they wanted.\r\n\r\n```\r\nupdate zs\r\n   set\r\n      zs.toshipdate = dateadd(year,(2022-cast(convert(varchar(4),zs.fromshipdate,112) as int)+1),zs.fromshipdate),\r\n      zs.toorderdate = dateadd(year,(2022-cast(convert(varchar(4),zs.fromorderdate,112) as int)+1),zs.fromorderdate)\r\n   from z_sales zs\r\n   where zs.toorderdate is null and zs.toshipdate is null;\r\n```\r\n\r\n**OTHER LESSONS LEARNED**\r\n\r\n**Creating bins for histograms**\r\n\r\nAPAC Superstore uses a Profit (bin) field to round down profits to nearest $200, including going into negatives. To apply that to the invoice line profit field in Wide World Importers, the basic select for this would be:\r\n\r\n```\r\nfloor( il.LineProfit / 200 ) * 200\r\n```\r\n\r\n**For Flat File connections, Visual Studio may retain cached file loads**\r\n\r\nSimply running the Play button won't reflect changes to the file, but I found closing and reopening Visual Studio before hitting play to work in some cases.\r\n\r\n**Unicode and non-Unicode conversion**\r\n\r\nFor this error message that appears when using OLE DB Source: _\"Column cannot convert between unicode and non-unicode string data types.\"_\r\nRight-click the OLE DB Source element > Show Advanced Editor > navigate to \"Input and Output Properties\" tab > expand \"OLE DB Source Output\" > expand \"Output Columns\" > check the DataType field for each column reported, and adjust where needed.\r\n\r\n**Flat file data sources, with double quotations as string delimiter**\r\n\r\nTo get rid of them, the following can be used in Derived Column transformations **[https://stackoverflow.com/questions/65176461/ssis-flat-file-source-get-rid-of-some-embedded-unwanted-double-quotes](https://stackoverflow.com/questions/65176461/ssis-flat-file-source-get-rid-of-some-embedded-unwanted-double-quotes)**.\r\n\r\n```\r\nREPLACE([4-2-27  Data Conversion 3].Product,\"\\\"\",\"\")\r\n```\r\n**Note:** There's a backspace before the second double quotation mark, in order to escape the double quotation. Which is ironic since markdown files do the same thing any you may not see it in the code above.\r\n\r\n\r\n**SSIS Derived Columns can truncate flat file columns**\r\n\r\nThis can be resolved by right-clicking the Derived Column element > Show Advanced Editor, see above Unicode error for same steps, but this time involves changing the length for Derived Column Output **[https://nishantrana.me/2019/05/08/error-the-derived-column-failed-because-truncation-occurred-and-the-truncation-row-disposition-on-derived-column-outputsderived-column-output-columnsfilepath-specifies-failure-on-truncat/](https://nishantrana.me/2019/05/08/error-the-derived-column-failed-because-truncation-occurred-and-the-truncation-row-disposition-on-derived-column-outputsderived-column-output-columnsfilepath-specifies-failure-on-truncat/)**.\r\n\r\n**Creating text file-based remapping files may not work if NULLs are involved**\r\n\r\nRemapping files for Wide World Importers had to be separated from the other 2 data sources because it alone had null on categories. Otherwise produced whitespace that could not be trimmed because it used an ASCII 194 160 combination **[https://stackoverflow.com/questions/42424555/trim-whitespace-ascii-character-194-from-string](https://stackoverflow.com/questions/42424555/trim-whitespace-ascii-character-194-from-string)**.\r\n\r\n**Data Flow encapsulation**\r\n\r\nI noticed a strange behaviour where putting the flows to populate both the main State and City tables inside a single dataflow populates just State, but not the City table, but if the flows were separated into their own data flows, then they each populate. Currently suspecting it may be because they share the same OLE DB Source (though different SQL was used).\r\n\r\n\r\nClick **[here](https://github.com/datamesse/datamesse.github.io/blob/main/src/posts/2022-01-12.md)** for this post's markdown file in GitHub."}]},,,,,,,,,,,,function(e,t){e.exports="data:image/jpeg;base64,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"},function(e,t,a){e.exports=a.p+"static/media/bannervideo-bluerectangles2.b90dc4f8.mp4"},function(e,t,a){e.exports=a.p+"static/media/storyset-sentiment-analysis-rafiki-animated.2a24a8e5.svg"},function(e,t,a){e.exports=a.p+"static/media/storyset-on-the-way-pana.1ccac642.svg"},function(e,t,a){e.exports=a.p+"static/media/flaticon-calendar-lines-pen.f25f6194.svg"},function(e,t,a){e.exports=a.p+"static/media/flaticon-chart-histogram-white.96e56661.svg"},function(e,t,a){e.exports=a.p+"static/media/flaticon-cross-circle-white.712ceeff.svg"},function(e,t,a){e.exports=a.p+"static/media/flaticon-diploma-white.a5093c2a.svg"},function(e,t,a){e.exports=a.p+"static/media/flaticon-edit-white.82f6939c.svg"},function(e,t,a){e.exports=a.p+"static/media/flaticon-globe.1f287fc5.svg"},function(e,t,a){e.exports=a.p+"static/media/flaticon-keyboard.ee60aa38.svg"},function(e,t,a){e.exports=a.p+"static/media/flaticon-magic-wand.b3dd9403.svg"},function(e,t,a){e.exports=a.p+"static/media/flaticon-menu-burger-white.c72a01cd.svg"},function(e,t,a){e.exports=a.p+"static/media/flaticon-paper-plane-white.5d14ae2e.svg"},function(e,t,a){e.exports=a.p+"static/media/logo-aws.fdff4014.svg"},function(e,t,a){e.exports=a.p+"static/media/logo-azure.cc412b1e.svg"},function(e,t,a){e.exports=a.p+"static/media/logo-css.15678ffe.svg"},function(e,t,a){e.exports=a.p+"static/media/logo-excel.be1669c6.svg"},function(e,t,a){e.exports=a.p+"static/media/logo-github.c38b6985.svg"},function(e,t,a){e.exports=a.p+"static/media/logo-html.d84fde66.svg"},function(e,t,a){e.exports=a.p+"static/media/logo-javascript.f935b988.svg"},function(e,t,a){e.exports=a.p+"static/media/logo-json.e2d8a328.svg"},function(e,t,a){e.exports=a.p+"static/media/logo-linkedin.5a996c66.svg"},function(e,t,a){e.exports=a.p+"static/media/logo-mongodb.c2a864c9.svg"},function(e,t,a){e.exports=a.p+"static/media/logo-nodejs.a1231528.svg"},function(e,t,a){e.exports=a.p+"static/media/logo-photoshop.4c7cd5a2.svg"},function(e,t,a){e.exports=a.p+"static/media/logo-powerbi.c70d8b43.svg"},function(e,t,a){e.exports=a.p+"static/media/logo-python.9ecfb672.svg"},function(e,t,a){e.exports=a.p+"static/media/logo-react.06e73328.svg"},function(e,t,a){e.exports=a.p+"static/media/logo-rstudio.ef95b59e.svg"},function(e,t,a){e.exports=a.p+"static/media/logo-sqlserver.7afc51d3.svg"},function(e,t,a){e.exports=a.p+"static/media/logo-tableau.148cae2e.svg"},function(e,t,a){e.exports=a.p+"static/media/logo-twitter.c86b433a.svg"},function(e,t,a){e.exports=a.p+"static/media/logo-typescript.0cdb301b.svg"},function(e,t,a){e.exports=a.p+"static/media/logo-visualstudiocode.e60fcd09.svg"},function(e,t,a){e.exports=a.p+"static/media/logo-xml.100df9c5.svg"},function(e,t,a){e.exports=a.p+"static/media/logo-youtube.ee528d4f.svg"},function(e,t,a){e.exports=a.p+"static/media/logoblue-github.b3bb8988.svg"},function(e,t,a){e.exports=a.p+"static/media/logoblue-linkedin.56e8d75c.svg"},function(e,t,a){e.exports=a.p+"static/media/logoblue-powerbi.fbc8a721.svg"},function(e,t,a){e.exports=a.p+"static/media/logoblue-tableau.b8ed4136.svg"},function(e,t,a){e.exports=a.p+"static/media/logoblue-twitter.dfac15a2.svg"},function(e,t,a){e.exports=a.p+"static/media/logoblue-youtube.90400f1e.svg"},function(e,t){e.exports="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFYAAABXCAYAAACeCrJSAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsQAAA7EAZUrDhsAAAN4SURBVHhe7d1fSFNhGMfx52zNTVe5qVkSlpqlmQRdRARKUV0YReBN0EXUhXTbXdBF2F3UdZBREBhREdVFBP2D0pvIxD+FgvmnQOdEU2tr6jannSOPJtbmmZ6H4N3vA7L3PYoXX87Oec/O2LT8K42zBJYTCWvXiGqPbONZfN3fJ+huq59nahEJ67Bp1Huhgmfxve0dozOPOnimFhs/gsUQVgjCCkFYIQgrBGGFIKwQhBWCsEIQVgjCCkFYIQgrBGGFIKwQhBWCsEIQVgjCCkFYIQgrBGGFIKwQhBWCsEIQVgjCCkFYIQgrBGGFIKwQhBWCsEIQVgjCCkFYIQgrBGGFIKwQhBWCsEIQVgjCCkFYIQgrBGGFIKwQhBWCsEIQVgjCChEJq2k8SGFCYc2VnZ5R95OpRMIaHw9lRgxhk+Owm/u3UYRNjse1hkeJTUZneKQekbDZGQ4eJeYPhnmknv8a1heY4pF6ZMK6TYb9iT02KbnuNB4l5gsgbFJ2bVzLo8QQNknlJsKOTkQoPI1VgWnGUivf4+JZfO3+XzxSk+Vhd+eZOww0fh3nkZosD3u0JIdHiSFsEoxPOj5WunzYIf3CoGd0kmdqsjTswSKvfoxdfg2r+t5qsDRszd7NPErsXR/CmnZcPwTs3+rhWXyD+tr15ZdRnqnLkrDpDhtdOlTEs8TqPgwo/XLhvFWHddo1ulm9k/LWO3lLfMZFwYP2IZ6pzaafyFcsTY9aV12mn7SyeEtit5t8NKXw1dZitsoCL12t2k6ZJl+cnldZ4KHnZ/fQ4WJzUccno1Sv6Ld0/It2+uHn2fqT5RSNzVDzQIAa9KWQcdbuHA7xn/xRnJ1O+/IzqUq/CDhQ6OWt5tQ87qBX3WM8U99C2KVCkRgFw9NzT12bps3t0cnu1fPutfnp4osenqWGuCcvd5qdNq1zUoE3nbZ4XCuO2uIL0OXXvTxLHZZeICzV/2OKzj3ppHBM/eXVUmJhjeP1ifo2Gg5FeUtqEQn7tGOYTt3/pK9bUzOqwdKwEX1lca3hG51/1pWST//FLAlrrBzuNPuoou4jXX/fz1tTm824Bd018vea1YyRUIRuNQ1QxY0mqn3TR0PBCP8GFr7ab0dOBpXluqlkg5tK9Z/CLBc57ba592EZ7wo0bvyN6VdPrYNBatWXUC36o8p3WVcL3/Ipgug3SvjvmKPNVfUAAAAASUVORK5CYII="},function(e,t){e.exports="data:image/png;base64,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"},function(e,t){e.exports="data:image/png;base64,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"},function(e,t){e.exports="data:image/png;base64,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"},function(e,t){e.exports="data:image/png;base64,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"},,,,function(e,t,a){e.exports=a.p+"static/media/img-2022-05-portfolio-website-react.961a1d52.gif"},function(e,t,a){e.exports=a.p+"static/media/img-2021-10-power-bi-quarterly-singapore-rental-prices-by-currency.944dc5d6.gif"},function(e,t,a){e.exports=a.p+"static/media/img-2021-11-power-bi-follow-the-sun-customer-support.5d3b31d5.gif"},function(e,t,a){e.exports=a.p+"static/media/img-2022-01-sql-server-integration-services-international-marketplace-trimmed.7971d43c.png"},function(e,t,a){e.exports=a.p+"static/media/img-2022-04-power-bi-international-marketplace-python-deneb.fab78880.gif"},function(e,t,a){e.exports=a.p+"static/media/img-2022-06-excel-customer-support-agent-performance.785039de.gif"},function(e,t,a){e.exports=a.p+"static/media/img-2022-08-satellite-launch-overview.30e65a6b.gif"},function(e,t,a){e.exports=a.p+"static/media/img-2022-10-excel-international-marketplace-profit-forecast.3adfbfda.gif"},function(e,t,a){e.exports=a.p+"static/media/img-2022-11-power-bi-appsource-deneb-maps.f42b54e4.gif"},,,,,,function(e,t,a){e.exports=a(106)},,,,,function(e,t,a){},,,,,,,,,,,,,,,function(e,t,a){"use strict";a.r(t);var n=a(0),r=a.n(n),o=a(69),s=a.n(o),i=a(5),l=a(2),d=function(e){e&&e instanceof Function&&a.e(1).then(a.bind(null,110)).then(function(t){var a=t.getCLS,n=t.getFID,r=t.getFCP,o=t.getLCP,s=t.getTTFB;a(e),n(e),r(e),o(e),s(e)})},c=(a(91),a(1),a(83)),m=a(21),u=a.n(m),h=(a(22),a(23)),p=a.n(h),g=a(24),f=a.n(g),b=(a(25),a(26)),w=a.n(b),y=a(27),v=a.n(y),A=a(28),D=a.n(A),x=a(29),T=a.n(x),S=a(30),E=a.n(S),C=(a(31),a(32),a(33)),k=a.n(C),I=a(34),P=a.n(I),O=(a(35),a(36),a(37)),L=a.n(O),F=a(38),M=a.n(F),B=a(39),N=a.n(B),R=a(40),U=a.n(R),z=a(41),Y=a.n(z),q=(a(42),a(43)),G=a.n(q),Q=(a(44),a(45),a(46)),_=a.n(Q),j=a(47),V=a.n(j),H=(a(48),a(49),a(50),a(51)),W=a.n(H),X=a(52),J=a.n(X),K=a(53),Z=a.n(K),$=(a(54),a(55),a(56)),ee=a.n($),te=(a(57),a(58),a(59),a(60),a(61),a(62),a(63),a(64)),ae=a.n(te),ne=a(65),re=a.n(ne),oe=a(66),se=a.n(oe),ie=a(67),le=a.n(ie),de=a(68),ce=a.n(de),me=a(10);var ue=function(){return r.a.createElement("div",{className:"menu-desktop"},r.a.createElement("div",{className:"menu-desktop-option"},r.a.createElement(me.a,{to:"/portfolio"},r.a.createElement("img",{className:"menu-option",src:w.a}),"PORTFOLIO")),r.a.createElement("div",{className:"menu-desktop-option"},r.a.createElement(me.a,{to:"/blog"},r.a.createElement("img",{className:"menu-option",src:T.a}),"BLOG")),r.a.createElement("div",{className:"menu-desktop-option"},r.a.createElement(me.a,{to:"/#skills"},r.a.createElement("img",{className:"menu-option",src:D.a}),"SKILLS")),r.a.createElement("div",{className:"menu-desktop-option"},r.a.createElement(me.a,{to:"/#contact"},r.a.createElement("img",{className:"menu-option",src:P.a}),"CONTACT")))};var he=function(e){var t=e.isOpen,a=e.setIsOpen;return r.a.createElement("div",{className:"menu-mobile"},r.a.createElement("div",{onClick:function(){return a(!t)}},r.a.createElement("img",{className:"close-icon",src:v.a})),r.a.createElement("div",{className:"menu-mobile-options"},r.a.createElement("div",{className:"menu-mobile-option"},r.a.createElement(me.a,{to:"/portfolio"},r.a.createElement("img",{className:"menu-option",src:w.a}),"PORTFOLIO")),r.a.createElement("div",{className:"menu-mobile-option"},r.a.createElement(me.a,{to:"/blog"},r.a.createElement("img",{className:"menu-option",src:T.a}),"BLOG")),r.a.createElement("div",{className:"menu-mobile-option"},r.a.createElement(me.a,{to:"/#skills"},r.a.createElement("img",{className:"menu-option",src:D.a}),"SKILLS")),r.a.createElement("div",{className:"menu-mobile-option"},r.a.createElement(me.a,{to:"/#contact"},r.a.createElement("img",{className:"menu-option",src:P.a}),"CONTACT"))))},pe="https://raw.githubusercontent.com/datamesse/datamesse.github.io/main/src/assets-theme/",ge=String.fromCharCode(104,116,116,112,115,58,47,47,119,119,119,46,108,105,110,107,101,100,105,110,46,99,111,109,47,105,110,47),fe=(String.fromCharCode(74,46,32),String.fromCharCode(74,85,78,32),String.fromCharCode(77,65,75,85)),be=String.fromCharCode(72,65,82,73);var we=function(){var e=Object(n.useState)(!1),t=Object(c.a)(e,2),a=t[0],o=t[1];return r.a.createElement(r.a.Fragment,null,r.a.createElement("div",{className:"header"},r.a.createElement("div",{className:"header-body"},r.a.createElement("div",{className:"sitename",id:"title"},r.a.createElement("a",{href:"/"},r.a.createElement("img",{className:"site-title",src:ae.a}),r.a.createElement("img",{className:"site-title",src:re.a}),r.a.createElement("img",{className:"site-title",src:se.a}),r.a.createElement("img",{className:"site-title",src:le.a}),r.a.createElement("img",{className:"site-title",src:ce.a}))),r.a.createElement("div",{className:"menu"},r.a.createElement("div",{className:"menu-desktop"},r.a.createElement(ue,null)),r.a.createElement("div",{className:"menu-mobile"},r.a.createElement("div",{onClick:function(){return o(!a)}},r.a.createElement("img",{className:"menu-icon",src:k.a})),a&&r.a.createElement(he,{isOpen:a,setIsOpen:o}))))),r.a.createElement("div",{className:"header-subtitle"},r.a.createElement("div",{className:"header-subtitle-line"},r.a.createElement("p",null,"DATA ANALYTICS AND POWER BI ENTHUSIAST"))))},ye=[{platform:"LinkedIn",icon:G.a,link:ge+fe+be},{platform:"GitHub",icon:N.a,link:"https://github.com/datamesse/"},{platform:"Twitter",icon:Z.a,link:"https://twitter.com/data_messe/"},{platform:"Power BI Community",icon:V.a,link:"https://community.powerbi.com/t5/forums/recentpostspage/post-type/message/category-id/PBI_Comm_Galleries/user-id/331466"},{platform:"Tableau Public",icon:J.a,link:"https://public.tableau.com/profile/data.messe#!/"}];var ve=function(){var e=ye;return r.a.createElement("div",{className:"links"},r.a.createElement("ul",null,e.map(function(e){return r.a.createElement("li",null,r.a.createElement("a",{href:e.link,target:"_blank"},r.a.createElement("div",{className:"link"},r.a.createElement("div",{className:"link-icon-container"},r.a.createElement("img",{src:e.icon,className:"link-icon"})),r.a.createElement("div",{className:"link-text"},e.platform))))})))};var Ae=function(){return r.a.createElement("div",{className:"about"},r.a.createElement("div",{className:"about-top"},r.a.createElement("div",{className:"about-links"},r.a.createElement(ve,null)),r.a.createElement("div",{className:"about-info"},r.a.createElement("div",{className:"about-title"},r.a.createElement("img",{className:"about-avatar",src:u.a}),r.a.createElement("label",{className:"section-title"},"ABOUT ME")),r.a.createElement("p",null,"Customer service veteran with strong foundational data skills from SaaS support work and personal projects."),r.a.createElement("p",null,"My career goal is to constantly challenge myself learning data technologies and leveraging those skills to provide excellent solutions for clients.")),r.a.createElement("div",{className:"about-photo"},r.a.createElement("img",{className:"about-image",src:p.a}))),r.a.createElement("div",{className:"about-bottom"}))};var De=function(e){var t=e.project;return r.a.createElement("div",{className:"project"},r.a.createElement("div",{className:"project-info"},r.a.createElement("label",{className:"project-title"},t.title),r.a.createElement("div",{className:"project-links-date"},r.a.createElement("div",{className:"project-links"},t.siteURL&&r.a.createElement("a",{href:t.siteURL,target:"_blank"},r.a.createElement("div",{className:"project-link-button"},r.a.createElement("img",{className:"project-link-icon",src:E.a}),"SITE")),t.codeURL&&r.a.createElement("a",{href:t.codeURL,target:"_blank"},r.a.createElement("div",{className:"project-link-button"},r.a.createElement("img",{className:"project-link-icon",src:N.a}),"CODE"))),r.a.createElement("div",{className:"project-date"},r.a.createElement("label",null,t.shortdate))),r.a.createElement("p",null,t.description),r.a.createElement("div",{className:"project-tags"},t.tags.map(function(e){return r.a.createElement("label",{className:"project-tag"},e)}))),r.a.createElement("a",{href:t.siteURL,target:"_blank"},r.a.createElement("center",null,r.a.createElement("img",{src:t.photo,className:"project-photo"}))))};var xe=function(){return r.a.createElement("center",null,r.a.createElement("div",{className:"separator"}))},Te=a(72),Se=a.n(Te),Ee=a(73),Ce=a.n(Ee),ke=a(74),Ie=a.n(ke),Pe=a(75),Oe=a.n(Pe),Le=a(76),Fe=a.n(Le),Me=a(77),Be=a.n(Me),Ne=a(78),Re=a.n(Ne),Ue=a(79),ze=a.n(Ue),Ye=a(80),qe=a.n(Ye),Ge=[{title:"AppSource Deneb maps report",description:"Power BI report demonstrating custom AppSource Deneb maps using embedded geometry JSON data.",stamp:"2022-11",shortdate:"NOV 2022",stack:V.a,photo:qe.a,siteURL:"https://community.powerbi.com/t5/Data-Stories-Gallery/AppSource-Deneb-Maps/m-p/2930366",codeURL:"https://datamesse.github.io/#/post/1669381200",tags:["Power BI","Deneb","Vega-Lite","Vega"]},{title:"International marketplace profit dashboard",description:"Excel dashboard using an out-of-the-box exponential smoothing algorithm to forecast profit.",stamp:"2022-10",shortdate:"OCT 2022",stack:M.a,photo:ze.a,siteURL:"https://datamesse.github.io/#/project/ExcelInternationalMarketplaceProfitForecast",codeURL:"",tags:["Excel","Power Pivot","DAX","Forecasting"]},{title:"Satellite launch overview report",description:"Power BI report showing actively monitored satellites using a custom Deneb scatterplot visual.",stamp:"2022-08",shortdate:"AUG 2022",stack:V.a,photo:Re.a,siteURL:"https://community.powerbi.com/t5/Data-Stories-Gallery/Satellite-launch-overview/m-p/2730077",codeURL:"https://github.com/datamesse/data-visualisation-datasets/tree/main/Satellite%20launch%20overview",tags:["Power BI","Deneb","Vega-Lite","Power Query","DAX"]},{title:"Customer support agent performance dashboard",description:"Excel dashboard comparing overall team and individual staff KPIs. Includes a cell filter into Power Query and dynamically displays photos.",stamp:"2022-06",shortdate:"JUN 2022",stack:M.a,photo:Be.a,siteURL:"https://datamesse.github.io/#/project/ExcelCustomerSupportAgentPerformance",codeURL:"https://github.com/datamesse/data-visualisation-datasets/tree/main/Support%20ticket%20updates",tags:["Excel","Power Query","Power Pivot","DAX"]},{title:"International marketplace profit report",description:"Power BI report incorporating Python profit predictions and custom Deneb (JSON-based) visualisations.",stamp:"2022-04",shortdate:"APR 2022",stack:V.a,photo:Fe.a,siteURL:"https://community.powerbi.com/t5/Data-Stories-Gallery/International-Marketplace-profit-report-using-Python-and-Deneb/m-p/2480550",codeURL:"https://github.com/datamesse/data-visualisation-datasets/blob/main/International%20Marketplace%20sales/Power%20Query%20and%20Deneb%20code.md",tags:["Power BI","Python","Deneb","Vega-Lite","Power Query","Forecasting"]},{title:"International marketplace dataset",description:"SQL Server Integration Services package that merges different Microsoft and Tableau sample sales datasets into a single data warehouse.",stamp:"2022-01",shortdate:"JAN 2022",stack:W.a,photo:Oe.a,siteURL:"https://datamesse.github.io/#/post/1641906000",codeURL:"https://github.com/datamesse/data-visualisation-datasets/tree/main/International%20Marketplace%20sales",tags:["T-SQL","ETL","SQL Server Integration Services"]},{title:"Follow the sun customer support report",description:"Power BI report showing teamwork and performance for global customer service. Incorporates dynamic daylight saving offsets not inherent in Power BI time intelligence.",stamp:"2021-11",shortdate:"NOV 2021",stack:V.a,photo:Ie.a,siteURL:"https://community.powerbi.com/t5/Data-Stories-Gallery/Follow-the-sun-customer-service-support/m-p/2168279",codeURL:"https://github.com/datamesse/data-visualisation-datasets/tree/main/Support%20ticket%20updates",tags:["Power BI","Power Query","DAX"]},{title:"Singapore rental prices report",description:"Power BI report showing trends of Singapore rent by quarter, flat type, and major currency conversions.",stamp:"2021-10",shortdate:"OCT 2021",stack:V.a,photo:Ce.a,siteURL:"https://community.powerbi.com/t5/Data-Stories-Gallery/Quarterly-Singapore-median-rental-prices-by-currency/m-p/2125424",codeURL:"https://github.com/datamesse/data-visualisation-datasets/tree/main/Singapore%20rental%20prices",tags:["Power BI","Power Query","DAX"]},{title:"Website portfolio and blog",description:"Website coded using HTML, CSS, and JavaScript with React.js and GitHub Pages.",stamp:"2021-09",shortdate:"SEP 2021",stack:U.a,photo:Se.a,siteURL:"https://datamesse.github.io/",codeURL:"https://github.com/datamesse/datamesse.github.io/",tags:["HTML","CSS","React.js"]}];var Qe=function(){var e=Ge;return r.a.createElement("div",{className:"portfolio",id:"portfolio"},r.a.createElement(xe,null),r.a.createElement("label",{className:"section-title"},"PORTFOLIO"),r.a.createElement("div",{className:"portfolio-preview"},e.map(function(e){return r.a.createElement(De,{project:e})})),r.a.createElement(i.b,{to:"/portfolio"},r.a.createElement("h3",null,"Click here to see all projects with descriptions ")))},_e=[{type:"Business Skills",list:[{name:"Microsoft Excel",shortname:"Excel",date:"30th October 2019",shortdate:"OCT 2019",score:5,taken:"25.7 million",icon:M.a},{name:"Microsoft Power BI",shortname:"Power BI",date:"27th August 2021",shortdate:"AUG 2021",score:15,taken:"699.9 thousand",icon:V.a}]},{type:"Technical Skills",list:[{name:"Microsoft T-SQL",shortname:"T-SQL",date:"27th August 2021",shortdate:"AUG 2021",score:30,taken:"406.8 thousand",icon:W.a},{name:"HTML",shortname:"HTML",date:"10th September 2020",shortdate:"SEP 2020",score:30,taken:"3.3 million",icon:U.a},{name:"CSS",shortname:"CSS",date:"10th September 2020",shortdate:"SEP 2020",score:15,taken:"1.7 million",icon:L.a},{name:"JavaScript",shortname:"JavaScript",date:"10th May 2020",shortdate:"MAY 2020",score:5,taken:"2.2 million",icon:Y.a},{name:"XML",shortname:"XML",date:"5th May 2022",shortdate:"MAY 2022",score:15,taken:"237.4 thousand",icon:ee.a}]},{type:"Design Skills",list:[{name:"Adobe Photoshop",shortname:"Photoshop",date:"10th August 2020",shortdate:"AUG 2020",score:30,taken:"3.1 million",icon:_.a}]}];var je=function(e){var t=e.skill,a=100-t.score;return r.a.createElement("div",{className:"skill2"},r.a.createElement("div",{className:"skill2-iconcontainer"},r.a.createElement("img",{className:"skill2-icon",src:t.icon,alt:t.name})),r.a.createElement("div",{className:"skill2-bar"},r.a.createElement("div",{className:"skill2-progress-bar"},r.a.createElement("div",{className:"skill2-progress-bar-fill",style:{width:a+"%"}},"Top ","".concat(t.score,"%")," of"),r.a.createElement("div",{className:"skill2-progress-bar-fill-end"})),r.a.createElement("div",{className:"skill2-subtext"},r.a.createElement("label",{className:"skill2-name"},t.shortname),r.a.createElement("label",{className:"skill2-people"},t.taken+" people"))))};var Ve=function(){var e=_e;return r.a.createElement("div",{className:"skills",id:"skills"},r.a.createElement(xe,null),r.a.createElement("label",{className:"section-title"},"SKILLS"),r.a.createElement("p",null,"These charts indicate my skill and rank among the number of people that took each LinkedIn Skills Assessment test."),r.a.createElement("br",null),r.a.createElement("div",{className:"skills2-full-list"},r.a.createElement("div",null,e.map(function(e){return r.a.createElement("div",{className:"skills2-section"},r.a.createElement("label",{className:"skills-section-title"},e.type),r.a.createElement("div",{className:"skills2-list"},e.list.map(function(e){return r.a.createElement(je,{skill:e})})))}))),r.a.createElement("br",null),r.a.createElement("p",null,"For more information on my other skills, please visit my LinkedIn profile."),r.a.createElement("p",null,"For more information on LinkedIn's assessments, please visit the links below."),r.a.createElement("ul",null,r.a.createElement("li",null,r.a.createElement("a",{href:"https://www.linkedin.com/help/linkedin/answer/a507663",target:"_blank"},"How are LinkedIn Skill Assessments made and scored?")),r.a.createElement("li",null,r.a.createElement("a",{href:"https://www.linkedin.com/help/linkedin/answer/a507734",target:"_blank"},"What are the available LinkedIn Skill Assessments?"))))},He=a(9),We=a(111);function Xe(){var e=He.slice(0,3),t=He.length,a=e.map(function(e){return e.content.split(" ").slice(0,30).join(" ")+" . . . "});return r.a.createElement("div",{className:"blog-preview",id:"blog"},r.a.createElement(xe,null),r.a.createElement("label",{className:"section-title"},"RECENT BLOG POSTS"),r.a.createElement("div",{className:"blog-set-preview"},e.length&&e.map(function(e,t){return r.a.createElement("div",{key:t,className:"blog-tile-preview"},r.a.createElement("div",{className:"blog-tile-header-preview"},r.a.createElement("div",{className:"blog-icon-title-preview"},r.a.createElement("span",{className:"blogstamp-overlay"},r.a.createElement("img",{src:"".concat(pe).concat(e.tag,".svg"),alt:""})),r.a.createElement(i.b,{to:"/post/".concat(e.id)},e.title)),r.a.createElement("div",{className:"blog-meta-preview"},r.a.createElement("small",null,e.date))),r.a.createElement(We.a,{className:"blog-desc-preview",escapeHtml:!1},a[t]),r.a.createElement("div",{className:"blog-meta-preview"},r.a.createElement("small",null,r.a.createElement(i.b,{to:"/post/".concat(e.id)},"Read more..."))))})),r.a.createElement(i.b,{to:"/blog"},r.a.createElement("h3",null,"Click here to see all ",t," blog posts")))}var Je=function(){return r.a.createElement("div",{className:"contact",id:"contact"},r.a.createElement(xe,null),r.a.createElement("label",{className:"section-title"},"CONTACT"),r.a.createElement("p",null,"Please feel free to contact me on these platforms:"),r.a.createElement(ve,null))};var Ke=function(){return r.a.createElement("div",{className:"body"},r.a.createElement(Ae,null),r.a.createElement(Qe,null),r.a.createElement(Xe,null),r.a.createElement(Ve,null),r.a.createElement(Je,null))};var Ze=function(){var e=(new Date).getFullYear();return r.a.createElement("div",{className:"footer"},r.a.createElement("p",null,"website happily coded by me using React.js | \u24d2 ",e),r.a.createElement("a",{href:"https://www.flaticon.com/uicons",target:"_blank"},"User interface icons by Flaticon"),r.a.createElement("a",{href:"https://storyset.com/data",target:"_blank"},"Data illustration by Storyset"))};var $e=function(){return r.a.createElement(r.a.Fragment,null,r.a.createElement(we,null),r.a.createElement("div",{className:"home"},r.a.createElement(Ke,null)),r.a.createElement(Ze,null))};var et=function(){var e=Ge;return r.a.createElement(r.a.Fragment,null,r.a.createElement(we,null),r.a.createElement("div",{className:"home"},r.a.createElement("div",{className:"body"},r.a.createElement("div",{className:"portfolio",id:"portfolio"},r.a.createElement("br",null),r.a.createElement("label",{className:"section-title"},"PORTFOLIO"),r.a.createElement("div",null,e.map(function(e){return r.a.createElement(De,{project:e})}))))),r.a.createElement("br",null),r.a.createElement("br",null),r.a.createElement("br",null),r.a.createElement("br",null),r.a.createElement(Ze,null))};function tt(e){var t=parseInt(e.match.params.id);if(!t)return r.a.createElement(l.a,{to:"/404"});var a={},n=!1;return He.forEach(function(e,r){t===e.id&&(a.title=e.title?e.title:"No title given",a.tag=e.tag?e.tag:"No tag given",a.date=e.date?e.date:"No date given",a.content=e.content?e.content:"No content given",n=!0)}),!1===n?r.a.createElement(l.a,{to:"/404"}):r.a.createElement(r.a.Fragment,null,r.a.createElement(we,null),r.a.createElement("div",{className:"post"},r.a.createElement("center",null,r.a.createElement("div",{className:"post-body"},r.a.createElement("h4",{className:"post-meta"},a.date),r.a.createElement("h1",null,a.title),r.a.createElement(We.a,{escapeHtml:!1},a.content)))),r.a.createElement(Ze,null))}s.a.render(r.a.createElement(i.a,null,r.a.createElement("div",null,r.a.createElement(l.b,{exact:!0,path:"/",component:$e}),r.a.createElement(l.b,{exact:!0,path:"/portfolio",component:et}),r.a.createElement(l.b,{exact:!0,path:"/blog",component:function(){var e=He.map(function(e){return e.content.split(" ").slice(0,30).join(" ")+" . . . "});return console.log(He),r.a.createElement(r.a.Fragment,null,r.a.createElement(we,null),r.a.createElement("div",{className:"blog"},r.a.createElement("center",null,r.a.createElement("div",{className:"blog-header"},r.a.createElement("label",{className:"section-title"},"ALL BLOG POSTS")),r.a.createElement("div",{className:"blog-set"},He.length&&He.map(function(t,a){return r.a.createElement("div",{key:a,className:"blog-tile"},r.a.createElement("div",{className:"blog-tile-header"},r.a.createElement("span",{className:"blogstamp-overlay"},r.a.createElement("img",{src:"".concat(pe).concat(t.tag,".svg"),alt:""})),r.a.createElement(i.b,{to:"/post/".concat(t.id)},t.title)),r.a.createElement("div",{className:"blog-meta"},r.a.createElement("small",null,t.date)),r.a.createElement(We.a,{className:"blog-desc",escapeHtml:!1},e[a]),r.a.createElement("div",{className:"blog-meta"},r.a.createElement("small",null,r.a.createElement(i.b,{to:"/post/".concat(t.id)},"Read more..."))),r.a.createElement("div",{className:"blog-tile-footer"}))})))),r.a.createElement(Ze,null))}}),r.a.createElement(l.b,{exact:!0,path:"/post/:id",render:function(e){return r.a.createElement(tt,e)}}),r.a.createElement(l.b,{exact:!0,path:"/404",component:function(){return r.a.createElement("div",{className:"home"},r.a.createElement(we,null),r.a.createElement("img",{className:"notfound-image",src:f.a}),r.a.createElement("div",{className:"notfound"},r.a.createElement("h1",null,"The page you are looking for does not exist.")))}}),r.a.createElement(l.b,{exact:!0,path:"/project/excelcustomersupportagentperformance",component:function(){var e=document.createElement("script");return e.src="https://onedrive.live.com/embed?resid=DA6E81822ACC224B%21116&authkey=%21AM967yoy5TdZEY8&em=3&wdItem=%22'Dashboard'!A%3AXFD%22&wdDivId=%22myExcelDiv%22&wdActiveCell=%22'Dashboard'!B10%22&wdAllowInteractivity=1&wdAllowTyping=1&action=embedview&wdbipreview=true",e.async=!0,document.body.appendChild(e),r.a.createElement(r.a.Fragment,null,r.a.createElement(we,null),r.a.createElement("div",{className:"post"},r.a.createElement("center",null,r.a.createElement("div",{className:"post-body"},r.a.createElement("h4",{className:"post-meta"},"20 June 2022"),r.a.createElement("h1",null,"Excel project: Customer Support Agent Performance Dashboard"),r.a.createElement("p",null,'This is a single worksheet Excel redux of my "Follow the sun customer support report" Power BI report.'),r.a.createElement("br",null),r.a.createElement("p",null,r.a.createElement("img",{src:"https://github.com/datamesse/datamesse.github.io/raw/main/src/assets-portfolio/img-2022-06-excel-customer-support-agent-performance-original.png"})),r.a.createElement("br",null),r.a.createElement("p",null,"You can find a copy of this Excel dashboard to download here:"),r.a.createElement("ul",null,r.a.createElement("li",null,r.a.createElement("a",{href:"https://github.com/datamesse/data-visualisation-datasets/raw/main/Support%20ticket%20updates/Customer%20Support%20Agent%20Performance%20Dashboard.xlsx?raw=true"},"https://github.com/datamesse/data-visualisation-datasets/raw/main/Support%20ticket%20updates/Customer%20Support%20Agent%20Performance%20Dashboard.xlsx"))),r.a.createElement("p",null,'Note: This Excel file makes a connection to my GitHub repository where the data is stored, and may require you to "Enable content" when you open the file to access the External Data Connection.'),r.a.createElement("br",null),r.a.createElement("p",null,"Most of the Power Query and DAX code used in this Excel dashboard is similar to the code used in my Power BI report, except most of the DAX measures now include USERELATIONSHIP due to a data model improvement."),r.a.createElement("br",null),r.a.createElement("br",null),r.a.createElement("p",null,"The Excel dashboard is also embedded below using the OneDrive free version. Due to OneDrive's restrictions with External Data Connections and Camera Tools, the dashboard cannot dynamically display support agent images nor refresh its data connection. So it is recommended you download the Excel file from the link above and open it from a desktop version of Excel."),r.a.createElement("br",null),r.a.createElement("br",null),r.a.createElement("p",null,r.a.createElement("b",null,"References")),r.a.createElement("p",null,"These resources were used to create this Excel dashboard:"),r.a.createElement("ul",null,r.a.createElement("li",null,r.a.createElement("a",{href:"https://www.youtube.com/watch?v=gK2yBpiITvI"},"YouTube Tutorial on how to map an Excel cell value as an input parameter for Power Query by ExcelIsFun")),r.a.createElement("li",null,r.a.createElement("a",{href:"https://www.youtube.com/watch?v=6qOL69bIQz8"},"YouTube Tutorial on how to create a Call Center Performance Dashboard in Excel by Other Level's")),r.a.createElement("li",null,r.a.createElement("a",{href:"https://www.youtube.com/watch?v=T4sZSXdQ4Vg&t=1395s"},"YouTube Tutorial on how to dynamically display photos ased on slicer selection in Excel by Other Level's")),r.a.createElement("li",null,r.a.createElement("a",{href:"https://www.youtube.com/watch?v=uvA-U9FKgPw&t=385s"},"YouTube Tutorial on how to embed Excel workbook to web via OneDrive Personal free version by MyOnlineTrainingHub")),r.a.createElement("li",null,r.a.createElement("a",{href:"https://stackoverflow.com/questions/34424845/adding-script-tag-to-react-jsx"},"Stackoverflow Question on how to run HTML script tag equivalents inside a React component")))),r.a.createElement("div",{id:"myExcelDiv",className:"project-excelcustomersupportagentperformance"}))),r.a.createElement(Ze,null))}}),r.a.createElement(l.b,{exact:!0,path:"/project/excelinternationalmarketplaceprofitforecast",component:function(){var e=document.createElement("script");return e.src="https://onedrive.live.com/embed?resid=DA6E81822ACC224B%21121&authkey=%21ACJCWZMy8qHCeeU&em=3&wdItem=%22'Report'!A1%3AX50%22&wdDivId=%22myExcelDiv%22&wdHideGridlines=1&wdActiveCell=%22'Report'!A1%22&wdAllowTyping=1&action=embedview&wdbipreview=true",e.async=!0,document.body.appendChild(e),r.a.createElement(r.a.Fragment,null,r.a.createElement(we,null),r.a.createElement("div",{className:"post"},r.a.createElement("center",null,r.a.createElement("div",{className:"post-body"},r.a.createElement("h4",{className:"post-meta"},"26 October 2022"),r.a.createElement("h1",null,"Excel project: International Marketplace Profit Dashboard"),r.a.createElement("p",null,'This is a single worksheet Excel redux of my "International marketplace profit" Power BI report.'),r.a.createElement("br",null),r.a.createElement("p",null,r.a.createElement("img",{src:"https://github.com/datamesse/datamesse.github.io/raw/main/src/assets-portfolio/img-2022-10-excel-international-marketplace-profit-forecast-original.png"})),r.a.createElement("br",null),r.a.createElement("p",null,"You can find a copy of this Excel dashboard to download here:"),r.a.createElement("ul",null,r.a.createElement("li",null,r.a.createElement("a",{href:"https://github.com/datamesse/data-visualisation-datasets/raw/main/International%20Marketplace%20sales/International%20Marketplace%20Profit%20Dashboard.xlsx?raw=true"},"https://github.com/datamesse/data-visualisation-datasets/raw/main/International%20Marketplace%20sales/International%20Marketplace%20Profit%20Dashboard.xlsx"))),r.a.createElement("p",null,'Note: This Excel file makes a connection to my GitHub repository where the data is stored, and may require you to "Enable content" when you open the file to access the External Data Connection.'),r.a.createElement("br",null),r.a.createElement("p",null,"Some of the DAX code used in this Excel dashboard is similar to the DAX used in my Power BI report."),r.a.createElement("p",null,"I used Excel's out-of-the-box FORECAST.ETS function to create the monthly profit forecasting. You can find more information on my exploration of this feature in this blog post:"),r.a.createElement("ul",null,r.a.createElement("li",null,r.a.createElement("a",{href:"https://datamesse.github.io/#/post/1666962000"},"https://datamesse.github.io/#/post/1666962000"))),r.a.createElement("br",null),r.a.createElement("br",null),r.a.createElement("p",null,"The Excel dashboard is also embedded below using the OneDrive free version."),r.a.createElement("p",null,"Note my OneDrive version has no support for mapping nor interactivity for external data connections, so I would recommend you download a copy from the link above."),r.a.createElement("br",null),r.a.createElement("br",null)),r.a.createElement("div",{id:"myExcelDiv",className:"project-excelinternationalmarketplaceprofit"}))),r.a.createElement(Ze,null))}}))),document.getElementById("root")),d()}],[[86,3,2]]]);
//# sourceMappingURL=main.cfc75d88.chunk.js.map